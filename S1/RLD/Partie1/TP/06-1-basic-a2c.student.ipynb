{"cells": [{"cell_type": "markdown", "id": "4401f92b", "metadata": {"cell_marker": "\"\"\"", "id": "dYfGJCe52lP4"}, "source": ["# Outlook"]}, {"cell_type": "markdown", "id": "c2caf81d", "metadata": {"cell_marker": "\"\"\"", "id": "aZUSf0n_2otG"}, "source": ["In this notebook, we will implement a simple version of the A2C algorithm using BBRL. To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, details about the [AutoResetGymAgent](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)."]}, {"cell_type": "markdown", "id": "1c1269b6", "metadata": {"cell_marker": "\"\"\"", "id": "MD3A9UmNS2OX"}, "source": ["The A2C algorithm is explained in [this video](https://www.youtube.com/watch?v=BUmsTlIgrBI) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/a2c.pdf)."]}, {"cell_type": "markdown", "id": "76c7df31", "metadata": {"cell_marker": "\"\"\"", "id": "zJZDcDafp7Uf"}, "source": ["## Installation and Imports"]}, {"cell_type": "markdown", "id": "8af6e2a0", "metadata": {"cell_marker": "\"\"\"", "id": "aHO1nIdM21Lq"}, "source": ["### Installation"]}, {"cell_type": "markdown", "id": "05c5e46e", "metadata": {"cell_marker": "\"\"\""}, "source": ["try:\n", "    from easypip import easyimport\n", "except:\n", "    # !pip install easypip\n", "    from easypip import easyimport\n", "\n", "import functools\n", "import time\n", "\n", "easyimport(\"importlib_metadata<5\")\n", "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n", "bbrl = easyimport(\"bbrl\")\n", "import gym\n", "```\n", "\n", "<!-- #region id=\"m4kV9pWV3wRe\" -->\n", "### Imports\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"caqhJYbe5YcO\" -->\n", "Below, we import standard python packages, pytorch packages and gym environments.\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"4l7sTVXbJBE_\" -->\n", "[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms.\n", "<!-- #endregion -->\n", "\n", "import copy\n", "import time\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "\n", "import gym\n", "```"]}, {"cell_type": "markdown", "id": "fa86fb3b", "metadata": {"cell_marker": "\"\"\"", "id": "fE1c7ZLf60X_"}, "source": ["### BBRL imports"]}, {"cell_type": "markdown", "id": "b0b93091", "metadata": {"cell_marker": "\"\"\""}, "source": ["from bbrl.agents.agent import Agent\n", "from bbrl import get_arguments, get_class, instantiate_class\n", "\n", "# The workspace is the main class in BBRL, this is where all data is collected and stored\n", "from bbrl.workspace import Workspace\n", "\n", "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n", "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n", "# or until a given condition is reached\n", "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n", "\n", "# AutoResetGymAgent is an agent able to execute a batch of gym environments\n", "# with auto-resetting. These agents produce multiple variables in the workspace: \n", "# \u2019env/env_obs\u2019, \u2019env/reward\u2019, \u2019env/timestep\u2019, \u2019env/done\u2019, \u2019env/initial_state\u2019, \u2019env/cumulated_reward\u2019, \n", "# ... When called at timestep t=0, then the environments are automatically reset. \n", "# At timestep t>0, these agents will read the \u2019action\u2019 variable in the workspace at time t \u2212 1\n", "from bbrl.agents.gymb import AutoResetGymAgent\n", "```\n", "\n", "<!-- #region id=\"JVvAfhKm9S8p\" -->\n", "## Definition of agents\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"RdqzKSLKDtqz\" -->\n", "The [A2C](http://proceedings.mlr.press/v48/mniha16.pdf) algorithm is an actor-critic algorithm. Thus we need an Actor agent, a Critic agent and an Environment agent. \n", "The actor agent is built on an intermediate ProbAgent, see [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about the  ProbaAgent, the ActorAgent and the environment agent.\n", "<!-- #endregion -->\n", "\n", "class ProbAgent(Agent):\n", "    def __init__(self, observation_size, hidden_size, n_actions):\n", "        super().__init__()\n", "        self.model = nn.Sequential(\n", "            nn.Linear(observation_size, hidden_size),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_size, n_actions),\n", "        )\n", "\n", "    def forward(self, t, **kwargs):\n", "        observation = self.get((\"env/env_obs\", t))\n", "        scores = self.model(observation)\n", "        probs = torch.softmax(scores, dim=-1)\n", "        self.set((\"action_probs\", t), probs)\n", "```\n", "\n", "class ActorAgent(Agent):\n", "    def __init__(self):\n", "        super().__init__()\n", "\n", "    def forward(self, t, stochastic, **kwargs):\n", "        probs = self.get((\"action_probs\", t))\n", "        if stochastic:\n", "            action = torch.distributions.Categorical(probs).sample()\n", "        else:\n", "            action = probs.argmax(1)\n", "\n", "        self.set((\"action\", t), action)\n", "```\n", "\n", "def make_env(env_name):\n", "    return gym.make(env_name)\n", "```"]}, {"cell_type": "markdown", "id": "2df18b84", "metadata": {"cell_marker": "\"\"\"", "id": "Din6iU-1DnyH"}, "source": ["### CriticAgent"]}, {"cell_type": "markdown", "id": "c89043ff", "metadata": {"cell_marker": "\"\"\"", "id": "Nf0mXQvbEw7V"}, "source": ["A CriticAgent is a one hidden layer neural network which takes an observation as input and whose output is the value of this observation. It thus implements a $V(s)$ function. It would be straightforward to define another CriticAgent (call it a CriticQAgent by contrast to a CriticVAgent) that would take an observation and an action as input."]}, {"cell_type": "markdown", "id": "e74e3f9e", "metadata": {"cell_marker": "\"\"\"", "id": "bQLc3dywFiqy"}, "source": [" The `squeeze(-1)` removes the last dimension of the tensor since the critic is expected to output a scalar."]}, {"cell_type": "markdown", "id": "4dac8eb6", "metadata": {"cell_marker": "\"\"\""}, "source": ["class CriticAgent(Agent):\n", "    def __init__(self, observation_size, hidden_size):\n", "        super().__init__()\n", "        self.critic_model = nn.Sequential(\n", "            nn.Linear(observation_size, hidden_size),\n", "            nn.ReLU(),\n", "            nn.Linear(hidden_size, 1),\n", "        )\n", "\n", "    def forward(self, t, **kwargs):\n", "        observation = self.get((\"env/env_obs\", t))\n", "        critic = self.critic_model(observation).squeeze(-1)\n", "        self.set((\"critic\", t), critic)\n", "```\n", "\n", "<!-- #region id=\"EzxoIPtLVJ_i\" -->\n", "### Create the A2C agent\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"tJ0qhRVgVarb\" -->\n", "The code below is rather straightforward. Note that we have not defined anything about data collection, using a RolloutBuffer or something to store the n_step return so far. This will come inside the training loop below.\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"aaNnZw3bXEYd\" -->\n", "Interestingly, the loop between the policy and the environment is first defined as a collection of agents, and then embedded into a single TemporalAgent.\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"1kj9y1hjVrXi\" -->\n", "We delete the environment (not the environment agent) with `del env_agent.env` once we do not need it anymore just to avoid mistakes afterwards.\n", "<!-- #endregion -->\n", "\n", "# Create the A2C Agent\n", "def create_a2c_agent(cfg, env_agent):\n", "    observation_size,  n_actions = env_agent.get_obs_and_actions_sizes()\n", "    prob_agent = ProbAgent(\n", "        observation_size, cfg.algorithm.architecture.actor_hidden_size, n_actions\n", "    )\n", "    action_agent = ActorAgent()\n", "    critic_agent = CriticAgent(observation_size, cfg.algorithm.architecture.critic_hidden_size)\n", "\n", "    # Combine env and policy agents\n", "    agent = Agents(env_agent, prob_agent, action_agent)\n", "    \n", "    # Get an agent that is executed on a complete workspace\n", "    agent = TemporalAgent(agent)\n", "    agent.seed(cfg.algorithm.seed)\n", "    return agent, prob_agent, critic_agent\n", "```"]}, {"cell_type": "markdown", "id": "ceb8248f", "metadata": {"cell_marker": "\"\"\"", "id": "lU3cO6znHyDc"}, "source": ["### The Logger class"]}, {"cell_type": "markdown", "id": "d4208dfd", "metadata": {"cell_marker": "\"\"\"", "id": "E4BrXwTLdK0Z"}, "source": ["The logger class below is not generic, it is specifically designed in the context of this A2C colab."]}, {"cell_type": "markdown", "id": "aafd8fd1", "metadata": {"cell_marker": "\"\"\"", "id": "VKYYp8IHLhd-"}, "source": ["The logger parameters are defined below in `params = { \"logger\":{ ...`"]}, {"cell_type": "markdown", "id": "9c1de835", "metadata": {"cell_marker": "\"\"\"", "id": "rhwNN4oCNOhi"}, "source": ["In this colab, the logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation (see the parameters part below).\n", "Note that the salina Logger is also saving the log in a readable format such that you can use `Logger.read_directories(...)` to read multiple logs, create a dataframe, and analyze many experiments afterward in a notebook for instance. "]}, {"cell_type": "markdown", "id": "71a46c17", "metadata": {"cell_marker": "\"\"\"", "id": "10TUc-PHMqNm"}, "source": ["The code for the different kinds of loggers is available in the [bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/bbrl/utils/logger.py) file."]}, {"cell_type": "markdown", "id": "abbfcca8", "metadata": {"cell_marker": "\"\"\"", "id": "c872tM4WM5FH"}, "source": ["Having logging provided under the hood is one of the features where using RL libraries like BBRL will allow you to save time."]}, {"cell_type": "markdown", "id": "280e76d0", "metadata": {"cell_marker": "\"\"\"", "id": "lmsf5BENLz10"}, "source": ["`instantiate_class` is an inner BBRL mechanism. The `instantiate_class`function is available in the [`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file."]}, {"cell_type": "markdown", "id": "3b86c822", "metadata": {"cell_marker": "\"\"\""}, "source": ["class Logger():\n", "\n", "    def __init__(self, cfg):\n", "        self.logger = instantiate_class(cfg.logger)\n", "\n", "    def add_log(self, log_string, loss, epoch):\n", "        self.logger.add_scalar(log_string, loss.item(), epoch)\n", "\n", "    # Log losses\n", "    def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n", "        self.add_log(\"critic_loss\", critic_loss, epoch)\n", "        self.add_log(\"entropy_loss\", entropy_loss, epoch)\n", "        self.add_log(\"a2c_loss\", a2c_loss, epoch)\n", "\n", "```\n", "\n", "<!-- #region id=\"f2vq1OJHWCIE\" -->\n", "### Setup the optimizer\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"VzmEKF4J8qjg\" -->\n", "We use a single optimizer to tune the parameters of the actor (in the prob_agent part) and the critic (in the critic_agent part). It would be possible to have two optimizers which would work separately on the parameters of each component agent, but it would be more complicated because updating the actor requires the gradient of the critic.\n", "<!-- #endregion -->\n", "\n", "# Configure the optimizer over the a2c agent\n", "def setup_optimizer(cfg, prob_agent, critic_agent):\n", "    optimizer_args = get_arguments(cfg.optimizer)\n", "    parameters = nn.Sequential(prob_agent, critic_agent).parameters()\n", "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n", "    return optimizer\n", "```"]}, {"cell_type": "markdown", "id": "507df3f9", "metadata": {"cell_marker": "\"\"\"", "id": "I6SuPOdW_hxl"}, "source": ["### Execute agent"]}, {"cell_type": "markdown", "id": "e6e16f6d", "metadata": {"cell_marker": "\"\"\"", "id": "WqlH-8DaVWx2"}, "source": ["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]}, {"cell_type": "markdown", "id": "c6333c1f", "metadata": {"cell_marker": "\"\"\"", "id": "bWAmm0pPotTC"}, "source": ["The call to `agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]}, {"cell_type": "markdown", "id": "0351445c", "metadata": {"cell_marker": "\"\"\"", "id": "Rn3MlNQ3qGPr"}, "source": ["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]}, {"cell_type": "markdown", "id": "4333c982", "metadata": {"cell_marker": "r\"\"\""}, "source": ["def execute_agent(cfg, epoch, workspace, agent):\n", "    if epoch > 0:\n", "        workspace.zero_grad()\n", "        workspace.copy_n_last_steps(1)\n", "        agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)\n", "    else:\n", "        agent(workspace, t=0, n_steps=cfg.algorithm.n_timesteps, stochastic=True)\n", "```\n", "\n", "<!-- #region id=\"YQNvhO_VAJbh\" -->\n", "### Compute critic loss\n", "<!-- #endregion -->\n", "\n", "In this basic version, the critic loss is computed by estimating the advantage as an expectation over the temporal difference error $\\delta$. This is not what the standard A2C algorithm does.\n", "\n", "<!-- #region id=\"fxxobbxRaJXO\" -->\n", "Note the `critic[1:].detach()` in the computation of the temporal difference target. The idea is that we compute this target as a function of $V(s_{t+1})$, but we do not want to apply gradient descent on this $V(s_{t+1})$, we will only apply gradient descent to the $V(s_t)$ according to this target value.\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"Ngf5NHvWBbrE\" -->\n", "In practice, `x.detach()` detaches a computation graph from a tensor, so it avoids computing a gradient over this tensor.\n", "<!-- #endregion -->\n", "\n", "<!-- #region id=\"fXwrjbueoDw6\" -->\n", "Note also the trick to deal with terminal states. If the state is terminal, $V(s_{t+1})$ does not make sense. Thus we need to ignore this term. So we multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted into an int, it becomes a 1), we get the term. If `must_bootstrap` is False (=0), we are at a terminal state, so we ignore the term. This trick is used in many RL libraries, e.g. SB3.\n", "<!-- #endregion -->\n", "\n", "def compute_critic_loss(cfg, reward, must_bootstrap, critic):\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "    delta = ...\n", "    \n", "    # Compute critic loss\n", "    td_error = delta ** 2\n", "    critic_loss = td_error.mean()\n", "    return critic_loss, delta\n", "```"]}, {"cell_type": "markdown", "id": "8a0becec", "metadata": {"cell_marker": "\"\"\"", "id": "Jmi91gANWT4z"}, "source": ["## Main training loop"]}, {"cell_type": "markdown", "id": "56ea8d7c", "metadata": {"cell_marker": "\"\"\"", "id": "gAnnEjF9L9gk"}, "source": ["This version uses an AutoResetGymAgent. If you haven't done so yet, read  [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing) which explains a lot of details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line. Read also [the notebook about TimeLimits](https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing) to know more about the computation of `must_bootstrap`."]}, {"cell_type": "markdown", "id": "b91260e1", "metadata": {"cell_marker": "\"\"\"", "id": "OFB1XFE5YEc6"}, "source": ["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. Several things need to be explained here.\n", "- `optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n", "- note that we sum all the losses, both for the critic and the actor, before applying back-propagation with `loss.backward()`. At first glance, summing these losses may look weird, as the actor and the critic receive different updates with different parts of the loss. This mechanism relies on the central property of tensor manipulation libraries like TensorFlow and pytorch. In pytorch, each loss tensor comes with its own graph of computation for back-propagating the gradient, in such a way that when you back-propagate the loss, the adequate part of the loss is applied to the adequate parameters.\n", "These mechanisms are partly explained [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n", "- since the optimizer has been set to work with both the actor and critic parameters, `optimizer.step()` will optimize both agents and pytorch ensure that each will receive its own part of the gradient."]}, {"cell_type": "code", "execution_count": null, "id": "99f551a0", "metadata": {"id": "sk85_sRWW-5s", "tags": []}, "outputs": [], "source": ["def run_a2c(cfg):\n", "    # 1)  Build the  logger\n", "    logger = Logger(cfg)\n", "\n", "    # 2) Create the environment agent\n", "    env_agent = AutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.n_envs,\n", "        cfg.algorithm.seed,\n", "    )\n", "\n", "    # 3) Create the A2C Agent\n", "    a2c_agent, prob_agent, critic_agent = create_a2c_agent(cfg, env_agent)\n", "\n", "    # 4) Create the temporal critic agent to compute critic values over the workspace\n", "    tcritic_agent = TemporalAgent(critic_agent)\n", "\n", "    # 5) Configure the workspace to the right dimension\n", "    # Note that no parameter is needed to create the workspace. \n", "    # In the training loop, calling the agent() and critic_agent() \n", "    # will take the workspace as parameter\n", "    train_workspace = Workspace()\n", "\n", "    # 6) Configure the optimizer over the a2c agent\n", "    optimizer = setup_optimizer(cfg, prob_agent, critic_agent)\n", "\n", "    # 7) Training loop\n", "    for epoch in range(cfg.algorithm.max_epochs):\n", "        # Execute the agent in the workspace\n", "        execute_agent(cfg, epoch, train_workspace, a2c_agent)\n", "\n", "        # Compute the critic value over the whole workspace\n", "        tcritic_agent(train_workspace, n_steps=cfg.algorithm.n_timesteps)\n", "\n", "        transition_workspace = train_workspace.get_transitions()\n", "\n", "        # Get relevant tensors (size are timestep x n_envs x ....)\n", "\n", "        critic, done, reward, action, action_probs, truncated = transition_workspace[\n", "                    \"critic\", \"env/done\", \"env/reward\", \"action\", \"action_probs\", \"env/truncated\"]\n", "\n", "        # Determines whether values of the critic should be propagated\n", "        # True if the episode reached a time limit or if the task was not done\n", "        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n", "        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n", "\n", "        # Compute critic loss (see function above)\n", "        critic_loss, delta = compute_critic_loss(cfg, reward, must_bootstrap, critic)\n", "\n", "        # Take the log probability of the actions performed, after some reorganization\n", "        action_logp = action_probs[0].gather(1, action[0].view(-1, 1)).squeeze().log()\n", "\n", "        # \u00c0 compl\u00e9ter...  \n", "        assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "\n", "        # Store the losses for tensorboard display\n", "        logger.log_losses(cfg, epoch, critic_loss, entropy_loss, a2c_loss)\n", "\n", "        # Compute the total loss\n", "        loss = (\n", "          # [[skip]]\n", "          -cfg.algorithm.entropy_coef * entropy_loss\n", "          # [[/skip]]\n", "          + cfg.algorithm.critic_coef * critic_loss\n", "          - cfg.algorithm.a2c_coef * a2c_loss\n", "        )\n", "\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "\n", "        # Compute the cumulated reward on the final states\n", "        creward = train_workspace[\"env/cumulated_reward\"]\n", "        done = train_workspace[\"env/done\"]\n", "        creward = creward[done]\n", "        if creward.size()[0] > 0:\n", "          # print(creward)\n", "          logger.add_log(\"reward\", creward.mean(), epoch)"]}, {"cell_type": "markdown", "id": "2aa5e396", "metadata": {"cell_marker": "\"\"\"", "id": "uo6bc3zzKua_"}, "source": ["## Definition of the parameters"]}, {"cell_type": "markdown", "id": "e1380a3e", "metadata": {"cell_marker": "\"\"\"", "id": "36r4PAfvKx-f"}, "source": ["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]}, {"cell_type": "code", "execution_count": null, "id": "049aa38b", "metadata": {"id": "JB2B8zELNWQd"}, "outputs": [], "source": ["params={\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": \"./tblogs/tp6-basic/tblogs/\" + str(time.time()),\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 432,\n", "    \"n_envs\": 2,\n", "    \"n_timesteps\": 16,\n", "    \"max_epochs\": 7000,\n", "    \"discount_factor\": 0.95,\n", "    \"critic_coef\": 1.0,\n", "    \"a2c_coef\": 0.1,\n", "    \"architecture\":{\n", "      \"actor_hidden_size\": 32,\n", "      \"critic_hidden_size\": 32,\n", "    },\n", "  },\n", "\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_env\",\n", "    \"env_name\": \"CartPole-v1\",\n", "  },\n", "  \"optimizer\":\n", "  {\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 0.01,\n", "  }\n", "}"]}, {"cell_type": "markdown", "id": "5602040c", "metadata": {"cell_marker": "\"\"\"", "id": "jp7jDeGkaoM1"}, "source": ["### Launching tensorboard to visualize the results"]}, {"cell_type": "code", "execution_count": null, "id": "d8879408", "metadata": {"id": "bT-yD1ZnnNBQ", "tags": []}, "outputs": [], "source": ["# For Colab - otherwise, it is easier and better to launch tensorboard from\n", "# the terminal\n", "if get_ipython().__class__.__module__ == \"google.colab._shell\":\n", "    %load_ext tensorboard\n", "    %tensorboard --logdir ./tmp\n", "else:\n", "    import sys\n", "    import os\n", "    import os.path as osp\n", "    print(f'''Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir=\"{os.getcwd()}/tblogs\"''')"]}, {"cell_type": "code", "execution_count": null, "id": "d257a01d", "metadata": {"id": "l42OUoGROlSt"}, "outputs": [], "source": ["config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)\n", "run_a2c(config)"]}, {"cell_type": "markdown", "id": "f12a3ddf", "metadata": {"cell_marker": "\"\"\"", "id": "vx8c0n2dKuRS"}, "source": ["With the parameters provided in this colab, you should observe that the reward is collapsing after 6K time steps."]}, {"cell_type": "markdown", "id": "36d051ba", "metadata": {"cell_marker": "r\"\"\""}, "source": ["## And now... add some entropy\n", "\n", "To encourage the agent to explore more (or, said otherwise, to let the policy converge less quickly), you can add some entropy-based regularization. \n", "\n", "$$\n", "\\mathcal{L}_{entropy} = \\mathbb{E}_{s \\sim \\pi_s}\\left( p(a | s) \\right)\n", "$$\n", "where $\\pi_s$ corresponds to the stationnary distribution according to the current policy $\\pi$ and the underlying MDP.\n", "\n", "You can use  [`torch.distributions.Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical) to quickly compute entropy for a categorical distribution. "]}, {"cell_type": "code", "execution_count": null, "id": "35ad83d8", "metadata": {}, "outputs": [], "source": ["# \u00c0 compl\u00e9ter...  \n", "assert False, 'Code non impl\u00e9ment\u00e9'\n"]}, {"cell_type": "markdown", "id": "fb545a1e", "metadata": {"cell_marker": "\"\"\"", "id": "paHdoNlz9Lpg"}, "source": ["## What's next?"]}, {"cell_type": "markdown", "id": "04f4fc04", "metadata": {"cell_marker": "\"\"\"", "id": "F2OIv4em9Lpj"}, "source": ["The simple version of A2C above suffers from several limitations:\n", "- During training, the cumulated reward is measured from the training agent itself while it is changing. It is a better practice to stop training and perform a few evaluations on the trained agent from time to time.\n", "- The code above only illustrates A2C with discrete actions, though the algorithm can also deal with continuous actions. Doing so requires defining new Agent classes and uniformizing the way they are used to avoid using \"if discrete/continuous\" parts of codes."]}], "metadata": {"jupytext": {"cell_markers": "\"\"\""}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}
