{"cells": [{"cell_type": "markdown", "metadata": {"id": "O93rKzOfY20t"}, "source": ["# <center>M2 DAC -   Reinforcement Learning & Advanced Deep</center>\n", "##  <center> TME 9. Generative Adversarial Networks  </center>\n", "\n", "Ce TME a pour objectif d'exp\u00e9rimenter les Generative Adversarial Networks (GANs) sur un probl\u00e8me de g\u00e9n\u00e9ration de visages. \n", "\n", "De mani\u00e8re classique, un GAN se formule selon un probl\u00e8me adverse de la mani\u00e8re suivante: \n", "$$\\min\\limits_{G} \\max\\limits_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]$$\n", "Cette formulation met en jeu deux r\u00e9seaux adverse: \n", "*   Un r\u00e9seau discriminateur $D$, dont l'objectif est de savoir distinguer les donn\u00e9es r\u00e9elles des donn\u00e9es simul\u00e9es  \n", "*   Un r\u00e9seau g\u00e9n\u00e9rateur $G$, dont l'objectif est de flouer le discriminateur\n", "\n", "\u00c0 l'optimum, avec des r\u00e9seaux de capacit\u00e9 infinie, la distribution $p_G$ des donn\u00e9es g\u00e9n\u00e9r\u00e9es par $G$ est prouv\u00e9e suivre la distribution des donn\u00e9es r\u00e9elles $p_{data}$. Bien s\u00fbr nous ne travaillons pas avec des r\u00e9seaux de capacit\u00e9 infinie (et d'ailleurs heureusement car on ne veut pas apprendre par coeur les donn\u00e9es d'apprentissage), mais l'objectif est d'approcher cette distribution $p_{data}$ en apprenant un g\u00e9n\u00e9rateur neuronal dont les sorties sont difficilement distinguables des vraies donn\u00e9es pour le discriminateur. \n", "\n", "Nous proposons de mettre ce genre d'architecture pour un la g\u00e9n\u00e9ration de visages: selon un ensemble de visages d'entra\u00eenement, il s'agit d'apprendre \u00e0 g\u00e9n\u00e9rer des visages qui paraissent les plus r\u00e9alistes possibles tout en conservant une certaine diversit\u00e9 dans les distributions de sortie. Pour cela nous emploierons une architecture DCGAN, qui utilise des r\u00e9seaux de neurones convolutionnels (CNNs) pour le g\u00e9n\u00e9rateur et le discriminateur.    \n", "\n", "\n", " \n", "\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "NC19uHxEQzhg"}, "outputs": [], "source": ["import numpy as np\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torchvision\n", "import torchvision.transforms as transforms\n", "import torch.utils.data\n", "import torchvision.datasets as dset\n", "import torchvision.utils as vutils\n", "import torch.backends.cudnn as cudnn\n", "\n", "import datasets\n", "import os"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["cache_dir = os.environ.get(\"TORCHVISION_CACHE\", os.path.expanduser(\"$HOME/.cache/torchvision\"))\n", "cache_dir"]}, {"cell_type": "markdown", "metadata": {"id": "RiS5NSCjj1N2"}, "source": ["Le code ci-dessous permet de d\u00e9clarer la mani\u00e8re de charger les donn\u00e9es. \n", "\n", "Lorsque des donn\u00e9es sont demand\u00e9es (pour la construction d'un nouveau batch par exemple), une s\u00e9rie de transformations est appliqu\u00e9e sur les images, selon la composition de transformateurs d\u00e9clar\u00e9e pour le chargement: \n", "*    redimentionnement des images en 64 par 64\n", "*    recadrage au centre (qui ne fait rien ici car image d\u00e9j\u00e0 dans la taille du cadre mais si utile pour d'autres param\u00e8tres)\n", "*    conversion en tenseur pytorch \n", "*    normalisation des valeurs RGB selon une moyenne de 0.5 et un ecart-type de 0.5.\n", "\n"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"id": "IfkUPrmhi7EE"}, "outputs": [], "source": ["image_size = 64\n", "dataset = dset.CelebA(root=cache_dir,\n", "                           transform=transforms.Compose([\n", "                               transforms.Resize(image_size),\n", "                               transforms.CenterCrop(image_size),\n", "                               transforms.ToTensor(),\n", "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n", "                           ]), download=True)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "vRoYyNTSjs5e", "outputId": "57a081fe-147d-4a0a-cb7a-64b7d1878bb7"}, "outputs": [], "source": ["print(dataset)"]}, {"cell_type": "markdown", "metadata": {"id": "cpHar5v3mPo9"}, "source": ["Le code ci-dessous permet de d\u00e9clarer la mani\u00e8re de charger les images et en affiche un \u00e9chantillon. "]}, {"cell_type": "code", "execution_count": 10, "metadata": {"id": "Alkf-VTNmNrz", "outputId": "72adbbbd-20ca-4197-adf6-bc4956b6eda7"}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import os\n", "\n", "\n", "\n", "seed=0\n", "torch.manual_seed(seed)\n", "np.random.seed(seed)\n", "device=0\n", "if device>=0 and torch.cuda.is_available():\n", "  cudnn.benchmark = True\n", "  torch.cuda.device(device)\n", "  torch.cuda.manual_seed(seed)\n", "else: \n", "  device=-1\n", "\n", "batch_size = 128\n", "workers = 2\n", "\n", "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n", "                                         shuffle=True, num_workers=workers)\n", "\n", "# Affichage de quelques images\n", "real_batch = next(iter(dataloader)) #real_batch est une liste de 2 tenseurs o\u00f9 le 1er correspond aux images, les second correspond aux labels (ici 0 partout)\n", "\n", "plt.figure(figsize=(15,15))\n", "plt.axis(\"off\")\n", "plt.title(\"Training Images\")\n", "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=1, normalize=True).cpu(),(1,2,0)))\n", "os.makedirs(\"drive/My Drive/genFaces\",exist_ok=True)\n", "plt.savefig(\"drive/My Drive/genFaces/train.png\" ) # Pour sauvegarder l'image sur votre Google Drive \n"]}, {"cell_type": "markdown", "metadata": {"id": "8ssBMv7C1HL7"}, "source": ["Le r\u00e9seau $D$ est un empilement de couches de convolution 2D avec batchNorm2D et activations RELU: "]}, {"cell_type": "code", "execution_count": 11, "metadata": {"id": "cn-rGKpEaYtE", "outputId": "e8cc8a62-924f-4b31-a1f5-c800f2b0a302"}, "outputs": [], "source": ["\n", "nc = 3 # Nombre de canaux de l'entr\u00e9e\n", "ndf = 64 # Facteur du nombre de canaux de sortie des diff\u00e9rentes couches de convolution\n", "\n", "\n", "# Initialisation recommandee pour netG et netD dans DCGAN\n", "def weights_init(m):\n", "    classname = m.__class__.__name__\n", "    if classname.find('Conv') != -1:\n", "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n", "    elif classname.find('BatchNorm') != -1:\n", "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n", "        nn.init.constant_(m.bias.data, 0)\n", "\n", "class Discriminator(nn.Module):\n", "    def __init__(self):\n", "        super(Discriminator, self).__init__()\n", "        self.main = nn.Sequential(\n", "            # input is (nc) x 64 x 64\n", "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n", "            nn.LeakyReLU(0.2, inplace=True),\n", "            # state size. (ndf) x 32 x 32\n", "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n", "            nn.BatchNorm2d(ndf * 2),\n", "            nn.LeakyReLU(0.2, inplace=True),\n", "            # state size. (ndf*2) x 16 x 16\n", "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n", "            nn.BatchNorm2d(ndf * 4),\n", "            nn.LeakyReLU(0.2, inplace=True),\n", "            # state size. (ndf*4) x 8 x 8\n", "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n", "            nn.BatchNorm2d(ndf * 8),\n", "            nn.LeakyReLU(0.2, inplace=True),\n", "            # state size. (ndf*8) x 4 x 4\n", "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n", "            nn.Sigmoid()\n", "        )\n", "\n", "    def forward(self, input):\n", "        return self.main(input)\n", "\n", "\n", "netD = Discriminator().to(device)\n", "netD.apply(weights_init)\n", "print(netD)"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"id": "1jP0pKRVBKAt", "outputId": "a6484e78-ec1f-4d11-fae3-72c44ffe5415"}, "outputs": [], "source": ["nz=100  #Taille du vecteur z donn\u00e9 en entr\u00e9e du g\u00e9n\u00e9rateur\n", "ngf = 64 # Facteur du nombre de canaux de sortie des diff\u00e9rentes couches de deconvolution\n", "\n", "class Generator(nn.Module):\n", "    def __init__(self):\n", "        super(Generator, self).__init__()\n", "        self.main = nn.Sequential(\n", "            # input is Z, going into a convolution\n", "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n", "            nn.BatchNorm2d(ngf * 8),\n", "            nn.ReLU(True),\n", "            # state size. (ngf*8) x 4 x 4\n", "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n", "            nn.BatchNorm2d(ngf * 4),\n", "            nn.ReLU(True),\n", "            # state size. (ngf*4) x 8 x 8\n", "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n", "            nn.BatchNorm2d(ngf * 2),\n", "            nn.ReLU(True),\n", "            # state size. (ngf*2) x 16 x 16\n", "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n", "            nn.BatchNorm2d(ngf),\n", "            nn.ReLU(True),\n", "            # state size. (ngf) x 32 x 32\n", "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n", "            nn.Tanh()\n", "            # state size. (nc) x 64 x 64\n", "        )\n", "\n", "    def forward(self, input):\n", "        return self.main(input)\n", "netG = Generator().to(device)\n", "netG.apply(weights_init)\n", "print(netG)"]}, {"cell_type": "markdown", "metadata": {"id": "SB5h3eyywZ5L"}, "source": ["Donner la proc\u00e9dure d'entra\u00eenement de ces deux r\u00e9seaux. L'optimisation se fera ADAM selon les deux co\u00fbts adverses du discriminateur et du g\u00e9n\u00e9rateur. Pour chaque nouveau batch d'images, on alterne les deux mises \u00e0 jour suivantes, selon un batch de vecteurs $z$ tir\u00e9s al\u00e9atoirement selon une loi normale centr\u00e9e r\u00e9duite (un nouveau batch de $z$ \u00e0 chaque it\u00e9ration): \n", "\n", "1.   Un pas de gradient sur les param\u00e8tres du r\u00e9seau D pour maximiser:  $log(D(x)) + log(1 - D(G(z)))$\n", "2.   Un pas de gradient sur les param\u00e8tres du r\u00e9seau G pour maximiser:  $log(D(G(z)))$ \n", "\n", "\n", "Afin de suivre l'\u00e9volution de l'apprentissage, on pourra logguer l'erreur du discriminateur relev\u00e9e en 1, l'erreur du g\u00e9n\u00e9rateur relev\u00e9e en 2, la moyenne des sorties du discriminateur sur les images r\u00e9elles et la moyenne des sorties du discriminateur sur les images g\u00e9n\u00e9r\u00e9es.\n", "\n", "\u00c0 la fin de chaque \u00e9poque (i.e., lorsque l'on a it\u00e9r\u00e9 sur tous les batchs du DataLoader), on pourra enregistrer les images g\u00e9n\u00e9r\u00e9es \u00e0 partir d'un batch de vecteurs $z$ fixe dans le Google Drive pour observer l'\u00e9volution des capacit\u00e9s du g\u00e9n\u00e9rateur.  \n", "\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"id": "q1PIyW0Vs13s", "outputId": "32c6784f-3b0a-4652-c757-29043551cb24"}, "outputs": [], "source": ["# \u00c0 compl\u00e9ter...  \n", "assert False, 'Code non impl\u00e9ment\u00e9'\n", "####################################[ \n", "        ##########################\n", "        #########################\n", "        Train with all-real batch\n", "        Train with all-fake batch\n", "        ##########################\n", "        #########################\n", "########################################################################################################]\n"]}, {"cell_type": "markdown", "metadata": {"id": "HPKl9QI1_7fZ"}, "source": ["Le code ci-dessous applique votre r\u00e9seau \u00e0 un batch de $z$ al\u00e9atoires et affiche les images g\u00e9n\u00e9r\u00e9es (et enregistre dans fake.png). "]}, {"cell_type": "code", "execution_count": 14, "metadata": {"id": "UFbL5Es00IEL", "outputId": "e5e1426f-b2bf-4da9-d059-6019a84d66a7"}, "outputs": [], "source": ["noise = torch.randn(64, nz, 1, 1, device=device)\n", "with torch.no_grad():\n", "  netG.eval()\n", "  fake = netG(noise).detach().cpu()\n", "img=vutils.make_grid(fake, padding=2, normalize=True)\n", "img_list.append(img)\n", "plt.figure(figsize=(15,15))\n", "plt.axis(\"off\")\n", "plt.title(\"Fake Images\")\n", "plt.imshow(np.transpose(img.cpu(),(1,2,0)))\n", "plt.savefig( \"fake.png\" )\n"]}, {"cell_type": "markdown", "metadata": {"id": "zqb9k8hAUxFN"}, "source": ["# Bonus: G\u00e9n\u00e9rateur\n", "\n", "Le g\u00e9n\u00e9rateur du papier original DCGAN poss\u00e8de en fait l'architecture suivante: \n", "\n", "![Generator](https://pytorch.org/tutorials/_images/dcgan_generator.png)\n", "\n", "Comme le r\u00e9seau $G$ d\u00e9finit plus haut, il correspond \u00e0 un empilement de couches de convolutions transpos\u00e9es (appel\u00e9e dans certains papiers couches de d\u00e9convolution). Contrairement aux convolutions classiques qui m\u00e8nent \u00e0 une r\u00e9duction de la taille des sorties, les convolutions transpos\u00e9es agrandissent les cartes de caract\u00e9ristiques consid\u00e9r\u00e9es (feature maps). C'est particuli\u00e8rement adapt\u00e9 pour de la g\u00e9n\u00e9ration d'images \u00e0 partir d'un code de petite taille (ici $z$). \n", "\n", " Pour comprendre comment fonctionne la convolution transpos\u00e9e, voici un exemple simple avec une entr\u00e9e 2 x 2 et un noyau 2 x 2. Chaque \u00e9l\u00e9ment de l'entr\u00e9e (4 \u00e9l\u00e9ments) est multipli\u00e9 par le noyau et le r\u00e9sultat est ajout\u00e9 \u00e0 la sorte de taille 3 x 3: \n", "\n", "![BasicTransposeConv2D](https://d2l.ai/_images/trans_conv.svg)\n", "\n", "Et voici deux animations pour se repr\u00e9senter l'op\u00e9ration d'une mani\u00e8re plus g\u00e9n\u00e9rale. A gauche on utilise un stride de 1, \u00e0 droite un stride de 2: \n", "\n", "<p align=\"center\">\n", "<img src=\"https://i.stack.imgur.com/YyCu2.gif\">\n", "<img src=\"https://i.stack.imgur.com/f2RiP.gif\">\n", "</p>\n", "\n", "\n", "Suivant la doc Pytorch de torch.nn.ConvTranspose2d, la hauteur $H_{out}$ et la largeur $W_{out}$ des cartes de sortie du ConvTranspose2d peuvent se calculer de la mani\u00e8re suivante: \n", "\n", "$H_{out}$=($H_{in}$\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1\n", "\n", "$W_{out}$=($W_{in}$\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1\n", "\n", "\n", "Proposer un nouveau r\u00e9seau $G$ qui respecte l'architecture du sch\u00e9ma du papier DCGAN et comparer les r\u00e9sultats. On gardera le param\u00e8tre de dilation \u00e0 sa valeur de 1 par d\u00e9faut mais il est possible de moduler les valeurs de padding et output_padding pour obtenir des sorties de la taille d\u00e9sir\u00e9e. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "vfr9mDSlIJ02"}, "outputs": [], "source": ["# \u00c0 compl\u00e9ter...  \n", "assert False, 'Code non impl\u00e9ment\u00e9'\n", "####################################[ \n", "##################################################################]\n"]}, {"cell_type": "markdown", "metadata": {"id": "SvbeOuRJEYZR"}, "source": ["# Bonus\n", "\n", "R\u00e9aliser le m\u00eame genre d'apprentissage sur le corpus Mnist (Dataloader existant dans torch pour t\u00e9l\u00e9charger et charger le corpus)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "TME9_RLD_correction.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.6"}}, "nbformat": 4, "nbformat_minor": 4}
