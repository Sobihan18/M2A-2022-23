{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Introduction to machine learning\n", "\n", "*Maxime Sangnier*\n", "\n", "Fall, 2022\n", "\n", "## Practical session 3: nonparametric and ensemble methods"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Table of contents\n", "1. [Nonparametric methods](#part1)\n", "    - [k-nearest neighbors](#part1sec1)\n", "    - [Decision trees](#part1sec2)\n", "1. [Ensemble methods](#part2)\n", "    - [Bagging](#part2sec1)\n", "1. [Random forests](#part3)\n", "1. [Model selection](#part4)\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Packages:\n", "\tnympy as np\n", "\tmatplotlib.pyplot as plt\n", "\tseaborn as sns\n", "\n", "Functions:\n", "\tplotXY\n", "\tplot_frontiere\n", "\tmap_regions\n", "\tcovariance\n", "\tplot_cov\n", "\tsample_gmm\n", "\tscatter\n", "\tplot_level_set\n", "\tgaussian_sample\n", "\n"]}], "source": ["from mllab import *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Nonparametric methods <a id=\"part1\"></a>\n", "## k-nearest neighbors <a id=\"part1sec1\"></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [">Given a training dataset $\\{(X_1, Y_1), \\dots, (X_n, Y_n)\\}$, the k-nearest neighbors rule consists in predicting, for $x \\in \\mathbb R^d$, the majority vote (for classification, or the mean for regression) of the k-nearest neighbors of $x$.\n", "Formally, the predicted class is:\n", "$$\n", "    g(x) \\in \\operatorname{arg\\,max}_{y \\in \\mathcal Y} \\sum_{i=1}^k \\mathbb 1_{Y_{(i)}=y},\n", "$$\n", "where the ranked labeled $\\{Y_{(1)}, \\dots, Y_{(n)}\\}$ are such that $\\|X_{(1)}-x\\| \\le \\dots \\le \\|X_{(n)}-x\\|$.\n", "\n", ">Given the following dataset, train a [k-nearest neighbors classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and display the data along with the classification regions.\n", "\n", ">What is the default metric?"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_classification\n", "\n", "X, y = make_classification(n_samples=500, n_classes=4,\n", "                           n_features=2, n_redundant=0, n_clusters_per_class=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Repeat this experiment while making the number of neareset neighbors vary.\n", "Display the results on several subplots with the classification score indicated in the title.\n", "\n", ">What appens in the extreme situations where the number of nearest neighbors is either $1$ or $n$ (the size of the training set)?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">For a point $x \\in \\mathbb R^d$, the predicted class is given by the majority vote\n", "$$\n", "    g(x) \\in \\operatorname{arg\\,max}_{y \\in \\mathcal Y} \\sum_{i=1}^k \\mathbb 1_{Y_{(i)}=y},\n", "$$\n", "where the ranked labeled $\\{Y_{(1)}, \\dots, Y_{(n)}\\}$ are such that $\\|X_{(1)}-x\\| \\le \\dots \\le \\|X_{(n)}-x\\|$.\n", "\n", ">We would like, for a while, to weight the vote of each neighbor in the prediction by $e^{-\\gamma \\|X_{(j)}-x\\|^2}$, such that the new classification rule is\n", "$$\n", "    g_\\gamma(x) \\in \\operatorname{arg\\,max}_{y \\in \\mathcal Y} \\sum_{i=1}^k e^{-\\gamma \\|X_{(i)}-x\\|^2} \\mathbb 1_{Y_{(i)}=y}.\n", "$$\n", "\n", ">Again with subplots and score prints, assess the impact of the parameter $\\gamma$.\n", "For this purpose, you may want to define the `weights` parameter of the [k-nearest neighbors classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) to $x \\mapsto e^{-\\gamma x^2}$, using a lambda function.\n", "\n", ">Relevant values for $\\gamma$ are $\\{10^{-3}, \\dots, 10^4\\}$ and `n_neighbors` can be set to $10$."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Using the [train\\_test\\_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function, split the dataset into a training and a test set with ratio $0.2-0.8$.\n", "Plot the test accuracy with respect to the number of neighbors."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Repeat the random split 20 times and plot the mean and the variance of the test accuracy.\n", "What can you say about the variance of this estimator?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Let us consider the last random split.\n", "Using the [crossval\\_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function, find (and print) a good value for the number of nearest neighbors.\n", "\n", ">For this parameter, compare the crossvalidation score and the test accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Plot the confusion matrix for the best classifier obtained."]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["import itertools\n", "from sklearn.metrics import confusion_matrix\n", "\n", "def plot_confusion_matrix(y_pred, y, classes=None, normalize=False):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    title='Confusion matrix'\n", "    cmap=plt.cm.Blues\n", "    \n", "    cm = confusion_matrix(y, y_pred)\n", "    \n", "    if classes is None:\n", "        classes = np.unique(y)\n", "    \n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "        title = 'Normalized confusion matrix'\n", "    else:\n", "        title = 'Unnormalized confusion matrix'\n", "\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45)\n", "    plt.yticks(tick_marks, classes)\n", "\n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, format(cm[i, j], fmt),\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.tight_layout()\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')\n", "    plt.grid()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Decision trees <a id=\"part1sec2\"></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [">Decision trees, and in particular classification and regression trees (CART), are supervised estimators introduced by Leo Breiman et al.\n", "The paradigm of a binary decision tree is to recursively split the space $\\mathcal X$ with simple rules such that: is the explicative variable $x_j$ greater than the threshold $\\tau$ or not?\n", "Doing so, a decision tree is built, for which each node corresponds to a simple rule (and secondarly to a partition cell of $\\mathcal X$).\n", "The final result is a partition of $\\mathcal X$ by hypercubes.\n", "\n", ">At each step of the learning algorithm, \n", ">1. consider the partition $\\mathcal P = \\{\\mathcal X\\}$;\n", ">1. for each cell $\\mathcal A$ of $\\mathcal P$, define the two-cell partition\n", "$\\mathcal A = \\mathcal L_{j, \\tau} \\cup \\mathcal R_{j, \\tau}$, where $j \\in [d]$ is a feature index and $\\tau \\in \\mathbb R$ is a threhold, and\n", "$$\n", "    \\begin{cases}\n", "        \\mathcal L_{j, \\tau} = \\left\\{ x \\in \\mathcal A :  x_j \\le \\tau \\right\\}\\\\\n", "        \\mathcal R_{j, \\tau} = \\left\\{ x \\in \\mathcal A :  x_j > \\tau \\right\\}\n", "        %= \\mathcal A \\backslash \\mathcal L_{j, \\tau}.\n", "    \\end{cases}\n", "$$\n", "are the \"left\" and \"right\" parts of $\\mathcal A$.\n", "Then, find the best pair (feature, threhold) for splitting:\n", "$$\n", "    (j, \\tau) \\in \\operatorname{arg\\,min}_{1 \\le j \\le d \\atop \\tau \\in \\mathbb R}\n", "    \\frac{\\left| \\mathcal L_{j, \\tau} \\right|}{\\left| \\mathcal A \\right|} D(\\mathcal L_{j, \\tau}) +\n", "    \\frac{\\left| \\mathcal R_{j, \\tau} \\right|}{\\left| \\mathcal A \\right|} D(\\mathcal R_{j, \\tau})\n", "$$\n", "where $D$ is a distortion measure for a cell (see below);\n", ">1. replace $\\mathcal A$ by $\\mathcal L_{j, \\tau}$ and $\\mathcal R_{j, \\tau}$ in the partition $\\mathcal P$;\n", ">1. go to 2.\n", "\n", ">Given a cell $\\mathcal A$, one may define the ratio of observations of $\\mathcal A$ of class $y \\in \\mathcal Y$:\n", "$$\n", "    p_y(\\mathcal A) = \\frac{\\left| \\left\\{ i \\in [n] : X_i \\in \\mathcal A, Y_i=y \\right\\} \\right|}{\\left| \\mathcal A \\right|}.\n", "$$\n", "\n", ">Then, the distortion of the cell $\\mathcal A$ may be:\n", ">- Gini impurity: $D(\\mathcal A) = \\sum_{y \\in \\mathcal Y} p_y(\\mathcal A) (1-p_y(\\mathcal A))$ (classification);\n", ">- entropy: $D(\\mathcal A) = - \\sum_{y \\in \\mathcal Y} p_y(\\mathcal A) \\log(p_y(\\mathcal A))$ (classification);\n", ">- mean squared error: $D(\\mathcal A) = \\frac{1}{\\left| \\mathcal A \\right|}\\sum_{1 \\le i \\le n \\atop X_i \\in \\mathcal A} \\left( Y_i - \\bar Y_{\\mathcal A} \\right)^2$, with $\\bar Y_{\\mathcal A} = \\frac{1}{\\left| \\mathcal A \\right|}\\sum_{1 \\le i \\le n \\atop X_i \\in \\mathcal A} Y_i$ (regression).\n", "\n", ">For regression, Jerome Friedman suggested an improved criterion (in its original paper tackling gradient boosting), referred to as Friedman's mean squared error:\n", "$$\n", "    (j, \\tau) \\in \\operatorname{arg\\,min}_{1 \\le j \\le d \\atop \\tau \\in \\mathbb R}\n", "    \\frac{ \\left| \\mathcal L_{j, \\tau} \\right| \\left| \\mathcal R_{j, \\tau} \\right| }{ \\left| \\mathcal L_{j, \\tau} \\right| + \\left| \\mathcal R_{j, \\tau} \\right| }\n", "    \\left( \\bar Y_{\\mathcal L_{j, \\tau}} - \\bar Y_{\\mathcal R_{j, \\tau}} \\right)^2.\n", "$$\n", "\n", ">Last be not least, several stopping rules are of interests:\n", ">- maximal depth of the tree;\n", ">- minimal number of observations required to split an internal node;\n", ">- minimal number of observations required to be at a leaf node;\n", ">- maximal number of leaf nodes.\n", "\n", ">We would like to assess the accuracy of a [classification tree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) with respect to the kind of splitting criterion (Gini impurity or entropy) and to the maximal depth allowed.\n", "\n", ">For this purpose, fit a [classification tree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on the previous dataset and plot 4 curves depicting the classification error for:\n", ">1. the train set and the Gini impurity;\n", ">1. the test set and the Gini impurity;\n", ">1. the train set and the entropy;\n", ">1. the test set and the entropy.\n", "\n", ">What can you conclude based on this numerical experiment?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Map the regions of the \"best\" (with respect to the test error) decision tree obtained with the Gini impurity."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ensemble methods <a id=\"part2\"></a>\n", "## Bagging <a id=\"part2sec1\"></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [">Bagging is a portmanteau word for *bootstrap aggregating*.\n", "The paradigm of bagging is to train independently several base classifiers $(g_1, \\dots, g_T)$, with $g_t \\colon \\mathbb R^d \\to \\{\\pm 1\\}$, and to build a new classifier by averaging the predictions of the base classifiers:\n", "$$\n", "    g_n^T(x) = \\operatorname{sign} \\left( \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\right).\n", "$$\n", "Doing so, the variance of the prediction is reduced and so it is for the global error.\n", "The requirements for such a result are:\n", ">- base classifiers should be more accurate than chance;\n", ">- base classifiers should be estimated independently from each other.\n", "\n", ">In practice, base classifiers are trained *quasi-independently* by bootstrapping the training set.\n", "\n", ">Bagging is also valid for multiclass problems: for $C$ classes, the prediction is:\n", "$$\n", "    g_n^T(x)\n", "    = \\operatorname{arg\\,max}_{1 \\le j \\le C} \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\mathbb 1_{g_t(x)=j}\n", "    = \\operatorname{arg\\,max}_{1 \\le j \\le C} \\operatorname{card} \\left( \\left\\{ t \\in [T] : g_t(x) \\mathbb 1_{g_t(x)=j} \\right\\} \\right),\n", "$$\n", "where $g_t \\colon \\mathbb R^d \\to [C]$, which corresponds to the majority vote since base classifiers are equally weighted.\n", "\n", ">Finally, one may also bag regressors $g_t \\colon \\mathbb R^d \\to \\mathbb R$ by a simple averaging:\n", "$$\n", "    g_n^T(x) = \\frac{1}{T} \\sum_{t=1}^T g_t(x).\n", "$$\n", "\n", ">Assume that we are provided with a sequence of independent classifiers $(g_1, \\dots, g_T)$, with $g_t \\colon \\mathbb R^d \\to \\{\\pm 1\\}$, such that classifiers are equally good: there exists $p>0.5$ such that $\\mathbb P(g_t(X)=Y) = p$ for all $t \\in [T]$.\n", "We now consider the bagged classifier\n", "$$\n", "    g_n^T(x) = \\operatorname{sign} \\left( \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\right).\n", "$$\n", "What is the distribution of the random variable $\\sum_{t=1}^T \\mathbb 1_{g_t(X)=Y}$?\n", "Plot its probability mass function for $T=9$ and $p=0.7$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:**\n", "\u2026"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">For these particular values of $T$ and $p$, compute numerically $\\mathbb P(g_n^T(X)=Y)$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:**\n", "\u2026"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Plot the probability of success (or accuracy) with respect to the number of base classifiers $T \\in \\{1, 2, 3, \\dots, 49\\}$ (the formula used previously is only valid for odd numbers) for $p \\in [0.55, 0.65, \\dots, 0.95]$."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Complete the following script to implement bagging with regression trees."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "from sklearn.tree import DecisionTreeRegressor\n", "class BaggingTree(object):\n", "    def __init__(self, n_estimators=10, max_depth=1, max_samples=1.0):\n", "        \"\"\"\n", "        Parameters:\n", "        - n_estimators: number of estimators\n", "        - max_depth: maximal depth of the regressor tree\n", "        - max_samples: ratio of samples to use for learning base regressors.\n", "            - If max_samples=1.0: use bootstrap.\n", "            - If max_samples<1.0: use random sampling and extract max_samples x n points\n", "            (where n is the total numer of points).\n", "        \"\"\"\n", "        self.n_estimators = n_estimators\n", "        self.max_depth = max_depth\n", "        self.max_samples = max_samples\n", "        \n", "    def fit(self, X, y):\n", "        # Make X a 2d-array\n", "        X = np.asarray(X)\n", "        if X.ndim == 1:\n", "            X = X[:, np.newaxis]\n", "            \n", "        n = X.shape[0]  # Sample size\n", "        m = int(self.max_samples * n)  # Number of points for random sampling\n", "        \n", "        self.estimators_ = []\n", "        # Build estimators\n", "        # Todo\n", "\n", "        # End todo\n", "        return self\n", "    \n", "    def predict(self, X):\n", "        # Make X a 2d-array\n", "        X = np.asarray(X)\n", "        if X.ndim == 1:\n", "            X = X[:, np.newaxis]\n", "        \n", "        # Compute predictions\n", "        # Todo\n", "\n", "        # End todo\n", "        return predictions\n", "    \n", "    def error(self, X, y):\n", "        # Make X a 2d-array\n", "        X = np.asarray(X)\n", "        if X.ndim == 1:\n", "            X = X[:, np.newaxis]\n", "        return np.sum((y - self.predict(X))**2)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Apply bagging to the following regression dataset and plot (on the same figure), the training data and the prediction for test data."]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"data": {"text/plain": ["<matplotlib.collections.PathCollection at 0x7f113e2c1400>"]}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": ["# Regression dataset\n", "n = 100\n", "X_train = np.sort(5 * np.random.rand(n))\n", "y_train = np.sin(X_train)\n", "y_train[::5] += 1 * (0.5 - np.random.rand(n//5))\n", "\n", "X_test = np.arange(0, 5, step=1e-2)\n", "y_test = np.sin(X_test)\n", "\n", "# Make 2d-arrays\n", "X_train = X_train[:, np.newaxis]\n", "X_test = X_test[:, np.newaxis]\n", "\n", "plt.scatter(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Analyze the behavior of the prediction curve and of the test error with respect to the maximal depth of decision trees and to the number of base regressors."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">We consider decision trees with `max_depth = 5`.\n", "Plot two curves (one with bootstrap and one with 25%-subsampling) showing the test error with respect to the number of base regressors.\n", "What can we conclude?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Random forests <a id=\"part3\"></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [">Random forests are bagged trees: for binary classification, a random forest is\n", "$$\n", "    g_n^T(x) = \\operatorname{sign} \\left( \\frac{1}{T} \\sum_{t=1}^T g_t(x) \\right),\n", "$$\n", "where the base classifiers $(g_1, \\dots, g_T)$, with $g_t \\colon \\mathbb R^d \\to \\{\\pm 1\\}$, are learned quasi-independently by bootstrap.\n", "\n", ">However, in order to enforce the independent learning, each decision tree $g_t$ owns an additional randomization step in its learning procedure:\n", "1. at each cell, select a subset of features at random;\n", "1. find the best pair (feature, threshold) for splitting.\n", "\n", ">The following script loads and preprocesses the [diabetes dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html).\n", "Explain each step and indicate its purpose.\n", "\n", ">Is it useful for decision trees?"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_diabetes\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "data = load_diabetes()\n", "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n", "\n", "scaler = StandardScaler()\n", "X_train = scaler.fit_transform(X_train)\n", "X_test = scaler.transform(X_test)\n", "\n", "y_train = scaler.fit_transform(y_train.reshape(-1, 1))[:, 0]\n", "y_test = scaler.transform(y_test.reshape(-1, 1))[:, 0]"]}, {"cell_type": "markdown", "metadata": {}, "source": [">On the [diabetes dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html), compare scikit-learn [bagging](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) (with bootstrap and 25%-subsampling) and [random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n", "For this purpose, consider `max_depth = 5` and plot three curves showing the regression score for the test set with respect to the number of base regressors.\n", "What can we conclude?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Model selection <a id=\"part4\"></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [">Load the [digits classification dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits), split it randomly and preprocess it similarly to the diabetes dataset (be careful, we are now handling a classification dataset)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">We aim at comparing three models based on the [digits classification dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits):\n", "1. [gradient boosting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html);\n", "1. [random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html);\n", "1. [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n", "\n", ">For each model, some possible values of parameters are defined below.\n", "Explain the role of these parameters."]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["# Grid parameters\n", "tree_params = {\"max_depth\": [5, 10, 15],\n", "              \"n_estimators\": [10, 100]}  # Parameters for random forests\n", "gb_params = tree_params.copy()  # Parameters for gradient boosting\n", "gb_params.update({\"learning_rate\": np.logspace(-2, 0, num=3),\n", "                 \"max_features\": [0.1]})\n", "lr_params = {\"C\": np.logspace(-3, 3, num=7)}  # Parameters for logistic regression"]}, {"cell_type": "markdown", "metadata": {}, "source": [">With the training set previously generated, perform a [grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) in order to determine \"good\" parameters among the ones proposed above.\n", "Compare the test scores of the models trained with the \"best\" parameters."]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["# Auxiliary parameters\n", "gs_params = {\"cv\": 3, \"n_jobs\": -1, \"verbose\": 1}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">The general pattern of this numerical experiment is:\n", "1. spliting randomly the dataset;\n", "1. preprocessing it;\n", "1. finding suitable parameters by grid search;\n", "1. assessing the generalization error.\n", "\n", ">Repeat this experiment 20 times and record the score for each method and each run.\n", "Print the mean score and the standard deviation for each method."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">Perform a [Wilcoxon signed-rank test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html) to assess the difference between the mean scores of Gradient boosting and random forests."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}