{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Introduction \u00e0 l'apprentissage automatique\n", "\n", "*Maxime Sangnier*\n", "\n", "Octobre, 2022\n", "\n", "## Contr\u00f4le continu"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Table of contents\n", "1. [Consignes](#part1)\n", "1. [*Instructions*](#part2)\n", "1. [Exercice](#part3)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center style=\"color:green; font-weight:bold; font-size:18px\">\n", "INDIQUEZ VOTRE NOM ICI / <i>WRITE YOUR NAME HERE</i>\n", "</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Consignes <a id=\"part1\"></a>\n", "\n", "1. Vos r\u00e9ponses doivent se trouver dans les cellules de **ce fichier**.\n", "1. En fin d'\u00e9preuve, **red\u00e9marrez le noyau et relancez toutes les cellules**, puis **exportez** votre notebook en format `html`.\n", "1. **T\u00e9l\u00e9versez** ensuite les fichiers `ipynb` **et** `html` dans ce dossier distant : [https://www.dropbox.com/request/oPQXE75uYGUWHtENZvqK](https://www.dropbox.com/request/oPQXE75uYGUWHtENZvqK).\n", "1. La dur\u00e9e de l'\u00e9preuve est **1h**. Tous les documents sont autoris\u00e9s.\n", "\n", "Si vous n\u2019arrivez pas \u00e0 faire une question, vous pouvez **utiliser les \u00e9l\u00e9ments donn\u00e9s et continuer** l'exercice.\n", "\n", "---\n", "\n", "# *Instructions* <a id=\"part2\"></a>\n", "\n", "1. *You should answer in **this file**.*\n", "1. *At the end, **restart the kernel and run all cells**, then **export** the notebook to `html`.*\n", "1. *.**Upload** the `ipynb` **and** `html` files to [https://www.dropbox.com/request/oPQXE75uYGUWHtENZvqK](https://www.dropbox.com/request/oPQXE75uYGUWHtENZvqK).*\n", "1. *The exam is **1 hour** long. All documents are allowed.*\n", "\n", "*If you do not succeed at a question, you can **use the hints given in order to answer the next questions**.*"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from seaborn import set_theme\n", "set_theme()\n", "from scipy.optimize import minimize"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercice <a id=\"part3\"></a>\n", ">Soient $n \\in \\mathbb N^*$, $A \\in \\mathbb R^{n \\times n}$ une matrice sym\u00e9trique et semi-d\u00e9finie positive et $b \\in \\mathbb R^n$.\n", "On suppose que $A_{i,i} \\neq 0$ pour tout $i \\in \\{1, \\dots, n\\}$ et on s'int\u00e9resse \u00e0 la fonction $$f : x \\in \\mathbb R^n \\mapsto \\frac 12 x^\\top A x + b^\\top x.$$\n", ">\n", "> ---\n", ">\n", ">*Let $n \\in \\mathbb N^*$, $A \\in \\mathbb R^{n \\times n}$ be a symmetric and positive semi-definite matrix and $b \\in \\mathbb R^n$.\n", "It is assumed that $A_{i,i} \\neq 0$ for every $i \\in \\{1, \\dots, n\\}$.\n", "We are interested in the function $$f : x \\in \\mathbb R^n \\mapsto \\frac 12 x^\\top A x + b^\\top x.$$*"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["n = 5\n", "A = np.random.randn(n, n)\n", "A = A@A.T\n", "b = np.random.randn(n)"]}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 1.**\n", "Soient $x \\in \\mathbb R^n$ et $i \\in \\{1, \\dots, n\\}$.\n", "Afficher le graphe de la fonction\n", "$$\n", "f_{i-1}^x : u \\in \\mathbb R \\mapsto f \\left(\\begin{bmatrix} x_1\\\\ \\vdots\\\\ x_{i-1}\\\\ u\\\\ x_{i+1}\\\\ \\vdots\\\\ x_n \\end{bmatrix}\\right),\n", "$$\n", "i.e. de $f$ selon la seule coordonn\u00e9e $x_i$.\n", ">\n", "> ---\n", ">\n", ">*Let $x \\in \\mathbb R^n$ and $i \\in \\{1, \\dots, n\\}$.\n", "Display the graph of the function \n", "$$\n", "f_{i-1}^x : u \\in \\mathbb R \\mapsto f \\left(\\begin{bmatrix} x_1\\\\ \\vdots\\\\ x_{i-1}\\\\ u\\\\ x_{i+1}\\\\ \\vdots\\\\ x_n \\end{bmatrix}\\right),\n", "$$\n", "i.e. of $f$ with respect to $x_i$ only.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "x = np.random.randn(n)\n", "i = 2\n", "\n", "# To do\n", "\n", "# End to do"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 2.**\n", "En d\u00e9composant $A$ en colonnes : $A = \\left[ a_1 | \\dots | a_n \\right]$,\n", "on obtient :\n", "$$\n", "\\operatorname{arg\\,min}_{u \\in \\mathbb R} f_{i-1}^x(u) = \\left\\{ x_i - \\frac{a_i^\\top x + b_i}{A_{i,i}} \\right\\}.\n", "$$\n", ">\n", ">D\u00e9finir une fonction `solve_i(A, b, i, x)` (pour `i` variant dans $\\{0, \\dots, n-1\\}$) retournant l'unique lieu de minimum de $f_{x, i}$ (on remarquera qu'avec le jeu d'indices Python, qui commencent \u00e0 $0$, et math\u00e9matiques, qui commencent \u00e0 $1$, cela revient \u00e0 impl\u00e9menter **exactement** la formule ci-dessus).\n", ">\n", ">Illustrer le r\u00e9sultat sur un graphique.\n", ">\n", "> ---\n", ">\n", ">*Denoting $a_i$ the columns of $A$: $A = \\left[ a_1 | \\dots | a_n \\right]$,\n", "we get:*\n", "$$\n", "\\operatorname{arg\\,min}_{u \\in \\mathbb R} f_{i-1}^x(u) = \\left\\{ x_i - \\frac{a_i^\\top x + b_i}{A_{i,i}} \\right\\}.\n", "$$\n", ">\n", ">*Define a function `solve_i(A, b, i, x)` (for `i` in $\\{0, \\dots, n-1\\}$) that returns the unique minimizer of $f_{x, i}$ (remark: given that Python indexes start \u00e0 $0$ and that mathematical indexes start \u00e0 $1$, the question asks to implement **exactly** the previous formula).*\n", ">\n", ">*Illustrate the result with a graph.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 3.**\n", "On souhaite maintenant minimiser $f$ via un algorithme de descente coordonn\u00e9e par coordonn\u00e9e :\n", ">- initialiser $x$ \u00e0 $0_{\\mathbb R^n}$ ;\n", ">- it\u00e9rer :\n", "    - choisir al\u00e9atoirement et uniform\u00e9ment une coordonn\u00e9e $i$ dans $\\{1, \\dots, n\\}$ ;\n", "    - mettre \u00e0 jour la $i^e$ coordonn\u00e9e de $x$ : $x_i \\in \\operatorname{arg\\,min}_{u \\in \\mathbb R} f_{i-1}^x(u)$ (i.e. $x_i \\gets x_i - \\frac{a_i^\\top x + b_i}{A_{i,i}}$).\n", ">\n", ">Impl\u00e9menter cette proc\u00e9dure (en it\u00e9rant 1000 fois) et v\u00e9rifier le r\u00e9sultat obtenu en le comparant au lieu de minimum global.\n", ">\n", "> ---\n", ">\n", ">*We would like to minimize $f$ thanks to a coordinate descent algorithm:*\n", ">- *initiaze $x$ at $0_{\\mathbb R^n}$ ;*\n", ">- *iterate:*\n", "    - *sample a coordinate $i$ according to a uniform distribution on $\\{1, \\dots, n\\}$ ;*\n", "    - *update the $i^{th}$ coordinate of $x$ : $x_i \\in \\operatorname{arg\\,min}_{u \\in \\mathbb R} f_{i-1}^x(u)$ (i.e. $x_i \\gets x_i - \\frac{a_i^\\top x + b_i}{A_{i,i}}$).*\n", ">\n", ">*Implement this procedure (with 1000 iterations) and check that the result is correct by comparing it to the global minimizer.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 4.**\n", "Soient $\\{(t_1, y_1), \\dots, (t_n, y_n)\\}$ un jeu de donn\u00e9es de r\u00e9gression univari\u00e9e : $(t_i, y_i) \\in \\mathbb R \\times \\mathbb R$, $i \\in \\{1, \\dots, n\\}$.\n", "On consid\u00e8re le mod\u00e8le :\n", "$$\n", "\\forall i \\in \\{1, \\dots, n\\}, \\quad\n", "y_i = \\phi(t_i)^\\top w_\\text{true} + Z_i,\n", "$$\n", "o\u00f9 $\\phi : x \\in \\mathbb R \\mapsto [1, x, x^2, x^3] \\in \\mathbb R^4$ et $Z_1, \\dots, Z_n \\overset{i.i.d.}{\\sim} \\mathcal N(0, \\sigma^2)$.\n", "Les inconnus sont $w_\\text{true} \\in \\mathbb R^4$ et $\\sigma>0$.\n", "Les observations sont $Y_1, \\dots, Y_n$ et les mesures $t_1, \\dots, t_n$.\n", ">\n", "> ---\n", ">\n", ">*Let $\\{(t_1, y_1), \\dots, (t_n, y_n)\\}$ be a univariate regression dataset: $(t_i, y_i) \\in \\mathbb R \\times \\mathbb R$, $i \\in \\{1, \\dots, n\\}$.\n", "We consider the model:*\n", "$$\n", "\\forall i \\in \\{1, \\dots, n\\}, \\quad\n", "y_i = \\phi(t_i)^\\top w_\\text{true} + Z_i,\n", "$$\n", "*where $\\phi : x \\in \\mathbb R \\mapsto [1, x, x^2, x^3] \\in \\mathbb R^4$ and $Z_1, \\dots, Z_n \\overset{i.i.d.}{\\sim} \\mathcal N(0, \\sigma^2)$.\n", "Unknown parameters are $w_\\text{true} \\in \\mathbb R^4$ and $\\sigma>0$.\n", "Observations are $Y_1, \\dots, Y_n$ and measures are $t_1, \\dots, t_n$.*"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Model\n", "def phi(x):\n", "    return np.c_[np.ones(x.size), x, x**2, x**3]\n", "\n", "n = 10  # Sample size\n", "t = np.random.rand(n)*3 - .5  # Measures\n", "\n", "X = phi(t)  # Design matrix\n", "w_true = np.r_[-.2, 1, 0, -.1]\n", "Y = X@w_true + np.random.randn(n)*.3  # Obersvations\n", "\n", "w_ols = np.linalg.solve(X.T@X, X.T@Y)  # Ordinary least squares estimator"]}, {"cell_type": "markdown", "metadata": {}, "source": [">Repr\u00e9senter sur un graphique les observations $Y_i$ en fonction des mesures $t_i$, la fonction \u00e0 estimer $t \\mapsto \\phi(t)^\\top w_\\text{true}$ et l'estimateur des moindres carr\u00e9s (comme une fonction).\n", ">\n", "> ---\n", ">\n", ">*Display the observations $Y_i$ versus the measures $t_i$, the graph of the function $t \\mapsto \\phi(t)^\\top w_\\text{true}$ to be estimated and the least squares estimator (as a function).*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 5.**\n", "Soient $\\varepsilon \\ge 0$ et $\\lambda>0$.\n", "On souhaite estimer $w_\\text{true}$ en r\u00e9solvant\n", "$$\n", "\\operatorname{minimiser}_{w \\in \\mathbb R^4}~ R(w),\n", "\\quad\\text{avec}\\quad\n", "R(w) = \\frac{\\lambda}{2} \\|w\\|_2^2 + \\frac{1}{2} \\sum_{i=1}^n \\max \\left(0, |y_i-\\phi(t_i)^\\top w|-\\varepsilon \\right)^2.\n", "$$\n", ">\n", ">D\u00e9finir une fonction `loss(x, eps=1.)` retournant la valeur de la fonction de perte utilis\u00e9e et afficher son graphe pour $\\varepsilon=1$.\n", ">\n", "> ---\n", ">\n", ">*Let $\\varepsilon \\ge 0$ and $\\lambda>0$.\n", "We would like to estimate $w_\\text{true}$ by solving*\n", "$$\n", "\\operatorname{minimize}_{w \\in \\mathbb R^4}~ R(w),\n", "\\quad\\text{with}\\quad\n", "R(w) = \\frac{\\lambda}{2} \\|w\\|_2^2 + \\frac{1}{2} \\sum_{i=1}^n \\max \\left(0, |y_i-\\phi(t_i)^\\top w|-\\varepsilon \\right)^2.\n", "$$\n", ">\n", ">*Define a function `loss(x, eps=1.)` that returns the value of the loss function used here  and display its graph for $\\varepsilon=1$.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 6.**\n", "On admet qu'un probl\u00e8me d'optimisation dual au pr\u00e9c\u00e9dent est (en minimisant l'oppos\u00e9 plut\u00f4t qu'en maximisant)\n", "$$\n", "\\operatorname{minimiser}_{\\alpha \\in \\mathbb R_+^n, \\beta \\in \\mathbb R_+^n}~ D(\\alpha, \\beta),\n", "$$\n", "avec\n", "$$\n", "D(\\alpha, \\beta) = \\frac 12 \\alpha^\\top Q \\alpha + \\frac 12 \\beta^\\top Q \\beta + \\alpha^\\top P \\beta + \\alpha^\\top (\\varepsilon \\mathbb 1 - Y) + \\beta^\\top (\\varepsilon \\mathbb 1 + Y),\n", "$$\n", "o\u00f9 on aura fait attention \u00e0 la contrainte de positivit\u00e9 sur chaque variable et o\u00f9 on a not\u00e9\n", "$$\n", "\\begin{cases}\n", "Y = [y_1, \\dots, y_n] \\in \\mathbb R^n\\\\\n", "K = \\left( \\phi(t_i) ^\\top \\phi(t_j) \\right)_{1 \\le i, j \\le n}\\\\\n", "Q = I_n + \\frac{1}{\\lambda} K\\\\\n", "P = I_n - \\frac{1}{\\lambda} K.\n", "\\end{cases}\n", "$$\n", "De plus, les conditions KKT indiquent que si $(\\alpha^\\star, \\beta^\\star)$ est solution du dual, $w^\\star = \\frac{1}{\\lambda} \\sum_{i=1}^n (\\alpha^\\star_i - \\beta^\\star_i) \\phi(t_i)$ est solution du primal.\n", ">On remarquera qu'en notant $X \\in \\mathbb R^{n \\times 4}$ la matrice des donn\u00e9es, dont la $i^e$ ligne est $\\phi(t_i)^\\top$, $K = XX^\\top$ et $w^\\star = \\frac{1}{\\lambda} X^\\top (\\alpha^\\star-\\beta^\\star)$.\n", ">\n", "> ---\n", ">\n", ">*We admit that a dual optimization problem is (minimizing of the opposite value instead of maximizing)*\n", "$$\n", "\\operatorname{minimize}_{\\alpha \\in \\mathbb R_+^n, \\beta \\in \\mathbb R_+^n}~ D(\\alpha, \\beta),\n", "$$\n", "*with*\n", "$$\n", "D(\\alpha, \\beta) = \\frac 12 \\alpha^\\top Q \\alpha + \\frac 12 \\beta^\\top Q \\beta + \\alpha^\\top P \\beta + \\alpha^\\top (\\varepsilon \\mathbb 1 - Y) + \\beta^\\top (\\varepsilon \\mathbb 1 + Y),\n", "$$\n", "*where one has to pay attention to the positivity constraint on each optimization variable and where we have used the definition*\n", "$$\n", "\\begin{cases}\n", "Y = [y_1, \\dots, y_n] \\in \\mathbb R^n\\\\\n", "K = \\left( \\phi(t_i) ^\\top \\phi(t_j) \\right)_{1 \\le i, j \\le n}\\\\\n", "Q = I_n + \\frac{1}{\\lambda} K\\\\\n", "P = I_n - \\frac{1}{\\lambda} K.\n", "\\end{cases}\n", "$$\n", "*In addition, KKT conditions state that if $(\\alpha^\\star, \\beta^\\star)$ is a dual solution, $w^\\star = \\frac{1}{\\lambda} \\sum_{i=1}^n (\\alpha^\\star_i - \\beta^\\star_i) \\phi(t_i)$ is a primal solution.\n", ">Let us remark that, denoting $X \\in \\mathbb R^{n \\times 4}$ the data matrix, the $i^{th}$ row of which is $\\phi(t_i)^\\top$, $K = XX^\\top$ and $w^\\star = \\frac{1}{\\lambda} X^\\top (\\alpha^\\star-\\beta^\\star)$.*"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["alpha obtained with SciPy [0.00301731 0.         0.         0.         0.         0.\n", " 0.         0.         0.         0.        ]\n", "alpha obtained with SciPy [0.         0.         0.         0.         0.         0.0010968\n", " 0.00024416 0.         0.         0.        ]\n"]}], "source": ["l = 1e-2  # lambda parameter\n", "eps = .5  # epsilon parameter\n", "K = X@X.T\n", "\n", "res = minimize(lambda x: (x[:n]-x[n:])@K@(x[:n]-x[n:])/(2*l) + (x[:n]+x[n:])@(x[:n]+x[n:])/2 - x[:n]@(Y-eps) + x[n:]@(Y+eps),\n", "               np.zeros(2*n), bounds=[(0, None)]*2*n)\n", "alpha_opt, beta_opt = res.x[:n], res.x[n:]\n", "\n", "print('alpha obtained with SciPy', alpha_opt)\n", "print('alpha obtained with SciPy', beta_opt)"]}, {"cell_type": "markdown", "metadata": {}, "source": [">En notant, avec un l\u00e9ger abus de notation, $D_{i-1}^{\\alpha}$ (respectivement $D_{i-1}^{\\beta}$) la fonction $D$ selon la seule coordon\u00e9e $\\alpha_i$ (respectivement $\\beta_i$), on s'int\u00e9resse \u00e0 l'algorithme de descente coordonn\u00e9e par coordonn\u00e9e :\n", ">- initialiser $\\alpha$ et $\\beta$ \u00e0 $0_{\\mathbb R^n}$ ;\n", ">- it\u00e9rer :\n", "    - choisir al\u00e9atoirement et uniform\u00e9ment une coordonn\u00e9e $i$ dans $\\{1, \\dots, n\\}$ ;\n", "    - mettre \u00e0 jour la $i^e$ coordonn\u00e9e de $\\alpha$ : $\\alpha_i \\in \\operatorname{arg\\,min}_{u \\ge 0} D_{i-1}^{\\alpha}(u)$ ;\n", "    - mettre \u00e0 jour la $i^e$ coordonn\u00e9e de $\\beta$ : $\\beta_i \\in \\operatorname{arg\\,min}_{u \\ge 0} D_{i-1}^{\\beta}(u)$ ;\n", ">\n", ">o\u00f9 on aura de nouveau fait attention \u00e0 la contrainte de positivit\u00e9 sur $u$.\n", ">\n", ">Impl\u00e9menter cette proc\u00e9dure (en it\u00e9rant 10000 fois) et v\u00e9rifier le r\u00e9sultat obtenu en le comparant au r\u00e9sultat donn\u00e9.\n", ">\n", "> ---\n", ">\n", ">*Denoting $D_{i-1}^{\\alpha}$ (respectively $D_{i-1}^{\\beta}$) the function $D$ with respect to the $\\alpha_i$ (respectively $\\beta_i$) only, we focus on the coordinate descent algorithm:*\n", ">- *initialize $\\alpha$ and $\\beta$ at $0_{\\mathbb R^n}$;*\n", ">- *iterate:*\n", "    - *sample a coordinate $i$ according to a uniform distribution on $\\{1, \\dots, n\\}$ ;*\n", "    - *update the $i^{th}$ coordinate of $\\alpha$ : $\\alpha_i \\in \\operatorname{arg\\,min}_{u \\ge 0} D_{i-1}^{\\alpha}(u)$ ;*\n", "    - *update the $i^{th}$ coordinate of $\\beta$ : $\\beta_i \\in \\operatorname{arg\\,min}_{u \\ge 0} D_{i-1}^{\\beta}(u)$.*\n", ">\n", ">*Again, one has to pay attention to the positivity constraint on $u$.*\n", ">\n", ">*Implement the previous procedure (with 10000 iterations) and check that the result is correct by comparing it to the result given.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 7.**\n", "Repr\u00e9senter sur un graphique les observations, la fonction \u00e0 estimer $t \\mapsto \\phi(t)^\\top w_\\text{true}$, l'estimateur des moindres carr\u00e9s et l'estimateur obtenue \u00e0 la question pr\u00e9c\u00e9dente.\n", ">\n", "> ---\n", ">\n", ">*Display the observations, the graph of the function $t \\mapsto \\phi(t)^\\top w_\\text{true}$ to be estimated, the least squares estimator and the estimator obtained at the previous question.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 8.**\n", "Comparer la valeur de la fonction objectif $R$ pour l'estimateur des moindres carr\u00e9s et pour l'estimateur obtenu pr\u00e9c\u00e9demment.\n", ">\n", ">Calculer la fonction duale $D$ en $(\\alpha, \\beta)$ obtenu pr\u00e9c\u00e9demment et en d\u00e9duire la valeur du saut de dualit\u00e9 associ\u00e9.\n", ">\n", "> ---\n", ">\n", ">*Compare the value of the objective function $R$ at the least squares estimator and at the estimator obtained previously.*\n", ">\n", ">*Compute the dual function $D$ at $(\\alpha, \\beta)$ obtained previously and deduce the value of the duality gap.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 9.**\n", "Impl\u00e9menter la m\u00e9thode de r\u00e9gression \u00e9tudi\u00e9e dans la classe suivante et illustrer son fonctionnement.\n", "On remplacera les 10000 it\u00e9rations par un crit\u00e8re d'arr\u00eat sur le saut de dualit\u00e9.\n", ">\n", "> ---\n", ">\n", ">*Implement the regression method studied up to now in the following class and illustrate its proper functioning.\n", "The 10000 iterations should be replaced by a stopping criterion based on the duality gap.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "class METHOD:\n", "    def __init__(self, l=1., eps=1., tol=1e-5):\n", "        self.l = l\n", "        self.eps = eps\n", "        self.tol = tol\n", "        self.alpha = None\n", "        self.beta = None\n", "        self.w = None\n", "        self.X = None\n", "        self.Y = None\n", "        self.Q = None\n", "        self.P = None\n", "        \n", "    def obj(self, w):\n", "        # To do\n", "\n", "        # End to do\n", "    \n", "    def dual(self, alpha, beta):\n", "        # To do\n", "\n", "        # End to do\n", "    \n", "    def gap(self, w, alpha, beta):\n", "        # To do\n", "\n", "        # End to do\n", "        \n", "    def fit(self, X, Y):\n", "        # To do\n", "\n", "        # End to do\n", "        return self\n", "        \n", "    def predict(self, X):\n", "        # To do\n", "\n", "        # End to do\n", "        \n", "    def score(self, X, Y):\n", "        # To do\n", "\n", "        # End to do\n", "\n", "# To do\n", "\n", "# End to do"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 10.**\n", "Comparer \u00e0 la r\u00e9gression \u00e0 vecteurs supports (SVR) telle qu'impl\u00e9ment\u00e9e dans Scikit Learn.\n", ">\n", "> ---\n", ">\n", ">*Compare your result to support vector regression (SVR), as implemented in Scikit Learn.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">**Question 11 (bonus).**\n", "Proposer une version \u00e0 noyau de la m\u00e9thode de r\u00e9gression \u00e9tudi\u00e9e.\n", "Celle-ci devra comprendre le noyau lin\u00e9aire et le noyau de Laplace.\n", ">\n", "> ---\n", ">\n", ">*Propose a kernelized version of the regression method studied up to now.\n", "Your proposal should handle the linear and the Laplace kernels.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Answer\n", "from sklearn.metrics.pairwise import laplacian_kernel\n", "\n", "# Answer\n", "class KERNELMETHOD:\n", "    def __init__(self, kernel='linear', l=1., eps=1., gamma=1., tol=1e-5):\n", "        self.kernel = kernel\n", "        self.l = l\n", "        self.eps = eps\n", "        self.gamma = gamma\n", "        self.tol = tol\n", "        self.coef = None\n", "        self.alpha = None\n", "        self.beta = None\n", "        self.X = None\n", "        self.Y = None\n", "        self.K = None\n", "        self.Q = None\n", "        self.P = None\n", "        \n", "    def obj(self, coef):\n", "        # To do\n", "\n", "        # End to do\n", "    \n", "    def dual(self, alpha, beta):\n", "        # To do\n", "\n", "        # End to do\n", "    \n", "    def gap(self, alpha, beta):\n", "        # To do\n", "\n", "        # End to do\n", "        \n", "    def fit(self, X, Y):\n", "        # To do\n", "\n", "        # End to do\n", "        return self\n", "        \n", "    def predict(self, X):\n", "        # To do\n", "\n", "        # End to do\n", "        \n", "    def score(self, X, Y):\n", "        # To do\n", "\n", "        # End to do\n", "\n", "# To do\n", "\n", "# End to do"], "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.6"}}, "nbformat": 4, "nbformat_minor": 4}