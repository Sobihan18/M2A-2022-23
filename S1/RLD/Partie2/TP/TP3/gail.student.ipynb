{"cells": [{"cell_type": "markdown", "id": "9df051cb", "metadata": {"id": "dYfGJCe52lP4"}, "source": ["\n", "GAIL: Generative Adversarial Imitation Learning\n", "\n", "# Installation and Imports\n", "\n", "## Installation"]}, {"cell_type": "code", "execution_count": null, "id": "b70a759e", "metadata": {"id": "j0MaggiOl4KU"}, "outputs": [], "source": ["from easypip import easyimport\n", "import functools\n", "import time\n", "\n", "easyimport(\"importlib_metadata==4.13.0\")\n", "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n", "bbrl_gym = easyimport(\"bbrl_gym\")\n", "bbrl = easyimport(\"bbrl>=0.1.6\")\n", "expert_path = 'LunarLander-v2.expert.pkl'\n"]}, {"cell_type": "markdown", "id": "14081ad6", "metadata": {"id": "m4kV9pWV3wRe", "lines_to_next_cell": 2}, "source": ["### Imports\n", "\n", "Below, we import standard python packages, pytorch packages and gym environments.\n", "\n", "This is OmegaConf that makes it possible that by just defining the `def run_a2c(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n", "\n", "More precisely, the code is run by calling\n", "\n", "`config=OmegaConf.create(params)`\n", "\n", "`run_a2c(config)`\n", "\n", "at the very bottom of the colab, after starting tensorboard.\n", "\n", "[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]}, {"cell_type": "code", "execution_count": null, "id": "27186ecf", "metadata": {"id": "vktQB-AO5biu"}, "outputs": [], "source": ["import os\n", "import copy\n", "import time\n", "import pickle\n", "from pathlib import Path\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import gym\n", "\n", "from bbrl.agents.agent import Agent\n", "from bbrl import get_arguments, get_class, instantiate_class\n", "from bbrl.utils.functionalb import gae\n", "\n", "# The workspace is the main class in BBRL, this is where all data is collected and stored\n", "from bbrl.workspace import Workspace\n", "\n", "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n", "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n", "# or until a given condition is reached\n", "from bbrl.agents import Agents, TemporalAgent\n", "\n", "# AutoResetGymAgent is an agent able to execute a batch of gym environments\n", "# with auto-resetting. These agents produce multiple variables in the workspace: \n", "# \u2019env/env_obs\u2019, \u2019env/reward\u2019, \u2019env/timestep\u2019, \u2019env/done\u2019, \u2019env/initial_state\u2019, \u2019env/cumulated_reward\u2019, \n", "# ... When called at timestep t=0, then the environments are automatically reset. \n", "# At timestep t>0, these agents will read the \u2019action\u2019 variable in the workspace at time t \u2212 1\n", "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n", "\n", "# Allow to display the behavior of an agent\n", "from bbrl.visu.play import load_agent, play"]}, {"cell_type": "markdown", "id": "d6a9c38f", "metadata": {}, "source": ["\n", "We first load the expert trajectories for LunarLander-v2"]}, {"cell_type": "code", "execution_count": null, "id": "57b67336", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["\n", "with open(expert_path, 'rb') as handle:\n", "    # Dictionary\n", "    # states => states\n", "    # actions => long tensor\n", "    expert_data = pickle.load(handle)"]}, {"cell_type": "markdown", "id": "efd92f05", "metadata": {"id": "JVvAfhKm9S8p", "lines_to_next_cell": 2}, "source": ["\n", "## Definition of agents\n", "### Functions to build networks\n", "We use the same utilitary functions to build neural networks as before"]}, {"cell_type": "code", "execution_count": null, "id": "f923404f", "metadata": {"id": "HFLn1t5rmIDb"}, "outputs": [], "source": ["def build_backbone(sizes, activation):\n", "    layers = []\n", "    for j in range(len(sizes) - 1):\n", "        layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n", "    return layers\n", "\n", "\n", "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n", "    layers = []\n", "    for j in range(len(sizes) - 1):\n", "        act = activation if j < len(sizes) - 2 else output_activation\n", "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n", "    return nn.Sequential(*layers)"]}, {"cell_type": "markdown", "id": "d851f815", "metadata": {"cell_marker": "\"\"\""}, "source": ["We also implement a base agent for PPO actors"]}, {"cell_type": "code", "execution_count": null, "id": "d26186ef", "metadata": {}, "outputs": [], "source": ["class BaseActor(Agent):\n", "    def copy_parameters(self, other):\n", "        \"\"\"Copy parameters from other agent\"\"\"\n", "        for self_p, other_p in zip(self.parameters(), other.parameters()):\n", "            self_p.data.copy_(other_p)"]}, {"cell_type": "markdown", "id": "1bf1a89e", "metadata": {"cell_marker": "\"\"\"", "id": "ouwQ5WhxTKNV"}, "source": ["### The DiscreteActor"]}, {"cell_type": "markdown", "id": "62e9d291", "metadata": {"cell_marker": "\"\"\"", "id": "zNjmm6e9TPj8"}, "source": ["The DiscreteActor was already used in A2C to deal with discrete actions, but we have added the possibility to only predict the probability of an action using the ```predict_proba``` variable in the ```forward()``` function. The code is as follows."]}, {"cell_type": "code", "execution_count": null, "id": "805094bd", "metadata": {"id": "5LO_VNaOTeJu", "lines_to_next_cell": 2}, "outputs": [], "source": ["\n", "class DiscreteActor(BaseActor):\n", "    def __init__(self, state_dim, hidden_size, n_actions):\n", "        super().__init__()\n", "        self.model = build_mlp(\n", "            [state_dim] + list(hidden_size) + [n_actions], activation=nn.Tanh()\n", "        )\n", "        \n", "    def logits(self, obs):\n", "        return self.model(obs)\n", "\n", "    def dist(self, obs):\n", "        scores = self.model(obs)\n", "        probs = torch.softmax(scores, dim=-1)\n", "        return torch.distributions.Categorical(probs)\n", "\n", "    def forward(self, t, *, stochastic=True, predict_proba=False, compute_entropy=False, **kwargs):\n", "        \"\"\"\n", "        Compute the action given either a time step (looking into the workspace)\n", "        or an observation (in kwargs)\n", "        \"\"\"\n", "        if \"observation\" in kwargs:\n", "            observation = kwargs[\"observation\"]\n", "        else:\n", "            observation = self.get((\"env/env_obs\", t))\n", "        scores = self.model(observation)\n", "        probs = torch.softmax(scores, dim=-1)\n", "\n", "        if predict_proba:\n", "            action = self.get((\"action\", t))\n", "            log_prob = probs[torch.arange(probs.size()[0]), action].log()\n", "            self.set((\"logprob_predict\", t), log_prob)\n", "        else:\n", "            if stochastic:\n", "                action = torch.distributions.Categorical(probs).sample()\n", "            else:\n", "                action = scores.argmax(1)\n", "\n", "            log_probs = probs[torch.arange(probs.size()[0]), action].log()\n", "\n", "            self.set((\"action\", t), action)\n", "            self.set((\"action_logprobs\", t), log_probs)\n", "\n", "        if compute_entropy:\n", "            entropy = torch.distributions.Categorical(probs).entropy()\n", "            self.set((\"entropy\", t), entropy)\n", "\n", "    def predict_action(self, obs, stochastic):\n", "        scores = self.model(obs)\n", "\n", "        if stochastic:\n", "            probs = torch.softmax(scores, dim=-1)\n", "            action = torch.distributions.Categorical(probs).sample()\n", "        else:\n", "            action = scores.argmax(0)\n", "        return action"]}, {"cell_type": "markdown", "id": "5d5ee011", "metadata": {"id": "ajqSi5Nbmnxn", "lines_to_next_cell": 2}, "source": ["\n", "### Choosing a specific gym environment\n", "First, we need to make our gym environment. As usual, this is implemented with the simple function below."]}, {"cell_type": "code", "execution_count": null, "id": "bc41225b", "metadata": {"id": "Fsb5QRzw7V0o"}, "outputs": [], "source": ["def make_gym_env(env_name):\n", "    return gym.make(env_name)"]}, {"cell_type": "markdown", "id": "d3a2a27e", "metadata": {"id": "lddUojiWoxXz"}, "source": ["\n", "## Training/Testing environment and agents\n", "\n", "The code below corresponds to the PPO one\n", "it defines the different classes and functions needed to run\n", "PPO and define the train/test environment"]}, {"cell_type": "code", "execution_count": null, "id": "8f89e298", "metadata": {}, "outputs": [], "source": ["def get_env_agents(cfg):\n", "    train_env_agent = AutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.n_envs,\n", "        cfg.algorithm.seed,\n", "    )\n", "    eval_env_agent = NoAutoResetGymAgent(\n", "    get_class(cfg.gym_env),\n", "    get_arguments(cfg.gym_env),\n", "    cfg.algorithm.nb_evals,\n", "    cfg.algorithm.seed,\n", "    )\n", "    return train_env_agent, eval_env_agent\n", "\n", "\n", "class DAgent(Agent):\n", "    \"\"\"Discriminator agent\"\"\"\n", "    def __init__(self, state_dim, act_dim, hidden_layers):\n", "        super().__init__()\n", "        self.is_q_function = False\n", "        self.act_dim = act_dim\n", "        self.model = build_mlp(\n", "            [state_dim+act_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n", "        )\n", "\n", "    def forward(self, t, **kwargs):\n", "        observation = self.get((\"env/env_obs\", t))\n", "        action = self.get((\"action\", t))\n", "        input = torch.cat((observation, nn.functional.one_hot(action, num_classes=self.act_dim)), dim=1)\n", "        critic = self.model(input).squeeze(-1)\n", "        self.set((\"disc\", t), critic)\n", "\n", "class VAgent(Agent):\n", "    def __init__(self, state_dim, hidden_layers):\n", "        super().__init__()\n", "        self.is_q_function = False\n", "        self.model = build_mlp(\n", "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n", "        )\n", "\n", "    def forward(self, t, **kwargs):\n", "        observation = self.get((\"env/env_obs\", t))\n", "        critic = self.values(observation)\n", "        self.set((\"v_value\", t), critic)\n", "\n", "    def values(self, observation):\n", "        return self.model(observation).squeeze(-1)\n", "\n", "class KLAgent(Agent):\n", "    def __init__(self, model_1, model_2):\n", "        super().__init__()\n", "        self.model_1 = model_1\n", "        self.model_2 = model_2\n", "\n", "    def forward(self, t, **kwargs):\n", "        obs = self.get((\"env/env_obs\", t))\n", "        \n", "        dist_1 = self.model_1.dist(obs)\n", "        dist_2 = self.model_2.dist(obs)\n", "        kl = torch.distributions.kl.kl_divergence(dist_1, dist_2)\n", "        self.set((\"kl\", t), kl)        \n", "\n", "# Create the PPO Agent\n", "def create_ppo_agent(cfg, train_env_agent, eval_env_agent, needs_kl=None):\n", "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n", "\n", "    action_agent = DiscreteActor(\n", "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n", "    )\n", "\n", "    tr_agent = Agents(train_env_agent, action_agent)\n", "    ev_agent = Agents(eval_env_agent, action_agent)\n", "\n", "    critic_agent = TemporalAgent(\n", "        VAgent(obs_size, cfg.algorithm.architecture.critic_hidden_size)\n", "    )\n", "\n", "    # The agent for discriminating\n", "    disc_agent = DAgent(obs_size, act_size, cfg.algorithm.architecture.critic_hidden_size)\n", "\n", "    train_agent = TemporalAgent(tr_agent)\n", "    eval_agent = TemporalAgent(ev_agent)\n", "    train_agent.seed(cfg.algorithm.seed)\n", "\n", "    old_policy = copy.deepcopy(action_agent)\n", "    old_critic_agent = copy.deepcopy(critic_agent)\n", "    \n", "    kl_agent = None\n", "    if needs_kl:\n", "        kl_agent = TemporalAgent(KLAgent(old_policy, action_agent))\n", "\n", "    return action_agent, train_agent, eval_agent, critic_agent, old_policy, old_critic_agent, kl_agent, disc_agent\n", "\n", "class Logger():\n", "    def __init__(self, cfg, variant, env_name):\n", "        kwargs = dict(cfg.logger)\n", "        kwargs[\"log_dir\"] = f'{kwargs[\"log_dir\"]}/{env_name}/{variant}-{str(time.time())}'\n", "        self.logger = instantiate_class(kwargs)\n", "\n", "    def add_log(self, log_string, loss, epoch):\n", "        self.logger.add_scalar(log_string, loss.item(), epoch)\n", "\n", "    # Log losses\n", "    def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n", "        self.add_log(\"critic_loss\", critic_loss, epoch)\n", "        self.add_log(\"entropy_loss\", entropy_loss, epoch)\n", "        self.add_log(\"actor_loss\", actor_loss, epoch)\n", "\n", "def setup_optimizer(cfg, *agents):\n", "    optimizer_args = get_arguments(cfg.optimizer)\n", "    parameters = nn.Sequential(*agents).parameters()\n", "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n", "    return optimizer\n", "\n", "\n", "\n", "def compute_advantage_loss(cfg, reward, must_bootstrap, v_value):\n", "    # Compute temporal difference with GAE\n", "    advantage = gae(\n", "        v_value,\n", "        reward,\n", "        must_bootstrap,\n", "        cfg.algorithm.discount_factor,\n", "        cfg.algorithm.gae,\n", "    )\n", "    # Compute critic loss\n", "    td_error = advantage**2\n", "    critic_loss = td_error.mean()\n", "    return critic_loss, advantage\n", "\n", "def compute_clip_agent_loss(cfg, advantage, ratio, kl):\n", "    \"\"\"Computes the PPO CLIP loss\n", "    \"\"\"\n", "    clip_range = cfg.clip_range\n", "\n", "    actor_loss_1 = advantage * ratio\n", "    actor_loss_2 = advantage * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n", "    actor_loss = torch.minimum(actor_loss_1, actor_loss_2).mean()\n", "\n", "    return actor_loss"]}, {"cell_type": "markdown", "id": "3e98a9d6", "metadata": {}, "source": ["\n", "## Behavioral Cloning"]}, {"cell_type": "code", "execution_count": null, "id": "4bda9334", "metadata": {}, "outputs": [], "source": ["\n", "def run_behavioral_cloning(cfg):\n", "    # 1)  Build the  logger\n", "    logger = Logger(cfg, \"bc\", cfg.gym_env.env_name)\n", "    best_reward = -float('inf')\n", "\n", "    # 2) Create the environment agent   \n", "    eval_env_agent = NoAutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.nb_evals,\n", "        cfg.algorithm.seed,\n", "    )\n", "\n", "    obs_size, act_size = eval_env_agent.get_obs_and_actions_sizes()\n", "\n", "    # Creates our policy\n", "    policy = DiscreteActor(\n", "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n", "    )\n", "\n", "    eval_agent = TemporalAgent(Agents(eval_env_agent, policy))\n", "\n", "    # Configure the optimizer\n", "    optimizer_args = get_arguments(cfg.optimizer)\n", "    optimizer = get_class(cfg.optimizer)(policy.parameters(), **optimizer_args)\n", "\n", "    nb_steps = 0\n", "    tmp_steps = 0\n", "\n", "    ce_loss = nn.CrossEntropyLoss()\n", "\n", "    # Training loop\n", "    for epoch in range(cfg.algorithm.max_epochs):\n", "        # \u00c0 compl\u00e9ter...  \n", "        assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "        nb_steps += len(expert_data[\"states\"])\n", "\n", "        # Evaluate if enough steps have been performed\n", "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n", "            tmp_steps = nb_steps\n", "            eval_workspace = Workspace()  # Used for evaluation\n", "            eval_agent(\n", "                eval_workspace,\n", "                t=0,\n", "                stop_variable=\"env/done\",\n", "                stochastic=True,\n", "                predict_proba=False,\n", "            )\n", "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n", "            mean = rewards.mean()\n", "            logger.add_log(\"reward/mean\", mean, nb_steps)\n", "            logger.add_log(\"reward/max\", rewards.max(), nb_steps)\n", "            logger.add_log(\"reward/min\", rewards.min(), nb_steps)\n", "            logger.add_log(\"reward/std\", rewards.std(), nb_steps)\n", "            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n", "            if cfg.save_best and mean > best_reward:\n", "                best_reward = mean\n", "                directory = f\"./bc_agent/{cfg.gym_env.env_name}\"\n", "                if not os.path.exists(directory):\n", "                    os.makedirs(directory)\n", "                filename = directory + \"bc_\" + str(mean.item()) + \".agt\"\n", "                policy.save_model(filename)"]}, {"cell_type": "code", "execution_count": null, "id": "07e50440", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "2e61158d", "metadata": {"cell_marker": "\"\"\"", "id": "uo6bc3zzKua_"}, "source": ["## Definition of the parameters\n", "\n", "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation.\n", "\n", "The two parameters that are specific to PPO are \"clip_range\" and \"clip_range_vf\", which are used to clip the actor loss and the critic loss respectively."]}, {"cell_type": "code", "execution_count": null, "id": "faaf5af4", "metadata": {"id": "JB2B8zELNWQd"}, "outputs": [], "source": ["\n", "params={\n", "  \"save_best\": True,\n", "  \"plot_policy\": True,\n", "\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": f\"{os.getcwd()}/tblogs/gail\",\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 4,\n", "    \"nb_evals\": 10,\n", "    \"eval_interval\": 10000,\n", "    \"max_epochs\": 5000,\n", "    \"architecture\":{\n", "      \"actor_hidden_size\": [64, 32],\n", "    },\n", "  },\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_gym_env\",\n", "    \"env_name\": \"LunarLander-v2\",\n", "    },\n", "  \"optimizer\":{\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 0.001,\n", "  }\n", "}"]}, {"cell_type": "markdown", "id": "f2dda2d2", "metadata": {"id": "jp7jDeGkaoM1"}, "source": ["### Launching tensorboard to visualize the results"]}, {"cell_type": "code", "execution_count": null, "id": "159ffc12", "metadata": {"tags": ["not-colab"]}, "outputs": [], "source": ["import sys\n", "import os\n", "import os.path as osp\n", "print(f'''Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir=\"{params[\"logger\"][\"log_dir\"]}\"''')"]}, {"cell_type": "code", "execution_count": null, "id": "5fcb101a", "metadata": {}, "outputs": [], "source": ["config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)"]}, {"cell_type": "code", "execution_count": null, "id": "d584fd2d", "metadata": {}, "outputs": [], "source": ["run_behavioral_cloning(config)"]}, {"cell_type": "markdown", "id": "993e361d", "metadata": {"cell_marker": "\"\"\""}, "source": ["Now we can watch our agent..."]}, {"cell_type": "code", "execution_count": null, "id": "168b58ea", "metadata": {}, "outputs": [], "source": ["agent = load_agent(Path(\"bc_agent\") / config.gym_env.env_name, \"bc_\")\n", "play(make_gym_env(config.gym_env.env_name), agent)\n", "\n"]}, {"cell_type": "markdown", "id": "87992d2f", "metadata": {"cell_marker": "\"\"\"", "id": "Jmi91gANWT4z"}, "source": ["## Main training loop"]}, {"cell_type": "code", "execution_count": null, "id": "42fc08be", "metadata": {"id": "sk85_sRWW-5s"}, "outputs": [], "source": ["def run_gail(cfg, variant=\"gail\", needs_kl=False):\n", "    # 1)  Build the  logger\n", "    logger = Logger(cfg, variant, cfg.gym_env.env_name)\n", "    best_reward = -10e9\n", "\n", "    # 2) Create the environment agent\n", "    train_env_agent = AutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.n_envs,\n", "        cfg.algorithm.seed,\n", "    )\n", "    \n", "    eval_env_agent = NoAutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.nb_evals,\n", "        cfg.algorithm.seed,\n", "    )\n", "\n", "    (\n", "        policy,\n", "        train_agent,\n", "        eval_agent,\n", "        critic_agent,\n", "        old_policy,\n", "        old_critic_agent,\n", "        kl_agent,\n", "        disc_agent\n", "    ) = create_ppo_agent(cfg, train_env_agent, eval_env_agent, needs_kl=needs_kl)\n", "    \n", "    action_agent = TemporalAgent(policy)\n", "    old_train_agent = TemporalAgent(old_policy)\n", "    train_workspace = Workspace()\n", "\n", "    # Configure the optimizer\n", "    disc_optimizer = setup_optimizer(cfg, disc_agent)\n", "    optimizer = setup_optimizer(cfg, train_agent, critic_agent)\n", "    nb_steps = 0\n", "    tmp_steps = 0\n", "\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "    # Training loop\n", "    for epoch in range(cfg.algorithm.max_epochs):\n", "        # Execute the agent in the workspace\n", "        \n", "        # Handles continuation\n", "        delta_t = 0\n", "        if epoch > 0:\n", "            train_workspace.zero_grad()\n", "            delta_t = 1\n", "            train_workspace.copy_n_last_steps(delta_t)\n", "\n", "        # Run the train/old_train agents\n", "        train_agent(\n", "            train_workspace,\n", "            t=delta_t,\n", "            n_steps=cfg.algorithm.n_steps - delta_t,\n", "            stochastic=True,\n", "            predict_proba=False,\n", "            compute_entropy=False\n", "        )\n", "        old_train_agent(\n", "            train_workspace,\n", "            t=delta_t,\n", "            n_steps=cfg.algorithm.n_steps - delta_t,\n", "            # Just computes the probability\n", "            predict_proba=True,\n", "        )\n", "\n", "        # Compute the critic value over the whole workspace\n", "        critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)\n", "\n", "        transition_workspace = train_workspace.get_transitions()\n", "\n", "        # We ignore the rewards from the environment\n", "        done, truncated, action, action_logp, v_value = transition_workspace[\n", "            \"env/done\",\n", "            \"env/truncated\",\n", "            \"action\",\n", "            \"action_logprobs\",\n", "            \"v_value\",\n", "        ]\n", "        nb_steps += action[0].shape[0]\n", "\n", "        # ---- Discriminator training\n", "\n", "        # \u00c0 compl\u00e9ter...  \n", "        assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "        # \u00c0 compl\u00e9ter...  \n", "        assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "        # ---- PPO training     \n", "\n", "        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n", "\n", "        with torch.no_grad():\n", "            old_critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)\n", "            \n", "        old_action_logp = transition_workspace[\"logprob_predict\"].detach()\n", "        old_v_value = transition_workspace[\"v_value\"]\n", "        if cfg.algorithm.clip_range_vf > 0:\n", "            # Clip the difference between old and new values\n", "            # NOTE: this depends on the reward scaling\n", "            v_value = old_v_value + torch.clamp(\n", "                v_value - old_v_value,\n", "                -cfg.algorithm.clip_range_vf,\n", "                cfg.algorithm.clip_range_vf,\n", "            )\n", "            \n", "        critic_loss, advantage = compute_advantage_loss(\n", "            cfg, reward, must_bootstrap, v_value\n", "        )\n", "        \n", "        # We store the advantage into the transition_workspace\n", "        advantage = advantage.detach().squeeze(0)\n", "        transition_workspace.set(\"advantage\", 0, advantage)\n", "        transition_workspace.set(\"advantage\", 1, torch.zeros_like(advantage))\n", "        transition_workspace.set_full(\"old_action_logprobs\", transition_workspace[\"logprob_predict\"].detach())\n", "        transition_workspace.clear(\"logprob_predict\")\n", "    \n", "        for opt_epoch in range(cfg.algorithm.opt_epochs):\n", "            if cfg.algorithm.minibatch_size > 0:\n", "                sample_workspace = transition_workspace.sample_subworkspace(1, cfg.algorithm.minibatch_size, 2)\n", "            else:\n", "                sample_workspace = transition_workspace\n", "                                 \n", "            if opt_epoch > 0:\n", "                critic_loss = 0. # We don't want to optimize the critic after the first mini-epoch\n", "\n", "            action_agent(sample_workspace, t=0, n_steps=1, compute_entropy=True, predict_proba=True)\n", "\n", "            advantage, action_logp, old_action_logp, entropy = sample_workspace[\n", "                \"advantage\",\n", "                \"logprob_predict\",\n", "                \"old_action_logprobs\",\n", "                \"entropy\"\n", "            ]\n", "            advantage = advantage[0]\n", "            act_diff = action_logp[0] - old_action_logp[0]\n", "            ratios = act_diff.exp()\n", "\n", "            kl = None\n", "            if kl_agent:\n", "                kl_agent(sample_workspace, t=0, n_steps=1)\n", "                kl = sample_workspace[\"kl\"][0]\n", "\n", "            actor_loss = compute_clip_agent_loss(\n", "                cfg.algorithm, advantage, ratios, kl\n", "            )\n", "                            \n", "            # Entropy loss favor exploration\n", "            entropy_loss = torch.mean(entropy[0])\n", "\n", "            # Store the losses for tensorboard display\n", "            if opt_epoch == 0:\n", "                # Just for the first epoch\n", "                logger.log_losses(nb_steps, critic_loss, entropy_loss, actor_loss)\n", "\n", "            loss = (\n", "                cfg.algorithm.critic_coef * critic_loss\n", "                - cfg.algorithm.actor_coef * actor_loss\n", "                - cfg.algorithm.entropy_coef * entropy_loss\n", "            )\n", "            \n", "\n", "            old_policy.copy_parameters(policy)\n", "            old_critic_agent = copy.deepcopy(critic_agent)\n", "\n", "\n", "            optimizer.zero_grad()\n", "            loss.backward()\n", "            torch.nn.utils.clip_grad_norm_(\n", "                critic_agent.parameters(), cfg.algorithm.max_grad_norm\n", "            )\n", "            torch.nn.utils.clip_grad_norm_(\n", "                train_agent.parameters(), cfg.algorithm.max_grad_norm\n", "            )\n", "            optimizer.step() \n", "\n", "        # Evaluate if enough steps have been performed\n", "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n", "            tmp_steps = nb_steps\n", "            eval_workspace = Workspace()  # Used for evaluation\n", "            eval_agent(\n", "                eval_workspace,\n", "                t=0,\n", "                stop_variable=\"env/done\",\n", "                stochastic=True,\n", "                predict_proba=False,\n", "            )\n", "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n", "            mean = rewards.mean()\n", "            logger.add_log(\"reward/mean\", mean, nb_steps)\n", "            logger.add_log(\"reward/max\", rewards.max(), nb_steps)\n", "            logger.add_log(\"reward/min\", rewards.min(), nb_steps)\n", "            logger.add_log(\"reward/std\", rewards.std(), nb_steps)\n", "            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n", "            if cfg.save_best and mean > best_reward:\n", "                best_reward = mean\n", "                directory = f\"./gail_agent/{cfg.gym_env.env_name}/{variant}/\"\n", "                if not os.path.exists(directory):\n", "                    os.makedirs(directory)\n", "                filename = directory + \"gail_\" + str(mean.item()) + \".agt\"\n", "                policy.save_model(filename)"]}, {"cell_type": "markdown", "id": "4b772bb2", "metadata": {"cell_marker": "\"\"\"", "id": "uo6bc3zzKua_"}, "source": ["## Definition of the parameters\n", "\n", "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation.\n", "\n", "The two parameters that are specific to PPO are \"clip_range\" and \"clip_range_vf\", which are used to clip the actor loss and the critic loss respectively."]}, {"cell_type": "code", "execution_count": null, "id": "40d6acc5", "metadata": {"id": "JB2B8zELNWQd"}, "outputs": [], "source": ["\n", "params={\n", "  \"save_best\": True,\n", "  \"plot_policy\": True,\n", "\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": f\"{os.getcwd()}/tblogs/ppo\",\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 4,\n", "    \"n_envs\": 8,\n", "    \"max_grad_norm\": 0.5,\n", "    \"nb_evals\":10,\n", "    \"n_steps\": 20,\n", "    \"eval_interval\": 1000,\n", "    \"max_epochs\": 3000,\n", "    \"discount_factor\": 0.95,\n", "    \"entropy_coef\": 2.55e-5,\n", "    \"beta_kl\": 1,\n", "    \"critic_coef\": 0.6,\n", "    \"actor_coef\": 1.0,\n", "    \"gae\": 0.9,\n", "    \"clip_range\": 0.2,\n", "    \"clip_range_vf\": 0,\n", "    \"opt_epochs\": 1,\n", "    \"minibatch_size\": 0,\n", "    \"architecture\":{\n", "      \"actor_hidden_size\": [25, 36],\n", "      \"critic_hidden_size\": [24, 36],\n", "    },\n", "  },\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_gym_env\",\n", "    \"env_name\": \"LunarLander-v2\",\n", "    },\n", "  \"optimizer\":{\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 0.001,\n", "  }\n", "}"]}, {"cell_type": "markdown", "id": "47247518", "metadata": {"cell_marker": "\"\"\"", "id": "jp7jDeGkaoM1"}, "source": ["### Launching tensorboard to visualize the results"]}, {"cell_type": "code", "execution_count": null, "id": "71bbb0f4", "metadata": {"tags": ["not-colab"]}, "outputs": [], "source": ["import sys\n", "import os\n", "import os.path as osp\n", "print(f'''Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir=\"{params[\"logger\"][\"log_dir\"]}\"''')"]}, {"cell_type": "code", "execution_count": null, "id": "0a52d53b", "metadata": {}, "outputs": [], "source": ["config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)"]}, {"cell_type": "code", "execution_count": null, "id": "55c36598", "metadata": {}, "outputs": [], "source": ["run_gail(config)"]}, {"cell_type": "markdown", "id": "593724c1", "metadata": {"cell_marker": "\"\"\""}, "source": ["Now we can watch our agent..."]}, {"cell_type": "code", "execution_count": null, "id": "a56c7ee5", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["agent = load_agent(Path(\"gail_agent\") / config.gym_env.env_name, \"gail_\")\n", "play(make_gym_env(config.gym_env.env_name), agent)"]}], "metadata": {"jupytext": {"cell_markers": "\"\"\""}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}
