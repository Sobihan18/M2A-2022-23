{"cells": [{"cell_type": "markdown", "id": "f4ab5dc7", "metadata": {"cell_marker": "\"\"\"", "id": "dYfGJCe52lP4"}, "source": ["# Outlook\n", "\n", "In this notebook, using BBRL, we code a version of the DQN algorithm with a replay buffer and a target network. To understand this code, you need to know more about BBRL. You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, [details about the data collection implementation](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing).\n", "\n", "The DQN algorithm is explained in [this video](https://www.youtube.com/watch?v=CXwvOMJujZk) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/dqn.pdf).\n", "\n", "In this notebook, you will learn how to modify the previous notebook:\n", "\n", "- to use a replay buffer and an environment that resets\n", "- to use a target network for $Q$\n", "- to use a better estimation for the maximum (Double-DQN)\n", "\n", "## Installation and Imports\n", "\n", "### Installation\n", "\n", "The BBRL library is [here](https://github.com/osigaud/bbrl).\n", "\n", "This is OmegaConf that makes it possible that by just defining the `def run_dqn(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n", "\n", "More precisely, the code is run by calling\n", "\n", "`config=OmegaConf.create(params)`\n", "\n", "`run_dqn(config)`\n", "\n", "at the very bottom of the colab, after starting tensorboard."]}, {"cell_type": "code", "execution_count": null, "id": "b7a725c8", "metadata": {}, "outputs": [], "source": ["\n", "try:\n", "    from easypip import easyimport\n", "except:\n", "    !pip install easypip\n", "    from easypip import easyimport"]}, {"cell_type": "code", "execution_count": null, "id": "32f09b9f", "metadata": {}, "outputs": [], "source": ["import os\n", "import functools\n", "import time\n", "from typing import Tuple\n", "\n", "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n", "torch = easyimport(\"torch\")\n", "bbrl_gym = easyimport(\"bbrl_gym\")\n", "import gym\n", "import bbrl\n", "import copy\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import copy"]}, {"cell_type": "markdown", "id": "1cf98a47", "metadata": {"cell_marker": "\"\"\""}, "source": ["### BBRL imports"]}, {"cell_type": "code", "execution_count": null, "id": "ac5830a8", "metadata": {}, "outputs": [], "source": ["from bbrl.agents.agent import Agent\n", "from bbrl import get_arguments, get_class, instantiate_class\n", "\n", "# The workspace is the main class in BBRL, this is where all data is collected and stored\n", "from bbrl.workspace import Workspace\n", "\n", "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n", "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n", "# or until a given condition is reached\n", "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n", "\n", "# AutoResetGymAgent is an agent able to execute a batch of gym environments\n", "# with auto-resetting. These agents produce multiple variables in the workspace: \n", "# \u2019env/env_obs\u2019, \u2019env/reward\u2019, \u2019env/timestep\u2019, \u2019env/done\u2019, \u2019env/initial_state\u2019, \u2019env/cumulated_reward\u2019, \n", "# ... When called at timestep t=0, then the environments are automatically reset. \n", "# At timestep t>0, these agents will read the \u2019action\u2019 variable in the workspace at time t \u2212 1\n", "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n", "# Not present in the A2C version...\n", "from bbrl.utils.logger import TFLogger\n", "from bbrl.utils.replay_buffer import ReplayBuffer\n", "\n", "def make_env(env_name):\n", "    return gym.make(env_name)"]}, {"cell_type": "markdown", "id": "83e2c87f", "metadata": {"cell_marker": "\"\"\""}, "source": ["## The replay buffer\n", "\n", "Differently from the previous case, we use a replace buffer that stores the a set of transitions $(s_t, a_t, r_t, s_{t+1})$"]}, {"cell_type": "code", "execution_count": null, "id": "b493ad1d", "metadata": {}, "outputs": [], "source": ["# We deal with 3 running episodes at a time (random seed 2139)\n", "env_agent = NoAutoResetGymAgent(make_env, {'env_name': 'CartPole-v1'}, 1, 2139)\n", "obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n", "print(f\"Environment: observation space in R^{obs_size} and action space R^{action_dim}\")\n", "\n", "class RandomAgent(Agent):\n", "    def __init__(self, action_dim):\n", "        super().__init__()\n", "        self.action_dim = action_dim\n", "\n", "    def forward(self, t: int, choose_action=True, **kwargs):\n", "        \"\"\"An Agent can use self.workspace\"\"\"\n", "        obs = self.get((\"env/env_obs\", t))\n", "        action = torch.randint(0, self.action_dim, (len(obs), ))\n", "        self.set((\"action\", t), action)\n", "\n", "# Each agent will be run (in the order given when constructing Agents)\n", "agents = Agents(env_agent, RandomAgent(action_dim))\n", "t_agents = TemporalAgent(agents)"]}, {"cell_type": "code", "execution_count": null, "id": "9fd77d0a", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["# Creates a new workspace\n", "workspace = Workspace() \n", "t_agents(workspace, stop_variable=\"env/done\")\n", "\n", "# We get the transitions: each tensor is transformed so\n", "# that: \n", "# - we have the value at time step t and t+1 (so all the tensors first dimension have a size of 2)\n", "# - there is no distinction between the different environments (here, there is just one environment run in parallel to make it easy)\n", "transitions = workspace.get_transitions()\n", "\n", "# You can see that each pair of actions in the transitions can be found in the workspace\n", "display(\"Observations (first 3)\", workspace[\"env/env_obs\"][:3, 0])\n", "\n", "display(\"Transitions of actions (first 3)\")\n", "for t in range(3):\n", "    display(f'(s_{t}, s_{t+1})')\n", "    display(transitions[\"env/env_obs\"][:, t])"]}, {"cell_type": "code", "execution_count": null, "id": "38d506ea", "metadata": {}, "outputs": [], "source": ["# Finally, the replay buffer keeps slices [:, i, ...] of the transition workspace\n", "# (here at most 100 transitions)\n", "rb = ReplayBuffer(max_size=100)\n", "\n", "# We add the transitions to the buffer....\n", "rb.put(transitions)\n", "\n", "# And sample from them\n", "# here we get 3 tuples (s_t, s_{t+1})\n", "rb.get_shuffled(3)[\"env/env_obs\"]"]}, {"cell_type": "markdown", "id": "a762d4d1", "metadata": {"cell_marker": "\"\"\""}, "source": ["A transition workspace is still a workspace... this is quite handy since each transition can be seen as a mini-episode of two time steps; we can use our agents on it:"]}, {"cell_type": "code", "execution_count": null, "id": "2b45a0d0", "metadata": {"vscode": {"languageId": "python"}}, "outputs": [], "source": ["# Just as a reference\n", "\n", "display(transitions[\"action\"])\n", "\n", "t_random_agent = TemporalAgent(RandomAgent(action_dim))\n", "t_random_agent(transitions, t=0, n_steps=2)\n", "\n", "# Here, the action tensor will have been overwritten by the new actions\n", "display(transitions[\"action\"])"]}, {"cell_type": "markdown", "id": "a672df8d", "metadata": {"cell_marker": "\"\"\"", "id": "JVvAfhKm9S8p"}, "source": ["## Definition of agents"]}, {"cell_type": "markdown", "id": "42584748", "metadata": {"cell_marker": "\"\"\"", "id": "RdqzKSLKDtqz"}, "source": ["The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only algorithm. Thus we just need a Critic agent (which will also be used to output actions) and an Environment agent. We reuse the `DiscreteQAgent` class that we have already explained in the previous notebook."]}, {"cell_type": "code", "execution_count": null, "id": "b8dd0afa", "metadata": {"executionInfo": {"elapsed": 5, "status": "ok", "timestamp": 1662009762891, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "r_QIxxHNtBMH", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n", "    layers = []\n", "    for j in range(len(sizes) - 1):\n", "        act = activation if j < len(sizes) - 2 else output_activation\n", "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n", "    return nn.Sequential(*layers)"]}, {"cell_type": "code", "execution_count": null, "id": "fe58b680", "metadata": {"executionInfo": {"elapsed": 4, "status": "ok", "timestamp": 1662009762891, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "ARPW1Mmo7NB-", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["class DiscreteQAgent(Agent):\n", "    def __init__(self, state_dim, hidden_layers, action_dim):\n", "        super().__init__()\n", "        self.model = build_mlp(\n", "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n", "        )\n", "\n", "    def forward(self, t, choose_action=True, **kwargs):\n", "        obs = self.get((\"env/env_obs\", t))\n", "        q_values = self.model(obs)\n", "        self.set((\"q_values\", t), q_values)\n", "\n", "        # Sets the action\n", "        if choose_action:\n", "            action = q_values.argmax(1)\n", "            self.set((\"action\", t), action)"]}, {"cell_type": "markdown", "id": "bf28701e", "metadata": {"cell_marker": "\"\"\"", "id": "yoG1eNBguNTN"}, "source": ["### Creating an Exploration method"]}, {"cell_type": "markdown", "id": "e74502d9", "metadata": {"cell_marker": "r\"\"\"", "id": "qvxOy0e180_6"}, "source": ["As Q-learning, DQN needs some exploration to prevent too early convergence. Here we will use the simple $\\epsilon$-greedy exploration method. The method is implemented as an agent which chooses an action based on the Q-values."]}, {"cell_type": "code", "execution_count": null, "id": "4e5be668", "metadata": {"executionInfo": {"elapsed": 4, "status": "ok", "timestamp": 1662009762892, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "Rk0DTVsLtz4a", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["class EGreedyActionSelector(Agent):\n", "    def __init__(self, epsilon):\n", "        super().__init__()\n", "        self.epsilon = epsilon\n", "\n", "    def forward(self, t, **kwargs):\n", "        q_values = self.get((\"q_values\", t))\n", "        nb_actions = q_values.size()[1]\n", "        size = q_values.size()[0]\n", "        is_random = torch.rand(size).lt(self.epsilon).float()\n", "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n", "        max_action = q_values.max(1)[1]\n", "        action = is_random * random_action + (1 - is_random) * max_action\n", "        action = action.long()\n", "        self.set((\"action\", t), action)"]}, {"cell_type": "markdown", "id": "64f7e9f7", "metadata": {"cell_marker": "\"\"\"", "id": "W0AgHYc2ywoS"}, "source": ["### Training and evaluation environments"]}, {"cell_type": "markdown", "id": "3768ad0d", "metadata": {"cell_marker": "\"\"\"", "id": "1gfbKZMFrghw"}, "source": ["We build two environments: one for training and another one for evaluation."]}, {"cell_type": "markdown", "id": "03a0c59d", "metadata": {"cell_marker": "\"\"\"", "id": "jDM2Z0THyrtx"}, "source": ["For training, it is more efficient to use an AutoResetGymAgent, as we do not want to waste time if the task is done in an environment sooner than in the others."]}, {"cell_type": "markdown", "id": "5f505f4c", "metadata": {"cell_marker": "\"\"\"", "id": "7kN-SniayxRq"}, "source": ["By contrast, for evaluation, we just need to perform a fixed number of episodes (for statistics), thus it is more convenient to use a NoAutoResetGymAgent with a set of environments and just run one episode in each environment. Thus we can use the `env/done` stop variable and take the average over the cumulated reward of all environments."]}, {"cell_type": "markdown", "id": "2217baf6", "metadata": {"cell_marker": "\"\"\"", "id": "7_D4Fb4Fz6M1"}, "source": ["\n", "See [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about agents and environment agents."]}, {"cell_type": "code", "execution_count": null, "id": "ec7103e2", "metadata": {"executionInfo": {"elapsed": 348, "status": "ok", "timestamp": 1662009763237, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "hT5mr2yGyeUP", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["def get_env_agents(cfg) -> Tuple[AutoResetGymAgent, NoAutoResetGymAgent]:\n", "    # Train environment\n", "    train_env_agent = AutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.n_envs,\n", "        cfg.algorithm.seed,\n", "    )\n", "\n", "    # Evaluation environment\n", "    eval_env_agent = NoAutoResetGymAgent(\n", "    get_class(cfg.gym_env),\n", "    get_arguments(cfg.gym_env),\n", "    cfg.algorithm.nb_evals,\n", "    cfg.algorithm.seed,\n", "    )\n", "    return train_env_agent, eval_env_agent"]}, {"cell_type": "markdown", "id": "d31d7258", "metadata": {"cell_marker": "\"\"\"", "id": "EzxoIPtLVJ_i"}, "source": ["### Create the DQN agent"]}, {"cell_type": "markdown", "id": "e570fde2", "metadata": {"cell_marker": "\"\"\"", "id": "aaNnZw3bXEYd"}, "source": ["Interestingly, the loop between the policy and the environment is first defined as a collection of agents, and then embedded into a single TemporalAgent."]}, {"cell_type": "code", "execution_count": null, "id": "8accea01", "metadata": {"executionInfo": {"elapsed": 221, "status": "ok", "timestamp": 1662009894803, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "G8Uk_RQh8QrO", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n", "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n", "\n", "    # Get the two agents (critic and target critic)\n", "    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n", "    target_critic = copy.deepcopy(critic)\n", "\n", "    # Builds the train agent that will produce transitions\n", "    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n", "    tr_agent = Agents(train_env_agent, critic, explorer)\n", "    train_agent = TemporalAgent(tr_agent)\n", "\n", "    # Creates two temporal agents just for \"replaying\" some parts\n", "    # of the transition buffer    \n", "    q_agent = TemporalAgent(critic)\n", "    target_q_agent = TemporalAgent(target_critic)\n", "\n", "\n", "    # Get an agent that is executed on a complete workspace\n", "    ev_agent = Agents(eval_env_agent, critic)\n", "    eval_agent = TemporalAgent(ev_agent)\n", "    train_agent.seed(cfg.algorithm.seed)\n", "\n", "    return train_agent, eval_agent, q_agent, target_q_agent"]}, {"cell_type": "markdown", "id": "dcce9886", "metadata": {"cell_marker": "\"\"\"", "id": "lU3cO6znHyDc"}, "source": ["### The Logger class"]}, {"cell_type": "markdown", "id": "308cba4b", "metadata": {"cell_marker": "\"\"\"", "id": "_1gszmpwhitv"}, "source": ["Explanations for the logger were already given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing)."]}, {"cell_type": "code", "execution_count": null, "id": "c14262c1", "metadata": {"executionInfo": {"elapsed": 18, "status": "ok", "timestamp": 1662009763240, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "aOkauz_0H2GA", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["class Logger():\n", "\n", "  def __init__(self, cfg):\n", "    self.logger = instantiate_class(cfg.logger)\n", "\n", "  def add_log(self, log_string, loss, epoch):\n", "    self.logger.add_scalar(log_string, loss.item(), epoch)\n", "\n", "  # Log losses\n", "  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, actor_loss):\n", "    self.add_log(\"critic_loss\", critic_loss, epoch)\n", "    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n", "    self.add_log(\"actor_loss\", actor_loss, epoch)\n", "\n"]}, {"cell_type": "markdown", "id": "bc4b7b52", "metadata": {"cell_marker": "\"\"\"", "id": "f2vq1OJHWCIE"}, "source": ["### Setup the optimizers"]}, {"cell_type": "markdown", "id": "750124e9", "metadata": {"cell_marker": "\"\"\"", "id": "VzmEKF4J8qjg"}, "source": ["We use a single optimizer to tune the parameters of the actor (in the prob_agent part) and the critic (in the critic_agent part). It would be possible to have two optimizers which would work separately on the parameters of each component agent, but it would be more complicated because updating the actor requires the gradient of the critic."]}, {"cell_type": "code", "execution_count": null, "id": "681cb047", "metadata": {"executionInfo": {"elapsed": 17, "status": "ok", "timestamp": 1662009763241, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "YFfzXEu2WFWj", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["# Configure the optimizer over the q agent\n", "def setup_optimizers(cfg, q_agent):\n", "    optimizer_args = get_arguments(cfg.optimizer)\n", "    parameters = q_agent.parameters()\n", "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n", "    return optimizer"]}, {"cell_type": "markdown", "id": "851f09cb", "metadata": {"cell_marker": "\"\"\"", "id": "YQNvhO_VAJbh"}, "source": ["### Compute critic loss"]}, {"cell_type": "markdown", "id": "b98c5aff", "metadata": {"cell_marker": "\"\"\"", "id": "fxxobbxRaJXO"}, "source": ["Detailed explanations of the function to compute the critic loss when using a NoAutoResetGymAgent are given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV)."]}, {"cell_type": "markdown", "id": "342a55cf", "metadata": {"cell_marker": "\"\"\"", "id": "fXwrjbueoDw6"}, "source": ["The case where we use the AutoResetGymAgent is very similar, but we need to specify that we use the first part of the Q-values (`q_values[0]`) for representing $Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing $Q(s_{t+1},a)$, as these values are stored into a transition model."]}, {"cell_type": "code", "execution_count": null, "id": "c7e5b1c6", "metadata": {"executionInfo": {"elapsed": 204, "status": "ok", "timestamp": 1662012575928, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "2sepUK-gAM3u", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "    #Don't forget that we deal with transitions (and not episodes)\n", "\n", "    # Compute critic loss (no need to use must_bootstrap here since we are dealing with \"full\" transitions)\n", "    mse = nn.MSELoss()\n", "    critic_loss = mse(target, qvals)\n", "    return critic_loss"]}, {"cell_type": "markdown", "id": "64c14586", "metadata": {"cell_marker": "\"\"\"", "id": "Jmi91gANWT4z"}, "source": ["## Main training loop"]}, {"cell_type": "markdown", "id": "17155476", "metadata": {"cell_marker": "\"\"\"", "id": "8ixFZeCRN6Y6"}, "source": ["Note that everything about the shared workspace between all the agents is completely hidden under the hood. This results in a gain of productivity, at the expense of having to dig into the BBRL code if you want to understand the details, change the multiprocessing model, etc."]}, {"cell_type": "markdown", "id": "3473d899", "metadata": {"cell_marker": "\"\"\"", "id": "I6SuPOdW_hxl"}, "source": ["### Agent execution"]}, {"cell_type": "markdown", "id": "a1f11c52", "metadata": {"cell_marker": "\"\"\"", "id": "WqlH-8DaVWx2"}, "source": ["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]}, {"cell_type": "markdown", "id": "974f2a13", "metadata": {"cell_marker": "\"\"\"", "id": "bWAmm0pPotTC"}, "source": ["The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]}, {"cell_type": "markdown", "id": "5835c080", "metadata": {"cell_marker": "\"\"\"", "id": "Rn3MlNQ3qGPr"}, "source": ["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous colab](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]}, {"cell_type": "markdown", "id": "b23c6e32", "metadata": {"cell_marker": "\"\"\"", "id": "gAnnEjF9L9gk"}, "source": ["The [previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5) explains a lot of these details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line and the computation of `must_bootstrap`."]}, {"cell_type": "markdown", "id": "ca5f29a1", "metadata": {"cell_marker": "\"\"\"", "id": "OFB1XFE5YEc6"}, "source": ["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. `optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations"]}, {"cell_type": "code", "execution_count": null, "id": "85a398e6", "metadata": {"executionInfo": {"elapsed": 214, "status": "ok", "timestamp": 1662013643487, "user": {"displayName": "Olivier Sigaud", "userId": "07951496868143636810"}, "user_tz": -120}, "id": "sk85_sRWW-5s", "vscode": {"languageId": "python"}}, "outputs": [], "source": ["def run_dqn(cfg, compute_critic_loss):\n", "    # 1)  Build the  logger\n", "    logger = Logger(cfg)\n", "    best_reward = -10e9\n", "\n", "    # 2) Create the environment agent\n", "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n", "\n", "    # 3) Create the DQN-like Agent\n", "    train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(\n", "        cfg, train_env_agent, eval_env_agent\n", "    )\n", "\n", "    # 5) Configure the workspace to the right dimension\n", "    # Note that no parameter is needed to create the workspace.\n", "    # In the training loop, calling the agent() and critic_agent()\n", "    # will take the workspace as parameter\n", "    train_workspace = Workspace()  # Used for training\n", "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n", "\n", "    # 6) Configure the optimizer over the a2c agent\n", "    optimizer = setup_optimizers(cfg, q_agent)\n", "    nb_steps = 0\n", "    last_eval_step = 0\n", "    last_critic_update_step = 0\n", "\n", "    # 7) Training loop\n", "    for epoch in range(cfg.algorithm.max_epochs):\n", "        # Execute the agent in the workspace\n", "        if epoch > 0:\n", "            train_workspace.zero_grad()\n", "            train_workspace.copy_n_last_steps(1)\n", "            train_agent(\n", "                train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n", "            )\n", "        else:\n", "            train_agent(\n", "                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n", "            )\n", "\n", "        # Get the transitions\n", "        transition_workspace = train_workspace.get_transitions()\n", "\n", "        action = transition_workspace[\"action\"]\n", "        nb_steps += action[0].shape[0]\n", "        \n", "        # Adds the transitions to the workspace\n", "        rb.put(transition_workspace)\n", "        for _ in range(cfg.algorithm.n_updates):\n", "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n", "\n", "            # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace)\n", "            q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n", "            q_values, done, truncated, reward, action = rb_workspace[\n", "                \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n", "            ]\n", "\n", "            with torch.no_grad():\n", "                target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n", "            target_q_values = rb_workspace[\"q_values\"]\n", "\n", "            # Determines whether values of the critic should be propagated\n", "            # True if the episode reached a time limit or if the task was not done\n", "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n", "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n", "\n", "            if rb.size() > cfg.algorithm.learning_starts:\n", "              # Compute critic loss\n", "              critic_loss = compute_critic_loss(\n", "                  cfg, reward, must_bootstrap, q_values, target_q_values[1], action\n", "              )\n", "              # Store the loss for tensorboard display\n", "              logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n", "\n", "              optimizer.zero_grad()\n", "              critic_loss.backward()\n", "              torch.nn.utils.clip_grad_norm_(q_agent.parameters(), cfg.algorithm.max_grad_norm)\n", "              optimizer.step()\n", "              if nb_steps - last_critic_update_step > cfg.algorithm.target_critic_update:\n", "                  last_critic_update_step = nb_steps\n", "                  target_q_agent.agent = copy.deepcopy(q_agent.agent)\n", "\n", "        if nb_steps - last_eval_step > cfg.algorithm.eval_interval:\n", "            last_eval_step = nb_steps\n", "            eval_workspace = Workspace()  # Used for evaluation\n", "            eval_agent(\n", "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n", "            )\n", "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n", "            mean = rewards.mean()\n", "            logger.add_log(\"reward\", mean, nb_steps)\n", "            print(f\"epoch: {epoch}, reward: {mean}\")\n", "            if cfg.save_best and mean > best_reward:\n", "                best_reward = mean\n", "                directory = \"./dqn_critic/\"\n", "                if not os.path.exists(directory):\n", "                    os.makedirs(directory)\n", "                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n", "                eval_agent.save_model(filename)\n", "\n"]}, {"cell_type": "markdown", "id": "2fd0f9b2", "metadata": {"cell_marker": "\"\"\"", "id": "uo6bc3zzKua_"}, "source": ["## Definition of the parameters\n", "\n", "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation.\n", "\n", "### Launching tensorboard to visualize the results"]}, {"cell_type": "code", "execution_count": null, "id": "1c686610", "metadata": {}, "outputs": [], "source": ["# For Colab - otherwise, it is easier and better to launch tensorboard from\n", "# the terminal\n", "if get_ipython().__class__.__module__ == \"google.colab._shell\":\n", "    pass\n", "    %load_ext tensorboard\n", "    %tensorboard --logdir ./tmp\n", "else:\n", "    import sys\n", "    import os\n", "    import os.path as osp\n", "    print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={os.getcwd()}/tmp\")"]}, {"cell_type": "code", "execution_count": null, "id": "d02e12bd", "metadata": {}, "outputs": [], "source": ["params={\n", "  \"save_best\": False,\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": \"./tmp/dqn-buffer-\" + str(time.time()),\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 4,\n", "    \"max_grad_norm\": 0.5,\n", "    \"epsilon\": 0.02,\n", "    \"n_envs\": 8,\n", "    \"n_steps\": 32,\n", "    \"n_updates\": 32,\n", "    \"eval_interval\": 2000,\n", "    \"learning_starts\": 2000,\n", "    \"nb_evals\": 10,\n", "    \"buffer_size\": 1e6,\n", "    \"batch_size\": 256,\n", "    \"target_critic_update\": 5000,\n", "    \"max_epochs\": 3500,\n", "    \"discount_factor\": 0.99,\n", "    \"architecture\":{\"hidden_size\": [128, 128]},\n", "  },\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_env\",\n", "    \"env_name\": \"CartPole-v1\",\n", "  },\n", "  \"optimizer\":\n", "  {\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 1e-3,\n", "  }\n", "}\n", "\n", "config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)\n", "run_dqn(config, compute_critic_loss)"]}, {"cell_type": "markdown", "id": "ce94f713", "metadata": {"cell_marker": "\"\"\"", "id": "lou6LTvVK7YY"}, "source": ["The version used in this colab uses $< s_t, a_t, r_t, s_{t+1}>$ samples. As an exercise, you may switch to $< s_t, a_t, r_{t+1}, s_{t+1}>$ samples, going back to the standard SaLinA notation. For that, replace the import to `bbrl.agents.gyma` instead of `gymb`, and change the temporal difference update rule (in `compute_critic_loss(...)`) accordingly. See [this notebook](https://colab.research.google.com/drive/1Cld72_FBA1aMS2U4EsyV3LGZIlQC_PsC?usp=sharing) for more explanations.\n", "\n", "## Coding Exercise: Double DQN (DDQN)\n", "\n", "In DQN, the same network is responsible for selecting and estimating the best next action (in the TD-target) and that may lead to over-estimation: the action which q-value is over-estimated will be chosen more often. As a result, training is slower.\n", "\n", "To reduce over-estimation, double q-learning (and then DDQN) was proposed. It decouples the action selection from the value estimation.\n", "\n", "Concretely, in DQN, the target value in the critic loss (used to update the Q critic) for a sample at time $t$ is defined as:\n", "\n", "$$Y^{DQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right); \\mathbb{\\theta}_{target}\\right)$$\n", "\n", "where the target network `target_q_agent` with parameters $\\mathbb{\\theta}_{target}$ is used for both action selection and estimation, and can therefore be rewritten:\n", "\n", "$$Y^{DQN}_{t} = r_{t+1} + \\gamma \\max_{a}{Q}\\left(s_{t+1}, a; \\mathbb{\\theta}_{target}\\right)$$\n", "\n", "Instead, DDQN uses the online critic `q_agent` with parameters $\\mathbb{\\theta}_{online}$ to select the action, whereas it uses the target network `target_q_agent` to estimate the associated Q-values:\n", "\n", "$$Y^{DDQN}_{t} = r_{t+1} + \\gamma{Q}\\left(s_{t+1}, \\arg\\max_{a}Q\\left(s_{t+1}, a; \\mathbb{\\theta}_{online}\\right); \\mathbb{\\theta}_{target}\\right)$$\n", "\n", "\n", "The goal in this exercise is for you to write the update method for `DDQN`."]}, {"cell_type": "code", "execution_count": null, "id": "04e5b7ea", "metadata": {}, "outputs": [], "source": ["def compute_ddqn_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "    # Compute critic loss\n", "    mse = nn.MSELoss()\n", "    critic_loss = mse(target, qvals)\n", "    return critic_loss"]}, {"cell_type": "code", "execution_count": null, "id": "025137ec", "metadata": {}, "outputs": [], "source": ["params={\n", "  \"save_best\": False,\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": \"./tmp/ddqn-buffer-\" + str(time.time()),\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 4,\n", "    \"max_grad_norm\": 0.5,\n", "    \"epsilon\": 0.02,\n", "    \"n_envs\": 8,\n", "    \"n_steps\": 32,\n", "    \"eval_interval\": 2000,\n", "    \"learning_starts\": 2000,\n", "    \"nb_evals\": 10,\n", "    \"buffer_size\": 1e6,\n", "    \"batch_size\": 256,\n", "    \"target_critic_update\": 5000,\n", "    \"max_epochs\": 3500,\n", "    \"discount_factor\": 0.99,\n", "    \"architecture\":{\"hidden_size\": [128, 128]},\n", "  },\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_env\",\n", "    \"env_name\": \"CartPole-v1\",\n", "  },\n", "  \"optimizer\":\n", "  {\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 1e-3,\n", "  }\n", "}\n", "\n", "config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)\n", "run_dqn(config, compute_ddqn_loss)"]}], "metadata": {"jupytext": {"cell_markers": "\"\"\""}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}
