{"cells": [{"cell_type": "markdown", "id": "45919e41", "metadata": {"cell_marker": "\"\"\"", "id": "t5Y434EKaEji"}, "source": ["# Outlook #"]}, {"cell_type": "markdown", "id": "1c3b4701", "metadata": {"cell_marker": "r\"\"\"", "id": "7t49ETcZaJg3"}, "source": ["In this colab we will study basic reinforcement learning algorithms: TD learning, q-learning and sarsa. We will also investigate two basic exploration strategies: $\\epsilon$-greedy and softmax."]}, {"cell_type": "markdown", "id": "090e2bd1", "metadata": {"cell_marker": "\"\"\"", "id": "pfP_irDZNa0J"}, "source": ["# Installation #"]}, {"cell_type": "code", "execution_count": null, "id": "9fbb6ed7", "metadata": {}, "outputs": [], "source": ["\"\"\"!pip install easypip\"\"\""]}, {"cell_type": "code", "execution_count": null, "id": "7ea75deb", "metadata": {}, "outputs": [], "source": ["\n", "from easypip import easyimport\n", "easyimport(\"bbrl_gym\")\n", "import os\n", "from typing import Tuple, List\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from tqdm.notebook import tqdm\n", "\n", "from mazemdp.toolbox import egreedy, egreedy_loc, softmax, sample_categorical\n", "from mazemdp.maze_plotter import show_videos\n", "from mazemdp.mdp import Mdp\n", "from my_gym.envs.maze_mdp import MazeMDPEnv\n", "\n", "# For visualization\n", "os.environ[\"VIDEO_FPS\"] = \"5\"\n", "if not os.path.isdir(\"./videos\"):\n", "    os.mkdir(\"./videos\")\n", "\n", "from IPython.display import Video\n", "\n", "# Settings\n", "NB_EPISODES = 50\n", "TIMEOUT = 25\n"]}, {"cell_type": "markdown", "id": "fa05d274", "metadata": {"cell_marker": "\"\"\"", "id": "ytERHME1NfYr"}, "source": ["# Reinforcement Learning\n", "\n", "Reinforcement Learning is about finding the optimal policy in an MDP which is initially unknown to the agent. More precisely, the state and action spaces are known, but the agent does not know the transition and reward functions. Generally speaking, the agent has to explore the MDP to figure out which action in which state leads to which other state and reward. The model-free case is about finding this optimal policy just through very local updates, without storing any information about previous interactions with the environment. Principles of these local updates can already be found in the Temporal Difference (TD) algorithm, which iteratively computes optimal values for all state using local updates.\n", "The most widely used model-free RL algorithms are **q-learning**, **sarsa** and **actor-critic** algorithms. Below we focus on the first two.\n", "\n", "As for dynamic programming, we first create a maze-like MDP. Reinforcement learning is slower than dynamic programming, so we will work with smaller mazes."]}, {"cell_type": "code", "execution_count": null, "id": "fa5c9074", "metadata": {"id": "srNM1JqTtYAc"}, "outputs": [], "source": ["import gym\n", "import my_gym\n", "\n", "# Environment with a little bit of negative reward (when hitting a wall)\n", "env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2, \"hit\": 0.0})\n", "env.reset()\n", "\n", "# in dynamic programming, there is no agent moving in the environment\n", "env.init_draw(\"The maze\")"]}, {"cell_type": "markdown", "id": "d1ae2869", "metadata": {"cell_marker": "\"\"\"", "id": "fSbk6GyAN0-e"}, "source": ["## Temporal Difference (TD) learning ##"]}, {"cell_type": "markdown", "id": "8188c712", "metadata": {"cell_marker": "r\"\"\"", "id": "JNnNJpTON5aU"}, "source": ["\n", "Given a state and an action spaces as well as a policy, TD(0) computes the state value of this policy based on the following equations:\n", "$$\\delta_t = r(s_t,a_t) + \\gamma V^{(t)}(s_{t+1})-V^{(t)}(s_t)$$\n", "$$V^{(t+1)}(s_t) = V^{(t)}(s_t) + \\alpha\\delta_t$$\n", "\n", "where $\\delta$ is the TD error and $\\alpha$ is a parameter called \"learning rate\".\n", "\n", "The code is provided below, so that you can take inspiration later on. The important part is the computation of $\\delta$, and the update of the values of $V$.\n", "\n", "To run TD learning, a policy is needed as input. Such a policy can be retreived by using the `policy_iteration_q(mdp)` function defined in the dynamic programming notebook.\n", "\n", "If you want to run this notebook independently, you can use instead the `random_policy` provided in `mazemdp`. This is what we do here by default, replace it if you want to run TD learning from an optimal policy."]}, {"cell_type": "code", "execution_count": null, "id": "e3a7d586", "metadata": {"id": "xniYyOwU8C3b"}, "outputs": [], "source": ["from mazemdp import random_policy"]}, {"cell_type": "markdown", "id": "e3d1e952", "metadata": {"cell_marker": "\"\"\"", "id": "f_YBjgxOPCn8"}, "source": ["**Question:** In the code of the *temporal_difference(...)* function below, fill the missing parts with # some code = ... "]}, {"cell_type": "code", "execution_count": null, "id": "f3965324", "metadata": {"id": "0lL1I8Jx6Bzf"}, "outputs": [], "source": ["def temporal_difference(\n", "    mdp: MazeMDPEnv,\n", "    policy: np.ndarray,\n", "    nb_episodes: int = 50,\n", "    alpha: float = 0.2,\n", "    timeout: int = 25,\n", "    render: bool = True,\n", ") -> np.ndarray:\n", "    # alpha: learning rate\n", "    # timeout: timeout of an episode (maximum number of timesteps)\n", "    v = np.zeros(mdp.nb_states)  # initial state value v\n", "    mdp.timeout = timeout\n", "\n", "    if render:\n", "        mdp.init_draw(\"Temporal differences\")\n", "\n", "    for _ in tqdm(range(nb_episodes)):  # for each episode\n", "\n", "        # Draw an initial state randomly (if uniform is set to False, the state is drawn according to the P0\n", "        #                                 distribution)\n", "        x = mdp.reset(uniform=True)\n", "        done = False\n", "        while not done:  # update episode at each timestep\n", "            # Show agent\n", "            if render:\n", "                mdp.draw_v_pi(v, policy)\n", "\n", "            # Step forward following the MDP: x=current state,\n", "            #                                 pol[i]=agent's action according to policy pol,\n", "            #                                 r=reward gained after taking action pol[i],\n", "            #                                 done=tells whether the episode ended,\n", "            #                                 and info gives some info about the process\n", "            [y, r, done, _] = mdp.step(egreedy_loc(policy[x], mdp.action_space.n, epsilon=0.2))\n", "\n", "            # \u00c0 compl\u00e9ter...  \n", "            assert False, 'Code non impl\u00e9ment\u00e9'\n", "            delta = ...\n", "            v[x] = ...\n", "\n", "            # Update agent's position (state)\n", "            x = y\n", "\n", "    if render:\n", "        # Show the final policy\n", "        mdp.current_state = 0\n", "        mdp.draw_v_pi(v, policy, title=\"Temporal Differences\")\n", "    return v"]}, {"cell_type": "markdown", "id": "1dcfeab4", "metadata": {"cell_marker": "\"\"\"", "id": "8f0swnV9tlym"}, "source": ["Once this is done, you can run it.\n", "\n", "Unless you are lucky, the value function learned from a random policy will most probably be boring, as if it does not reach the rewarded state, all values keep at 0. To watch the behavior of the TD algorithm from an optimal policy, you can copy-paste from the dynamic programming lab the value iteration algorithm, use it to obtain an optimal policy and then call the TD algorithm from the obtained policy. "]}, {"cell_type": "markdown", "id": "4e951b14", "metadata": {"cell_marker": "\"\"\""}, "source": ["policy = random_policy(env)\n", "v = temporal_difference(env, policy, nb_episodes=NB_EPISODES, timeout=TIMEOUT)\n", "```\n", "\n", "show_videos(\"videos/\", \"Temporal\")\n", "```\n", "\n", "Unless you were lucky, the generated value function is boring: if the policy does not reach the final state, all values are 0. To avoid this, you can copy-paste a dynamic programming function from the previous notebook, use it to get an optimal policy, and use this policy for TD learning. You should get a much more interesting value function."]}, {"cell_type": "code", "execution_count": null, "id": "dc2ea431", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["# \u00c0 compl\u00e9ter...  \n", "assert False, 'Code non impl\u00e9ment\u00e9'\n"]}, {"cell_type": "markdown", "id": "4578c9ee", "metadata": {"cell_marker": "\"\"\"", "id": "wtog6PFsPhCn"}, "source": ["## Q-learning ##"]}, {"cell_type": "markdown", "id": "13d75b68", "metadata": {"cell_marker": "r\"\"\"", "id": "jP5cC07-Pjxl"}, "source": ["\n", "The **q-learning** algorithm accounts for an agent exploring an MDP and updating at each step a model of the state action-value function stored into a Q-table. It is updated as follows:\n", "\n", "$$\\delta_t = r(s_t,a_t) + \\gamma \\max_{a \\in A} Q^{(t)}(s_{t+1},a)-Q^{(t)}(s_t,a_t)$$\n", "\n", "$$Q^{(t+1)}(s_t,a_t) = Q^{(t)}(s_t,a_t) + \\alpha \\delta_t.$$"]}, {"cell_type": "markdown", "id": "a065145f", "metadata": {"cell_marker": "\"\"\"", "id": "2gVzVZCLxPh8"}, "source": ["To visualize the policy, we need the `get_policy_from_q(q)` function that we defined in the dynamic programming notebook. Import it below."]}, {"cell_type": "code", "execution_count": null, "id": "65c6e7e9", "metadata": {"id": "tUlSmESgxhXQ"}, "outputs": [], "source": ["def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n", "    # Outputs a policy given the action values\n", "    return np.argmax(q, axis=1)"]}, {"cell_type": "markdown", "id": "f08a4dcc", "metadata": {"cell_marker": "\"\"\"", "id": "7pq3cgWNPu_t"}, "source": ["**Question:**  Fill the code of the `q_learning(...)` function below."]}, {"cell_type": "code", "execution_count": null, "id": "71f315f8", "metadata": {"id": "LhM1c2Ct6dKb"}, "outputs": [], "source": ["# --------------------------- Q-Learning epsilon-greedy version -------------------------------#\n", "\n", "# Given an exploration rate epsilon, the QLearning algorithm computes the state action-value function\n", "# based on an epsilon-greedy policy\n", "# alpha is the learning rate\n", "\n", "\n", "def q_learning_eps(\n", "    mdp: MazeMDPEnv,\n", "    epsilon: float,\n", "    nb_episodes: int = 20,\n", "    timeout: int = 50,\n", "    alpha: float = 0.5,\n", "    render: bool = True,\n", "    init_q: float = 0.,\n", "    uniform: bool =  True\n", ") -> Tuple[np.ndarray, List[float]]:\n", "    # Initialize the state-action value function\n", "    # alpha is the learning rate\n", "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q[:, :] = init_q\n", "    q_list = []\n", "\n", "    # Run learning cycle\n", "    mdp.timeout = timeout  # episode length\n", "\n", "    if render:\n", "        mdp.init_draw(\"Q-learning e-greedy\")\n", "\n", "    for _ in tqdm(range(nb_episodes)):\n", "        # Draw the first state of episode i using a uniform distribution over all the states\n", "        x = mdp.reset(uniform=uniform)\n", "        done = False\n", "        while not done:\n", "            if render:\n", "                # Show the agent in the maze\n", "                mdp.draw_v_pi(q, q.argmax(axis=1))\n", "\n", "            # Draw an action using an epsilon-greedy policy\n", "            u = egreedy(q, x, epsilon)\n", "\n", "            # Perform a step of the MDP\n", "            [y, r, done, _] = mdp.step(u)\n", "\n", "            # \u00c0 compl\u00e9ter...  \n", "            assert False, 'Code non impl\u00e9ment\u00e9'\n", "            delta = ...\n", "            q[...] = ...\n", "\n", "            # Update the agent position\n", "            x = y\n", "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n", "\n", "    if render:\n", "        # Show the final policy\n", "        mdp.current_state = 0\n", "        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Q-learning e-greedy\")\n", "    return q, q_list"]}, {"cell_type": "markdown", "id": "8c5ff44e", "metadata": {"cell_marker": "\"\"\"", "id": "ME7SAuggvNfw"}, "source": ["And run it."]}, {"cell_type": "markdown", "id": "e4788f39", "metadata": {"cell_marker": "\"\"\""}, "source": ["epsilon = 0.02\n", "q, q_list = q_learning_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT)\n", "```\n", "\n", "Vous remarquerez que la convergence peut \u00eatre assez lente; une fa\u00e7on\n", "\n", "show_videos(\"videos/\", \"Q-learning\")\n", "```\n", "\n", "## Harder case: start point and exploration"]}, {"cell_type": "markdown", "id": "eeb45f41", "metadata": {"cell_marker": "\"\"\""}, "source": ["We now explore the case where the agent always start at the *beginning of the maze* (`uniform=False`). "]}, {"cell_type": "code", "execution_count": null, "id": "13a9309c", "metadata": {}, "outputs": [], "source": ["epsilon = 0.02\n", "start_q, start_q_list = q_learning_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT, uniform=False)"]}, {"cell_type": "markdown", "id": "e9ca1a89", "metadata": {"cell_marker": "\"\"\""}, "source": ["You will observe that it is very difficult for the agent to learn to reach the final state (and the larger the maze, the more difficult). A simple trick to avoid this is to initialize the value of each $(s,a)$ pair to a small (lower than the final reward) value. Try it with the example above !"]}, {"cell_type": "code", "execution_count": null, "id": "1608513a", "metadata": {}, "outputs": [], "source": ["# \u00c0 compl\u00e9ter...  \n", "assert False, 'Code non impl\u00e9ment\u00e9'\n"]}, {"cell_type": "markdown", "id": "011fd2cb", "metadata": {"cell_marker": "\"\"\"", "id": "MnLbmQsbQEIO"}, "source": ["### Learning dynamics"]}, {"cell_type": "markdown", "id": "6d180f82", "metadata": {"cell_marker": "\"\"\"", "id": "M_6fmrxlQHR3"}, "source": ["\n", "By watching carefully the values while the agent is learning, you can see that the agent favors certains paths over others which have a strictly equivalent value. This can be explained easily: as the agent chooses a path for the first time, it updates the values along that path, these values get higher than the surrounding values, and the agent chooses the same path again and again, increasing the phenomenon. Only steps of random exploration can counterbalance this effect, but they do so extremely slowly."]}, {"cell_type": "markdown", "id": "cf4240cb", "metadata": {"cell_marker": "\"\"\"", "id": "JsKOa2i1QMcl"}, "source": ["### Exploration ###"]}, {"cell_type": "markdown", "id": "f5d684ec", "metadata": {"cell_marker": "r\"\"\"", "id": "PXkEV9-cQQnP"}, "source": ["\n", "In the `q_learning(...)` function above, action selection is based on a $\\epsilon$-greedy policy. Instead, it could have relied on *`softmax`*."]}, {"cell_type": "markdown", "id": "d8536698", "metadata": {"cell_marker": "r\"\"\"", "id": "whX-_jP4QsrU"}, "source": ["**Question:** In the function below, you have to replace the call to the previous *$\\epsilon$-greedy* policy with a `softmax` policy. The `softmax(...)` and `egreedy(...)` functions are available in `mazemdp.toolbox`."]}, {"cell_type": "code", "execution_count": null, "id": "3e72bb8c", "metadata": {"id": "iu4WiXRm6mdL"}, "outputs": [], "source": ["# --------------------------- Q-Learning softmax version ----------------------------#\n", "\n", "# Given a temperature \"tau\", the QLearning algorithm computes the state action-value function\n", "# based on a softmax policy\n", "# alpha is the learning rate\n", "\n", "\n", "def q_learning_soft(\n", "    mdp: MazeMDPEnv,\n", "    tau: float,\n", "    nb_episodes: int = 20,\n", "    timeout: int = 50,\n", "    alpha: float = 0.5,\n", "    render: bool = True,\n", ") -> Tuple[np.ndarray, List[float]]:\n", "    # Initialize the state-action value function\n", "    # alpha is the learning rate\n", "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_list = []\n", "\n", "    # Run learning cycle\n", "    mdp.timeout = timeout  # episode length\n", "\n", "    if render:\n", "        mdp.init_draw(\"Q-learning softmax\")\n", "\n", "    for _ in tqdm(range(nb_episodes)):\n", "        # Draw the first state of episode i using a uniform distribution over all the states\n", "        x = mdp.reset(uniform=True)\n", "        done = False\n", "        while not done:\n", "            if render:\n", "                # Show the agent in the maze\n", "                mdp.draw_v_pi(q, q.argmax(axis=1))\n", "\n", "            # \u00c0 compl\u00e9ter...  \n", "            assert False, 'Code non impl\u00e9ment\u00e9'\n", "            u = ... # (here, call the softmax function)\n", "            \n", "            # \u00c0 compl\u00e9ter...  \n", "            assert False, 'Code non impl\u00e9ment\u00e9'\n", "            \n", "            x = y\n", "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n", "\n", "    if render:\n", "        # Show the final policy\n", "        mdp.current_state = 0\n", "        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Q-learning softmax\")\n", "    return q, q_list\n", "\n"]}, {"cell_type": "markdown", "id": "379a190c", "metadata": {"cell_marker": "\"\"\"", "id": "Xevh4vMxvm-q"}, "source": ["Run this new version"]}, {"cell_type": "code", "execution_count": null, "id": "3712dceb", "metadata": {"id": "FcAwK52avpur"}, "outputs": [], "source": ["epsilon = 0.02\n", "q, q_list = q_learning_soft(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT)"]}, {"cell_type": "code", "execution_count": null, "id": "fa3d07ed", "metadata": {"id": "0iXJ-j-i_5pK"}, "outputs": [], "source": ["show_videos(\"videos/\", \"Q-learningsoftmax\")"]}, {"cell_type": "markdown", "id": "e44dde05", "metadata": {"cell_marker": "\"\"\"", "id": "PW4wU1xjRT8J"}, "source": ["## Sarsa ##"]}, {"cell_type": "markdown", "id": "e4d39ca7", "metadata": {"cell_marker": "\"\"\"", "id": "bxDs-SywRWS6"}, "source": ["\n", "The **sarsa** algorithm is very similar to **q-learning**. At first glance, the only difference is in the update rule. However, to perform the update in **sarsa**, one needs to know the action the agent will take when it will be at the next state, even if the agent is taking a random action.\n", "\n", "This implies that the next state action is determined in advance and stored for being played at the next time step."]}, {"cell_type": "markdown", "id": "f2a637bb", "metadata": {"cell_marker": "\"\"\"", "id": "CUdQEF0Rv2uc"}, "source": ["**Question:** Fill the code below"]}, {"cell_type": "code", "execution_count": null, "id": "210aaa2a", "metadata": {"id": "93VUGbwM6wkq"}, "outputs": [], "source": ["# --------------------------- Sarsa, epsilon-greedy version -------------------------------#\n", "\n", "# Given an exploration rate epsilon, the SARSA algorithm computes the state action-value function\n", "# based on an epsilon-greedy policy\n", "# alpha is the learning rate\n", "def sarsa_eps(\n", "    mdp: MazeMDPEnv,\n", "    epsilon: float,\n", "    nb_episodes: int = 20,\n", "    timeout: int = 50,\n", "    alpha: float = 0.5,\n", "    render: bool = True,\n", ") -> Tuple[np.ndarray, List[float]]:\n", "    # Initialize the state-action value function\n", "    # alpha is the learning rate\n", "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_list = []\n", "\n", "    # Run learning cycle\n", "    mdp.timeout = timeout  # episode length\n", "\n", "    if render:\n", "        mdp.init_draw(\"Sarsa e-greedy\")\n", "\n", "    for _ in tqdm(range(nb_episodes)):\n", "        # Draw the first state of episode i using a uniform distribution over all the states\n", "        x = mdp.reset(uniform=True)\n", "        done = False\n", "\n", "        # \u00c0 compl\u00e9ter...  \n", "        assert False, 'Code non impl\u00e9ment\u00e9'\n", "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n", "\n", "    if render:\n", "        # Show the final policy\n", "        mdp.current_state = 0\n", "        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Sarsa e-greedy\")\n", "    return q, q_list"]}, {"cell_type": "markdown", "id": "fd4de927", "metadata": {"cell_marker": "\"\"\"", "id": "i8obKDfc0rhK"}, "source": ["And run it."]}, {"cell_type": "code", "execution_count": null, "id": "6fc68633", "metadata": {"id": "TgY_9mvU0tUr"}, "outputs": [], "source": ["epsilon = 0.02\n", "q, q_list = sarsa_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT)"]}, {"cell_type": "code", "execution_count": null, "id": "dc5995bb", "metadata": {"id": "opuFUT3u_-_I"}, "outputs": [], "source": ["show_videos(\"videos/\", \"Sarsae\")"]}, {"cell_type": "markdown", "id": "fdf150b2", "metadata": {"cell_marker": "\"\"\"", "id": "hbtNZZTNhRdx"}, "source": ["As for **q-learning** above, copy-paste the resulting code to get a *sarsa_soft(...)* and a *sarsa_eps(...)* function."]}, {"cell_type": "code", "execution_count": null, "id": "e1ca5d4c", "metadata": {"id": "UFMuH9Iq6026"}, "outputs": [], "source": ["# --------------------------- Sarsa, softmax version -------------------------------#\n", "\n", "# Given a temperature \"tau\", the SARSA algorithm computes the state action-value function\n", "# based on a softmax policy\n", "# alpha is the learning rate\n", "def sarsa_soft(\n", "    mdp: MazeMDPEnv,\n", "    tau: float,\n", "    nb_episodes: int = 20,\n", "    timeout: int = 50,\n", "    alpha: float = 0.5,\n", "    render: bool = True,\n", ") -> Tuple[np.ndarray, List[float]]:\n", "\n", "    # Initialize the state-action value function\n", "    # alpha is the learning rate\n", "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n", "    q_list = []\n", "\n", "    # Run learning cycle\n", "    mdp.timeout = timeout  # episode length\n", "\n", "    if render:\n", "        mdp.init_draw(\"Sarsa softmax\")\n", "\n", "    for _ in tqdm(range(nb_episodes)):\n", "        # Draw the first state of episode i using a uniform distribution over all the states\n", "        x = mdp.reset(uniform=True)\n", "        done = False\n", "\n", "        # \u00c0 compl\u00e9ter...  \n", "        assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n", "\n", "    if render:\n", "        # Show the final policy\n", "        mdp.current_state = 0\n", "        mdp.draw_v_pi(q, get_policy_from_q(q), title=\"Sarsa softmax\")\n", "    return q, q_list\n", "\n"]}, {"cell_type": "markdown", "id": "2c360ddd", "metadata": {"cell_marker": "\"\"\"", "id": "koHzhn0h02aa"}, "source": ["And run it."]}, {"cell_type": "code", "execution_count": null, "id": "4e8ce641", "metadata": {"id": "SVW4kFli04GB"}, "outputs": [], "source": ["tau = 6\n", "q, q_list = sarsa_soft(env, tau, nb_episodes=NB_EPISODES)"]}, {"cell_type": "code", "execution_count": null, "id": "2b334106", "metadata": {"id": "cUDvBXyKAEHo"}, "outputs": [], "source": ["show_videos(\"videos/\", \"Sarsasoftmax\")"]}, {"cell_type": "markdown", "id": "0dc692d0", "metadata": {"cell_marker": "\"\"\"", "id": "G46ht0tRWLj6"}, "source": ["## Study part"]}, {"cell_type": "markdown", "id": "34c4966c", "metadata": {"cell_marker": "\"\"\"", "id": "1BceAWIYYFg5"}, "source": ["### Impact of `epsilon` and `tau` on q-learning and sarsa"]}, {"cell_type": "markdown", "id": "8c0e7533", "metadata": {"cell_marker": "r\"\"\"", "id": "3B8GdWVfX5N2"}, "source": ["Compare the number of steps needed by **q-learning** and **sarsa** to converge on a given MDP using the *softmax* and *$\\epsilon$-greedy* exploration strategies. To figure out, you can use the provided `plot_ql_sarsa(m, epsilon, tau, nb_episodes, timeout, alpha, render)` function below with various values for $\\epsilon$ (e.g. 0.001, 0.01, 0.1) and $\\tau$ (e.g. 0.1, 5, 10) and comment the obtained curves. Other visualizations are welcome."]}, {"cell_type": "markdown", "id": "2ed92d89", "metadata": {"cell_marker": "\"\"\""}, "source": ["Note that instead of the temperature `tau`, computational neuroscience researchers, who generally prefer softmax exploration, use a parameter `beta` which behaves as an inverse of the temperature. That way, the three hyper-parameters of basic tabular RL algorithms are `alpha`, `beta`, and `gamma`."]}, {"cell_type": "code", "execution_count": null, "id": "127fd809", "metadata": {"id": "P9kycY4D696H"}, "outputs": [], "source": ["\n", "# -------- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax ----------#\n", "\n", "def plot_ql_sarsa(m, epsilon, tau, nb_episodes, timeout, alpha, render):\n", "    q, q_list1 = q_learning_eps(m, epsilon, nb_episodes, timeout, alpha, render)\n", "    q, q_list2 = q_learning_soft(m, tau, nb_episodes, timeout, alpha, render)\n", "    q, q_list3 = sarsa_eps(m, epsilon, nb_episodes, timeout, alpha, render)\n", "    q, q_list4 = sarsa_soft(m, tau, nb_episodes, timeout, alpha, render)\n", "\n", "    plt.clf()\n", "    plt.plot(range(len(q_list1)), q_list1, label='q-learning epsilon')\n", "    plt.plot(range(len(q_list2)), q_list2, label='q-learning tau')\n", "    plt.plot(range(len(q_list3)), q_list3, label='sarsa epsilon')\n", "    plt.plot(range(len(q_list4)), q_list4, label='sarsa tau')\n", "\n", "    plt.xlabel('Number of episodes')\n", "    plt.ylabel('Norm of Q values')\n", "    plt.legend(loc='lower right')\n", "    # plt.savefig(\"comparison_RL.png\")\n", "    plt.title(\"Comparison of convergence rates\")\n", "    plt.show()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "22c74253", "metadata": {"id": "QjSmclM31MkU"}, "outputs": [], "source": ["# example\n", "tau = 6\n", "epsilon = 0.02\n", "plot_ql_sarsa(env, epsilon, tau, 1000, 50, 0.5, False)"]}, {"cell_type": "markdown", "id": "d7e5dc2d", "metadata": {"cell_marker": "\"\"\"", "id": "sDquKHXZiPmA"}, "source": ["### Effect of hyper-parameters"]}, {"cell_type": "markdown", "id": "5d40d2d4", "metadata": {"cell_marker": "r\"\"\"", "id": "JqnYA-fKiTU3"}, "source": ["The other two hyper-parameters of **q-learning** and **sarsa** are $\\alpha$, and $\\gamma$. By varying the values of these hyper-parameters and watching the learning process and behavior of the agent, explain their impact on the algorithm. Using additional plotting functions is also welcome."]}], "metadata": {"jupytext": {"cell_markers": "\"\"\""}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}
