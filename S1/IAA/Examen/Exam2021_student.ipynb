{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'apprentissage automatique\n",
    "\n",
    "*Maxime Sangnier*\n",
    "\n",
    "Octobre, 2021\n",
    "\n",
    "## Contrôle continu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Consignes](#part1)\n",
    "1. [Exercice](#part2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consignes <a id=\"part1\"></a>\n",
    "\n",
    "1. Vos réponses doivent se trouver dans les cellules adéquates de **ce fichier**.\n",
    "1. En fin d'épreuve, **exportez** votre notebook en fichier `html`.\n",
    "1. **Téléversez** ensuite les fichiers `ipynb` **et** `html` dans ce dossier distant : [https://www.dropbox.com/request/oPQXE75uYGUWHtENZvqK](https://www.dropbox.com/request/oPQXE75uYGUWHtENZvqK).\n",
    "1. La durée de l'épreuve est **1h**. Tous les documents sont autorisés.\n",
    "\n",
    "Si vous n’arrivez pas à faire une question, **continuez** quand même l'exercice avec les éléments à votre disposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Packages:\n",
      "\tnympy as np\n",
      "\tmatplotlib.pyplot as plt\n",
      "\tseaborn as sns\n",
      "\n",
      "Functions:\n",
      "\tplotXY\n",
      "\tplot_frontiere\n",
      "\tmap_regions\n",
      "\tcovariance\n",
      "\tplot_cov\n",
      "\tsample_gmm\n",
      "\tscatter\n",
      "\tplot_level_set\n",
      "\tgaussian_sample\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "path.append('../')\n",
    "from mllab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice <a id=\"part2\"></a>\n",
    ">Considérons le jeu de données défini par la matrice $X \\in \\mathbb R^{n \\times 2}$ et par le vecteur d'étiquettes $y$ (respectivement représentés par `X` et `y`) suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "n = 200\n",
    "\n",
    "np.random.seed(314)\n",
    "X = np.random.randn(n, d) + np.r_[np.tile([2, 1], (n//2, 1)), np.tile([-2, -1], (n//2, 1))]\n",
    "w0 = 1.5*np.random.randn(2)\n",
    "b0 = -np.mean(X@w0) + 0.5*np.random.randn()\n",
    "p = 1 / (1 + np.exp(-(X@w0 + b0)))\n",
    "y = 2 * np.random.binomial(1, p) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 1.**\n",
    "Afficher les probabilités `p` en fonction de `X@w0+b0`.\n",
    "Quel est le modèle de génération des données (`X`, `y`) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 2.**\n",
    "Afficher les données `X` en distingant leur classe par des couleurs différentes et tracer la droite d'équation $w_0^\\top x +  b_0 = 0$ (on pourra s'aider de la fonction définie ci-dessous).\n",
    "Que représente cette droite ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fy = lambda x: -(b0 + w0[0]*x)/w0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 3.**\n",
    "Soit la fonction de perte définie, pour tout $x \\in \\mathbb R$, par\n",
    "$$\n",
    "\\ell(x) = -4x \\mathbb 1_{x<-1} + \\max (0, 1-x)^2 \\mathbb 1_{x \\ge -1}.\n",
    "$$\n",
    "Définir une fonction `loss(x)` retournant $[\\ell(x_1), \\dots, \\ell(x_n)]$ pour `x`$=(x_1, \\dots, x_n) \\in \\mathbb R^n$.\n",
    "Représenter les graphes de $\\ell$ et de la fonction $x \\mapsto \\mathbb 1_{x<0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 4.**\n",
    "En remarquant que $\\ell$ est dérivable, définir une fonction `diff_loss(x)` retournant $[\\ell'(x_1), \\dots, \\ell'(x_n)]$ pour `x`$=(x_1, \\dots, x_n) \\in \\mathbb R^n$. Représenter le graphe de $\\ell'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 5.**\n",
    "Soient $\\alpha>0$ et $x_1, \\dots, x_n \\in \\mathbb R^2$ les lignes de la matrice $X$ (vus comme vecteurs colonne).\n",
    "On souhaite fournir une solution numérique au problème d'optimisation\n",
    "$$\n",
    "\\operatorname{minimiser}_{(w, b) \\in \\mathbb R^2 \\times \\mathbb R} ~ R(w, b),\n",
    "\\quad \\text{avec} \\quad R(w, b) = \\frac \\alpha 2 \\|w\\|_2^2 + \\frac 1 n \\sum_{i=1}^n \\ell \\left(y_i(w^\\top x_i+b) \\right).\n",
    "$$\n",
    "On définit la valeur numérique de $\\alpha$ et le conteneur suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.\n",
    "data = X, y, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Définir une fonction `obj(w, b, data)` retournant $R(w, b)$ pour `w`$=w$ et `b`$=b$.\n",
    "Afficher la valeur de $R(w_0, b_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 6.**\n",
    ">Définir une fonction `grad(w, b, data)` retournant $\\left[ \\nabla_w R(w, b), \\frac{\\partial R}{\\partial b}(w, b) \\right]$ pour `w`$=w$ et `b`$=b$.\n",
    "Pour ce faire, on pourra remarquer (mais ce n'est pas nécessaire), qu'il est possible de multiplier les lignes d'une matrice par différentes valeurs de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [1. 1.]\n",
      " [2. 2.]\n",
      " [3. 3.]\n",
      " [4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(5)\n",
    "B = np.ones((5, 2))\n",
    "C = a[:, np.newaxis]*B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Afficher les valeurs de $\\nabla_w R(w_0, b_0)$ et $\\frac{\\partial R}{\\partial b}(w_0, b_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**En cas de problème.**\n",
    "Si vous n'avez pas réussi à définir la fonction `grad(w, b, data)`, vous pouvez utiliser celle-ci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msecure_grad\u001b[39m(w, b, data):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m approx_fprime(w, obj, \u001b[38;5;241m1e-5\u001b[39m, b, data), approx_fprime(b, \u001b[38;5;28;01mlambda\u001b[39;00m x: obj(w, x, data), \u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43msecure_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 4\u001b[0m, in \u001b[0;36msecure_grad\u001b[0;34m(w, b, data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msecure_grad\u001b[39m(w, b, data):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m approx_fprime(w, \u001b[43mobj\u001b[49m, \u001b[38;5;241m1e-5\u001b[39m, b, data), approx_fprime(b, \u001b[38;5;28;01mlambda\u001b[39;00m x: obj(w, x, data), \u001b[38;5;241m1e-5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "def secure_grad(w, b, data):\n",
    "    return approx_fprime(w, obj, 1e-5, b, data), approx_fprime(b, lambda x: obj(w, x, data), 1e-5)\n",
    "\n",
    "secure_grad(w0, b0, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 7.**\n",
    ">Définir deux *lambda* fonctions `obj_b(b)` et `grad_obj_b(b)` retournant respectivement $R(w_0, b)$ et $\\frac{\\partial R}{\\partial b}(w_0, b)$ pour `b`$=b$.\n",
    "Représenter leurs graphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 8.**\n",
    ">Vérifier les valeurs numériques de $\\nabla_w R(w_0, b_0)$ et $\\frac{\\partial R}{\\partial b}(w_0, b_0)$ à l'aide de la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function check_grad in module scipy.optimize.optimize:\n",
      "\n",
      "check_grad(func, grad, x0, *args, **kwargs)\n",
      "    Check the correctness of a gradient function by comparing it against a\n",
      "    (forward) finite-difference approximation of the gradient.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    func : callable ``func(x0, *args)``\n",
      "        Function whose derivative is to be checked.\n",
      "    grad : callable ``grad(x0, *args)``\n",
      "        Gradient of `func`.\n",
      "    x0 : ndarray\n",
      "        Points to check `grad` against forward difference approximation of grad\n",
      "        using `func`.\n",
      "    args : \\*args, optional\n",
      "        Extra arguments passed to `func` and `grad`.\n",
      "    epsilon : float, optional\n",
      "        Step size used for the finite difference approximation. It defaults to\n",
      "        ``sqrt(np.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    err : float\n",
      "        The square root of the sum of squares (i.e., the 2-norm) of the\n",
      "        difference between ``grad(x0, *args)`` and the finite difference\n",
      "        approximation of `grad` using func at the points `x0`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    approx_fprime\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> def func(x):\n",
      "    ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "    >>> def grad(x):\n",
      "    ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "    >>> from scipy.optimize import check_grad\n",
      "    >>> check_grad(func, grad, [1.5, -1.5])\n",
      "    2.9802322387695312e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "help(check_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 9.**\n",
    ">On donne ci-dessous les coefficients de Lipschitz continuité $L_w$ et $L_b$ de $\\nabla_w R$ et $\\frac{\\partial R}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.46147508005031, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_w = alpha + 2/n*np.linalg.norm(X.T@X, 2)\n",
    "L_b = 2\n",
    "\n",
    "L_w, L_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Définir une fonction `optim(L_w, L_b, data, n_it=200)`, implémentant l'algorithme de résolution suivant pour le problème d'optimisation considéré:\n",
    ">- Initialisation : $w=0$, $b=0$.\n",
    ">- Itération :\n",
    "    - $d_w \\gets \\nabla_w R(w, b)$ ;\n",
    "    - $d_b = \\frac{\\partial R}{\\partial b}(w, b)$ ;\n",
    "    - $w \\gets w - \\frac{d_w}{L_w}$ ;\n",
    "    - $b \\gets b - \\frac{d_b}{L_b}$ ;\n",
    ">\n",
    ">et retournant les valeurs $w$ et $b$.\n",
    "\n",
    ">Afficher les données `X` en distingant leur classe par des couleurs différentes et tracer les droites d'équation $w_0^\\top x + b_0 = 0$ et $w^\\top x + b = 0$, où $w$ et $b$ sont les valeurs obtenues en sorti de l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 10.**\n",
    ">Implémenter la méthode étudiée dans la classe suivante.\n",
    "Afficher la frontière de décision à l'aide de la fonction `plot_frontiere` de `mllab` ainsi que le score obtenu pour les données considérée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "class METHOD:\n",
    "    def __init__(self, alpha=1, n_iter=200):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # To do\n",
    "\n",
    "        # End to do\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        # To do\n",
    "\n",
    "        # End to do\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # To do\n",
    "\n",
    "        # End to do\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        # To do\n",
    "\n",
    "        # End to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 11.**\n",
    ">Comparer la frontière de décision à celles des classes suivantes (implémentées dans `sklearn`) :\n",
    ">- la régression logistique ;\n",
    ">- le risque construit sur la perte charnière et minimisé par [gradient stochastique](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question 12.**\n",
    "On souhaite maintenant résoudre le même problème dans le RKHS $\\mathcal H$ associé à un noyau $k : \\mathbb R^2 \\times \\mathbb R^2 \\to \\mathbb R$ :\n",
    "$$\n",
    "\\operatorname{minimiser}_{(h, b) \\in \\mathcal H \\times \\mathbb R} ~ R_{\\mathcal H}(h, b),\n",
    "\\quad \\text{avec} \\quad R_{\\mathcal H}(h, b) = \\frac \\alpha 2 \\|h\\|_{\\mathcal H}^2 + \\frac 1 n \\sum_{i=1}^n \\ell \\left(y_i(h(x_i)+b) \\right).\n",
    "$$\n",
    "Donner la restriction de ce problème issue du théorème du représentant, en nommant $K$ la matrice de Gram des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">En admettant que les coefficients de Lipschitz continuité sont :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168.56301079906015, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_beta = np.linalg.norm(alpha * K + 2/n*K.T@K, 2)\n",
    "L_b = 2\n",
    "\n",
    "L_beta, L_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">proposer une implémentation similaire à celle du travail précédent, incluant les noyaux linéaire et [gaussien](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html) (il pourra être avisé de suivre les mêmes grandes étapes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "class KERNELMETHOD:\n",
    "    def __init__(self, kernel='linear', alpha=1, gamma=1, n_iter=200):\n",
    "        self.kernel = kernel\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # To do\n",
    "\n",
    "        # End to do\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        # To do\n",
    "\n",
    "        # End to do\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # To do\n",
    "\n",
    "        # End to do\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        # To do\n",
    "\n",
    "        # End to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
