{"cells": [{"cell_type": "markdown", "id": "eeace604", "metadata": {"cell_marker": "\"\"\"", "id": "dYfGJCe52lP4"}, "source": ["# Outlook"]}, {"cell_type": "markdown", "id": "acab257c", "metadata": {"cell_marker": "\"\"\"", "id": "aZUSf0n_2otG"}, "source": ["In this notebook, we will implement the REINFORCE algorithm using BBRL. To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing):\n", "\n", "-  You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), \n", "\n", "- then [a first true RL example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) \n", "\n", "- and, most importantly, details about the [NoAutoResetGymAgent](https://colab.research.google.com/drive/1EX5O03mmWFp9wCL_Gb_-p08JktfiL2l5?usp=sharing)."]}, {"cell_type": "markdown", "id": "cf4d80a2", "metadata": {"cell_marker": "\"\"\"", "id": "MD3A9UmNS2OX"}, "source": ["The REINFORCE algorithm is explained in a series of 3 videos: [video 1](https://www.youtube.com/watch?v=R7ULMBXOQtE), [video 2](https://www.youtube.com/watch?v=dKUWto9B9WY) and [video 3](https://www.youtube.com/watch?v=GcJ9hl3T6x8). You can also read the corresponding slides: [slides1](http://pages.isir.upmc.fr/~sigaud/teach/ps/3_pg_derivation1.pdf), [slides2](http://pages.isir.upmc.fr/~sigaud/teach/ps/4_pg_derivation2.pdf), [slides3](http://pages.isir.upmc.fr/~sigaud/teach/ps/5_pg_derivation3.pdf)."]}, {"cell_type": "markdown", "id": "707c350e", "metadata": {"cell_marker": "\"\"\"", "id": "zJZDcDafp7Uf"}, "source": ["## Installation and Imports"]}, {"cell_type": "markdown", "id": "a34a7f8f", "metadata": {"cell_marker": "\"\"\"", "id": "aHO1nIdM21Lq"}, "source": ["### Installation"]}, {"cell_type": "markdown", "id": "508df893", "metadata": {"cell_marker": "\"\"\"", "id": "Ymc-lbXi9vDE"}, "source": ["The BBRL library is [here](https://github.com/osigaud/bbrl)."]}, {"cell_type": "markdown", "id": "571f0a9b", "metadata": {"cell_marker": "\"\"\"", "id": "pDy9yuQH73tJ"}, "source": ["This is OmegaConf that makes it possible that by just defining the `def run_a2c(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n", "\n", "More precisely, the code is run by calling\n", "\n", "`config=OmegaConf.create(params)`\n", "\n", "`run_a2c(config)`\n", "\n", "at the very bottom of the colab, after starting tensorboard."]}, {"cell_type": "code", "execution_count": null, "id": "0f509eab", "metadata": {"id": "j0MaggiOl4KU"}, "outputs": [], "source": ["try:\n", "    from easypip import easyimport\n", "except:\n", "    !pip install easypip\n", "    from easypip import easyimport\n", "\n", "import functools\n", "import time\n", "\n", "easyimport(\"importlib_metadata==4.13.0\")\n", "\n", "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n", "bbrl = easyimport(\"bbrl\")\n", "import gym\n"]}, {"cell_type": "markdown", "id": "07a70eb3", "metadata": {"cell_marker": "\"\"\"", "id": "m4kV9pWV3wRe"}, "source": ["### Imports"]}, {"cell_type": "markdown", "id": "9e8fe67e", "metadata": {"cell_marker": "\"\"\"", "id": "caqhJYbe5YcO"}, "source": ["Below, we import standard python packages, pytorch packages and gym environments."]}, {"cell_type": "markdown", "id": "1d941964", "metadata": {"cell_marker": "\"\"\"", "id": "4l7sTVXbJBE_"}, "source": ["[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]}, {"cell_type": "code", "execution_count": null, "id": "44d03760", "metadata": {"id": "vktQB-AO5biu"}, "outputs": [], "source": ["import copy\n", "import time\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "\n", "import gym"]}, {"cell_type": "markdown", "id": "5b54a675", "metadata": {"cell_marker": "\"\"\"", "id": "fE1c7ZLf60X_"}, "source": ["### BBRL imports"]}, {"cell_type": "code", "execution_count": null, "id": "9a125a57", "metadata": {"id": "RcuqoAvG3zMZ"}, "outputs": [], "source": ["from bbrl.agents.agent import Agent\n", "from bbrl import get_arguments, get_class, instantiate_class\n", "\n", "# The workspace is the main class in BBRL, this is where all data is collected and stored\n", "from bbrl.workspace import Workspace\n", "\n", "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n", "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n", "# or until a given condition is reached\n", "from bbrl.agents import Agents, RemoteAgent, TemporalAgent, PrintAgent\n", "\n", "# The NoAutoResetGymAgent is an agent executing a batch of gym environments\n", "# without auto-resetting. These agents produce multiple variables in the workspace: \n", "# \u2019env/env_obs\u2019, \u2019env/reward\u2019, \u2019env/timestep\u2019, \u2019env/done\u2019, \u2019env/initial_state\u2019, \u2019env/cumulated_reward\u2019, \n", "# At timestep t>0, these agents will read the \u2019action\u2019 variable in the workspace at time t \u2212 1\n", "from bbrl.agents.gymb import NoAutoResetGymAgent"]}, {"cell_type": "markdown", "id": "d876c978", "metadata": {"cell_marker": "\"\"\"", "id": "JVvAfhKm9S8p"}, "source": ["## Definition of agents"]}, {"cell_type": "markdown", "id": "dcd0b5aa", "metadata": {"cell_marker": "\"\"\"", "id": "RdqzKSLKDtqz"}, "source": ["The [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf) uses a stochastic policy and a baseline which is the value function. Thus we need an Actor agent, a Critic agent and an Environment agent. \n", "The actor agent is built on an intermediate ProbAgent, see [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about the  ProbaAgent, the ActorAgent and the environment agent."]}, {"cell_type": "markdown", "id": "8c9c9da8", "metadata": {"cell_marker": "\"\"\"", "id": "u4HT1xbzTTvC"}, "source": [" As in [a previous notebook about DQN](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing), the neural networks we build are multi-layer perceptrons."]}, {"cell_type": "code", "execution_count": null, "id": "cfc7651e", "metadata": {"id": "UcRxfHdLILKI"}, "outputs": [], "source": ["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n", "    layers = []\n", "    for j in range(len(sizes) - 1):\n", "        act = activation if j < len(sizes) - 2 else output_activation\n", "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n", "    return nn.Sequential(*layers)"]}, {"cell_type": "code", "execution_count": null, "id": "ed4d9006", "metadata": {"id": "Xe2thODO7E40"}, "outputs": [], "source": ["class ProbAgent(Agent):\n", "    \"\"\"Computes the distribution $p(a_t|s_t)$\"\"\"\n", "    \n", "    def __init__(self, state_dim, hidden_layers, n_action):\n", "        super().__init__(name=\"prob_agent\")\n", "        self.model = build_mlp(\n", "            [state_dim] + list(hidden_layers) + [n_action], activation=nn.ReLU()\n", "        )\n", "\n", "    def forward(self, t, **kwargs):\n", "        # Get $s_t$\n", "        observation = self.get((\"env/env_obs\", t))\n", "        # Compute the distribution over actions\n", "        scores = self.model(observation)\n", "        action_probs = torch.softmax(scores, dim=-1)\n", "        assert not torch.any(torch.isnan(action_probs)), \"NaN Here\"\n", "        \n", "        self.set((\"action_probs\", t), action_probs)\n", "        entropy = torch.distributions.Categorical(action_probs).entropy()\n", "        self.set((\"entropy\", t), entropy)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "3fdf16fe", "metadata": {"id": "ARPW1Mmo7NB-"}, "outputs": [], "source": ["class ActorAgent(Agent):\n", "    \"\"\"Choose an action (either acoording to p(a_t|s_t) when stochastic is true,\n", "       or with argmax if false.\n", "    \"\"\"\n", "    def __init__(self):\n", "        super().__init__()\n", "\n", "    def forward(self, t, stochastic, **kwargs):\n", "        probs = self.get((\"action_probs\", t))\n", "        if stochastic:\n", "            action = torch.distributions.Categorical(probs).sample()\n", "        else:\n", "            action = probs.argmax(1)\n", "\n", "        self.set((\"action\", t), action)"]}, {"cell_type": "code", "execution_count": null, "id": "df042d39", "metadata": {"id": "Fsb5QRzw7V0o"}, "outputs": [], "source": ["def make_env(env_name):\n", "    return gym.make(env_name)"]}, {"cell_type": "markdown", "id": "66174fef", "metadata": {"cell_marker": "\"\"\"", "id": "Din6iU-1DnyH"}, "source": ["### VAgent"]}, {"cell_type": "markdown", "id": "1b6b0fa7", "metadata": {"cell_marker": "\"\"\"", "id": "Nf0mXQvbEw7V"}, "source": ["The VAgent is a neural network which takes an observation as input and whose output is the value $V(s)$ of this observation."]}, {"cell_type": "markdown", "id": "06d82902", "metadata": {"cell_marker": "\"\"\"", "id": "HvmJl_IAIx_d"}, "source": ["The `squeeze(-1)` removes the last dimension of the tensor. TODO: explain why we need it"]}, {"cell_type": "code", "execution_count": null, "id": "7aec32ce", "metadata": {"id": "g8y-63nq7Pjo"}, "outputs": [], "source": ["class VAgent(Agent):\n", "    def __init__(self, state_dim, hidden_layers):\n", "        super().__init__()\n", "        self.is_q_function = False\n", "        self.model = build_mlp(\n", "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n", "        )\n", "\n", "    def forward(self, t, **kwargs):\n", "        observation = self.get((\"env/env_obs\", t))\n", "        critic = self.model(observation).squeeze(-1)\n", "        self.set((\"v_value\", t), critic)"]}, {"cell_type": "markdown", "id": "10cbae61", "metadata": {"cell_marker": "\"\"\"", "id": "EzxoIPtLVJ_i"}, "source": ["### Create the REINFORCE agent"]}, {"cell_type": "markdown", "id": "7d9c7aad", "metadata": {"cell_marker": "\"\"\"", "id": "tJ0qhRVgVarb"}, "source": ["The code below is rather straightforward. Note that we have not defined anything about data collection, using a RolloutBuffer or something to store the n_step return so far. This will come inside the training loop below."]}, {"cell_type": "markdown", "id": "3a697c03", "metadata": {"cell_marker": "\"\"\"", "id": "aaNnZw3bXEYd"}, "source": ["Interestingly, the loop between the policy and the environment is first defined as a collection of agents, and then embedded into a single TemporalAgent."]}, {"cell_type": "code", "execution_count": null, "id": "745047db", "metadata": {"id": "G8Uk_RQh8QrO"}, "outputs": [], "source": ["def create_reinforce_agent(cfg, env_agent):\n", "    obs_size, act_size = env_agent.get_obs_and_actions_sizes()\n", "    proba_agent = ProbAgent(obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size)\n", "    action_agent = ActorAgent()\n", "    # print_agent = PrintAgent()\n", "    tr_agent = Agents(env_agent, proba_agent, action_agent)  # , print_agent)\n", "\n", "    critic_agent = TemporalAgent(\n", "        VAgent(obs_size, cfg.algorithm.architecture.critic_hidden_size)\n", "    )\n", "\n", "    # Get an agent that is executed on a complete workspace\n", "    train_agent = TemporalAgent(tr_agent)\n", "    train_agent.seed(cfg.algorithm.seed)\n", "    return train_agent, proba_agent, critic_agent  # , print_agent"]}, {"cell_type": "markdown", "id": "1664d558", "metadata": {"cell_marker": "\"\"\"", "id": "lU3cO6znHyDc"}, "source": ["### The Logger class"]}, {"cell_type": "markdown", "id": "a4999770", "metadata": {"cell_marker": "\"\"\"", "id": "E4BrXwTLdK0Z"}, "source": ["The logger class below is not generic, it is specifically designed in the context of this A2C colab."]}, {"cell_type": "markdown", "id": "70c226b8", "metadata": {"cell_marker": "\"\"\"", "id": "VKYYp8IHLhd-"}, "source": ["The logger parameters are defined below in `params = { \"logger\":{ ...`"]}, {"cell_type": "markdown", "id": "d8a6a520", "metadata": {"cell_marker": "\"\"\"", "id": "rhwNN4oCNOhi"}, "source": ["In this colab, the logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation (see the parameters part below).\n", "Note that the salina Logger is also saving the log in a readable format such that you can use `Logger.read_directories(...)` to read multiple logs, create a dataframe, and analyze many experiments afterward in a notebook for instance. "]}, {"cell_type": "markdown", "id": "edd1ab03", "metadata": {"cell_marker": "\"\"\"", "id": "10TUc-PHMqNm"}, "source": ["The code for the different kinds of loggers is available in the [bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/bbrl/utils/logger.py) file."]}, {"cell_type": "markdown", "id": "4fea5f33", "metadata": {"cell_marker": "\"\"\"", "id": "c872tM4WM5FH"}, "source": ["Having logging provided under the hood is one of the features where using RL libraries like BBRL will allow you to save time."]}, {"cell_type": "markdown", "id": "02a55706", "metadata": {"cell_marker": "\"\"\"", "id": "lmsf5BENLz10"}, "source": ["`instantiate_class` is an inner BBRL mechanism. The `instantiate_class` function is available in the [`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file."]}, {"cell_type": "code", "execution_count": null, "id": "f55d9c33", "metadata": {"id": "aOkauz_0H2GA"}, "outputs": [], "source": ["class Logger():\n", "      def __init__(self, cfg):\n", "        self.logger = instantiate_class(cfg.logger)\n", "\n", "      def add_log(self, log_string, loss, epoch):\n", "        self.logger.add_scalar(log_string, loss.item(), epoch)\n", "\n", "      # Log losses\n", "      def log_losses(self, epoch, critic_loss, actor_loss):\n", "        self.add_log(\"critic_loss\", critic_loss, epoch)\n", "        self.add_log(\"actor_loss\", actor_loss, epoch)\n", "\n"]}, {"cell_type": "markdown", "id": "754653e2", "metadata": {"cell_marker": "\"\"\"", "id": "f2vq1OJHWCIE"}, "source": ["### Setup the optimizer"]}, {"cell_type": "markdown", "id": "9f47e0e3", "metadata": {"cell_marker": "\"\"\"", "id": "VzmEKF4J8qjg"}, "source": ["We use a single optimizer to tune the parameters of the actor (in the prob_agent part) and the critic (in the critic_agent part). It would be possible to have two optimizers which would work separately on the parameters of each component agent, but it would be more complicated because updating the actor requires the gradient of the critic."]}, {"cell_type": "code", "execution_count": null, "id": "5a6c8a4c", "metadata": {"id": "YFfzXEu2WFWj"}, "outputs": [], "source": ["# Configure the optimizer over the a2c agent\n", "def setup_optimizer(cfg, prob_agent, critic_agent):\n", "    optimizer_args = get_arguments(cfg.optimizer)\n", "    parameters = nn.Sequential(prob_agent, critic_agent).parameters()\n", "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n", "    return optimizer"]}, {"cell_type": "markdown", "id": "b1ec4195", "metadata": {"cell_marker": "\"\"\"", "id": "YQNvhO_VAJbh"}, "source": ["### Compute critic loss"]}, {"cell_type": "markdown", "id": "0e4a156c", "metadata": {"cell_marker": "\"\"\"", "id": "fxxobbxRaJXO"}, "source": ["Note the `critic[1:].detach()` in the computation of the temporal difference target. The idea is that we compute this target as a function of $V(s_{t+1})$, but we do not want to apply gradient descent on this $V(s_{t+1})$, we will only apply gradient descent to the $V(s_t)$ according to this target value."]}, {"cell_type": "markdown", "id": "61be2fd1", "metadata": {"cell_marker": "\"\"\"", "id": "Ngf5NHvWBbrE"}, "source": ["In practice, `x.detach()` detaches a computation graph from a tensor, so it avoids computing a gradient over this tensor."]}, {"cell_type": "markdown", "id": "74d49e68", "metadata": {"cell_marker": "\"\"\"", "id": "fXwrjbueoDw6"}, "source": ["Note also the trick to deal with terminal states. If the state is terminal, $V(s_{t+1})$ does not make sense. Thus we need to ignore this term. So we multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted into an int, it becomes a 1), we get the term. If `must_bootstrap` is False (=0), we are at a terminal state, so we ignore the term. This trick is used in many RL libraries, e.g. SB3."]}, {"cell_type": "code", "execution_count": null, "id": "6cbe4eeb", "metadata": {"id": "2sepUK-gAM3u"}, "outputs": [], "source": ["def compute_critic_loss(cfg, reward, must_bootstrap, critic):\n", "    # Compute temporal difference\n", "    target = reward[:-1] + cfg.algorithm.discount_factor * critic[1:].detach() * must_bootstrap[1:].int()\n", "    td = (target - critic[:-1]) * must_bootstrap[1:].int()\n", "\n", "    # Compute critic loss\n", "    td_error = td ** 2\n", "    critic_loss = td_error.mean()\n", "    return critic_loss, td"]}, {"cell_type": "markdown", "id": "40472b1d", "metadata": {"cell_marker": "\"\"\"", "id": "Jmi91gANWT4z"}, "source": ["## Main training loop"]}, {"cell_type": "markdown", "id": "2e271b0c", "metadata": {"cell_marker": "\"\"\"", "id": "ZzTIEHEDK14n"}, "source": ["### First algorithm: summing all the rewards along an episode"]}, {"cell_type": "markdown", "id": "6c1ddee9", "metadata": {"cell_marker": "\"\"\"", "id": "kSD9oqE0K_lv"}, "source": ["The most basic variant of the Policy Gradient algorithms just sums all the rewards along an episode."]}, {"cell_type": "markdown", "id": "3e79e77a", "metadata": {"cell_marker": "\"\"\"", "id": "PaKlA-S5LPE8"}, "source": ["This is implemented with the `apply_sum` function below."]}, {"cell_type": "code", "execution_count": null, "id": "84b70117", "metadata": {"id": "RqbkgMtELV2v"}, "outputs": [], "source": ["def apply_sum(cfg, reward, v_value):\n", "    reward_sum = reward.sum(axis=0)\n", "    for i in range(len(reward)):\n", "        reward[i] = reward_sum\n", "    return reward"]}, {"cell_type": "markdown", "id": "c193e94d", "metadata": {"cell_marker": "\"\"\"", "id": "e_y8kU4mK8d4"}, "source": ["### Main loop"]}, {"cell_type": "markdown", "id": "b5c986f3", "metadata": {"cell_marker": "\"\"\"", "id": "OFB1XFE5YEc6"}, "source": ["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. Several things need to be explained here.\n", "- `optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n", "- note that we sum all the losses, both for the critic and the actor, before applying back-propagation with `loss.backward()`. At first glance, summing these losses may look weird, as the actor and the critic receive different updates with different parts of the loss. This mechanism relies on the central property of tensor manipulation libraries like TensorFlow and pytorch. In pytorch, each loss tensor comes with its own graph of computation for back-propagating the gradient, in such a way that when you back-propagate the loss, the adequate part of the loss is applied to the adequate parameters.\n", "These mechanisms are partly explained [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n", "- since the optimizer has been set to work with both the actor and critic parameters, `optimizer.step()` will optimize both agents and pytorch ensure that each will receive its own part of the gradient."]}, {"cell_type": "code", "execution_count": null, "id": "a44dc52b", "metadata": {"id": "sk85_sRWW-5s"}, "outputs": [], "source": ["def run_reinforce(cfg, *, compute_reward=apply_sum, compute_critic_loss=compute_critic_loss):\n", "    \"\"\"Run Reinforce\n", "    \n", "    :param compute_reward: \n", "        The function called to compute the reward \n", "        for Reinforve at each time step (default to apply_sum)\n", "        \n", "    :param compute_critic_loss: \n", "        Function that specifies how to compute the critic loss\n", "    \"\"\"\n", "    logger = Logger(cfg)\n", "    best_reward = -10e10\n", "\n", "    # 2) Create the environment agent\n", "    env_agent = NoAutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.n_envs,\n", "        cfg.algorithm.seed,\n", "    )\n", "\n", "    reinforce_agent, proba_agent, critic_agent = create_reinforce_agent(cfg, env_agent)\n", "\n", "    # 7) Configure the optimizer over the a2c agent\n", "    optimizer = setup_optimizer(cfg, reinforce_agent, critic_agent)\n", "\n", "    # 8) Training loop\n", "    nb_steps = 0\n", "\n", "    for episode in range(cfg.algorithm.nb_episodes):\n", "        # print_agent.reset()\n", "        # Execute the agent on the workspace to sample complete episodes\n", "        # Since not all the variables of workspace will be overwritten, it is better to clear the workspace\n", "        # Configure the workspace to the right dimension.\n", "        train_workspace = Workspace()\n", "\n", "        reinforce_agent(train_workspace, stochastic=True, t=0, stop_variable=\"env/done\")\n", "\n", "        # Get relevant tensors (size are timestep x n_envs x ....)\n", "        obs, done, truncated, action_probs, reward, action = train_workspace[\n", "            \"env/env_obs\",\n", "            \"env/done\",\n", "            \"env/truncated\",\n", "            \"action_probs\",\n", "            \"env/reward\",\n", "            \"action\",\n", "        ]\n", "        critic_agent(train_workspace, stop_variable=\"env/done\")\n", "        v_value = train_workspace[\"v_value\"]\n", "\n", "        for i in range(cfg.algorithm.n_envs):\n", "            nb_steps += len(action[:, i])\n", "\n", "        # Determines whether values of the critic should be propagated\n", "        # True if the episode reached a time limit or if the task was not done\n", "        # See https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing\n", "        must_bootstrap = torch.logical_or(~done, truncated)\n", "\n", "        critic_loss, td = compute_critic_loss(cfg, reward, must_bootstrap, v_value)\n", "\n", "        reward = compute_reward(cfg, reward, v_value)\n", "\n", "        # Take the log probability of the actions performed\n", "        action = action.unsqueeze(-1)\n", "        action_logp = torch.gather(action_probs.squeeze(), dim=2, index=action).squeeze().log()\n", "        \n", "        # Compute the policy gradient loss based on the log probability of the actions performed\n", "        actor_loss = action_logp * reward.detach() * must_bootstrap.int()\n", "        actor_loss = actor_loss.mean()\n", "\n", "        # Log losses\n", "        logger.log_losses(nb_steps, critic_loss, actor_loss)\n", "\n", "        loss = (\n", "            cfg.algorithm.critic_coef * critic_loss\n", "            - cfg.algorithm.actor_coef * actor_loss\n", "        )\n", "\n", "\n", "        # Compute the cumulated reward on final_state\n", "        cumulated_reward = train_workspace[\"env/cumulated_reward\"][-1]\n", "        mean = cumulated_reward.mean()\n", "        print(f\"episode: {episode}, reward: {mean}\")\n", "        logger.add_log(\"reward\", mean, nb_steps)\n", "        \n", "\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n", "\n"]}, {"cell_type": "markdown", "id": "c30d6a54", "metadata": {"cell_marker": "\"\"\"", "id": "uo6bc3zzKua_"}, "source": ["## Definition of the parameters"]}, {"cell_type": "markdown", "id": "602cc81a", "metadata": {"cell_marker": "\"\"\"", "id": "36r4PAfvKx-f"}, "source": ["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]}, {"cell_type": "code", "execution_count": null, "id": "5eef4203", "metadata": {"id": "JB2B8zELNWQd"}, "outputs": [], "source": ["params={\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": \"./tblogs/reinforce-\" + str(time.time()),\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 1,\n", "    \"n_envs\": 8,\n", "    \"nb_episodes\": 1000,\n", "    \"discount_factor\": 0.95,\n", "    \"critic_coef\": 1.0,\n", "    \"actor_coef\": 1.0,\n", "    \"architecture\":{\n", "        \"actor_hidden_size\": [32],\n", "        \"critic_hidden_size\": [36],\n", "    },\n", "  },\n", "\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_env\",\n", "    \"env_name\": \"CartPole-v1\",\n", "  },\n", "  \"optimizer\":\n", "  {\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 0.001,\n", "  }\n", "}\n", "\n"]}, {"cell_type": "markdown", "id": "782806ad", "metadata": {"cell_marker": "\"\"\"", "id": "jp7jDeGkaoM1"}, "source": ["### Launching tensorboard to visualize the results"]}, {"cell_type": "markdown", "id": "1fce245b", "metadata": {"cell_marker": "r\"\"\""}, "source": ["# For Colab - otherwise, it is easier and better to launch tensorboard from\n", "# the terminal\n", "if get_ipython().__class__.__module__ == \"google.colab._shell\":\n", "    # %load_ext tensorboard\n", "    # %tensorboard --logdir ./tmp\n", "else:\n", "    import sys\n", "    import os\n", "    import os.path as osp\n", "    print(f'''Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir=\"{os.getcwd()}/tblogs\"''')\n", "```\n", "\n", "```{python id=\"l42OUoGROlSt\"}\n", "config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)\n", "run_reinforce(config)\n", "```"]}, {"cell_type": "markdown", "id": "d140473b", "metadata": {"cell_marker": "\"\"\"", "id": "149EUMQXPDWC", "tags": []}, "source": ["## Exercises"]}, {"cell_type": "markdown", "id": "30eaf215", "metadata": {"cell_marker": "\"\"\"", "id": "bXmV7T6TPKpk"}, "source": ["### First algorithm: summing discounted rewards"]}, {"cell_type": "markdown", "id": "667d81d2", "metadata": {"cell_marker": "\"\"\"", "id": "fuAIzH-YPRgj"}, "source": ["As explained in the [second video](https://www.youtube.com/watch?v=dKUWto9B9WY) and [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/4_pg_derivation2.pdf), using a discounted reward after the current step and ignoring the rewards before the current step results in lower variance."]}, {"cell_type": "markdown", "id": "02add24e", "metadata": {"cell_marker": "\"\"\"", "id": "2Ysccxl1Pzmx"}, "source": ["By taking inspiration from the `apply_sum()` function above, code a function `apply_discounted_sum()` that computes sum of discounted rewards from immediate rewards."]}, {"cell_type": "markdown", "id": "e8aa7179", "metadata": {"cell_marker": "\"\"\"", "id": "3RTorkmEQJoR"}, "source": ["Two hints:\n", "- you should proceed backwards, starting from the final step of the episode and storing the previous sum into a register\n", "- you need the discount factor as an input to your function."]}, {"cell_type": "code", "execution_count": null, "id": "488d3339", "metadata": {"id": "nEOcInDEQb2R"}, "outputs": [], "source": ["def apply_discounted_sum(cfg, reward, v_value):\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n"]}, {"cell_type": "markdown", "id": "7ac50c53", "metadata": {"cell_marker": "\"\"\"", "id": "96Ag4U_5S7mW"}, "source": ["Then compare the performance of this algorithm to that of the previous approach where the rewards were just summed up."]}, {"cell_type": "code", "execution_count": null, "id": "132c3a6d", "metadata": {"tags": []}, "outputs": [], "source": ["torch.manual_seed(config.algorithm.seed)\n", "\n", "config=OmegaConf.create(params)\n", "config.logger.log_dir = \"./tblogs/reinforce_dreward-\" + str(time.time())\n", "run_reinforce(config, compute_reward=apply_discounted_sum)"]}, {"cell_type": "markdown", "id": "65baa14f", "metadata": {"cell_marker": "\"\"\"", "id": "X_Lv2hYXQpLK"}, "source": ["### Second algorithm: Focus on learning the baseline value"]}, {"cell_type": "markdown", "id": "bc09bde2", "metadata": {"cell_marker": "\"\"\"", "id": "HUpbw6mTSc_l"}, "source": ["The `compute_critic_loss()` function above uses the Temporal Difference approach to critic estimation. In this part, we will compare it to using the Monte Carlo estimation approach."]}, {"cell_type": "markdown", "id": "0164b903", "metadata": {"cell_marker": "\"\"\"", "id": "5_HPc9LIO6t-"}, "source": ["As explained in [this video](https://www.youtube.com/watch?v=GcJ9hl3T6x8) and [these slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/5_pg_derivation3.pdf), the MC estimation approach uses the following equation:"]}, {"cell_type": "markdown", "id": "ca6ef2a1", "metadata": {"cell_marker": "r\"\"\"", "id": "pzuLUv4IPFKn"}, "source": ["$$\\phi_{j+1} = \\mathop{\\mathrm{argmin}}_{\\phi_j} \n", "    \\frac{1}{m\\times H}\\sum_{i=1}^m \n", "    \\sum_{t=1}^H \n", "        \\left(\n", "            \\left(\\sum_{k=t}^H \\gamma^{k-t} r(s_k^{(i)},a_k^{(i)}) \\right) - \\hat{V}^\\pi_{\\phi_j}(s_t^{(i)})\n", "        \\right)^2\n", "$$"]}, {"cell_type": "markdown", "id": "69f50141", "metadata": {"cell_marker": "r\"\"\"", "id": "1HQYarhNQTXy"}, "source": ["The innermost sum of discounted rewards exactly corresponds to the computation of the `apply_discounted_sum()` function. The rest just consists in computing the squared difference (also known as the Means Squared Error, or MSE) over the $m \\times H$ samples ($m$ episodes of lenght $H$) that we have collected."]}, {"cell_type": "markdown", "id": "9fbb0cc9", "metadata": {"cell_marker": "\"\"\"", "id": "Qg3VFKU8RYsv"}, "source": ["From the above information, create a `compute_critic_loss_mc()` function which must be called after `apply_discounted_sum()` on the reward."]}, {"cell_type": "code", "execution_count": null, "id": "f8ff9496", "metadata": {"id": "jRBlsiauRrw6", "lines_to_next_cell": 2}, "outputs": [], "source": ["# \u00c0 compl\u00e9ter...  \n", "assert False, 'Code non impl\u00e9ment\u00e9'\n", "def compute_critic_loss_mc():\n", "pass\n"]}, {"cell_type": "markdown", "id": "b92287df", "metadata": {"cell_marker": "\"\"\"", "id": "OQTHQohGR6N6"}, "source": ["Then compare the learning dynamics and the learned critic using the Temporal Difference estimation approach and the Monte Carlo estimation approach."]}, {"cell_type": "code", "execution_count": null, "id": "8417a8f8", "metadata": {"id": "xJXHpo51SKvK", "jupyter": {"outputs_hidden": true}, "tags": []}, "outputs": [], "source": ["torch.manual_seed(config.algorithm.seed)\n", "config=OmegaConf.create(params)\n", "config.gamma = 0.99\n", "config.logger.log_dir = \"./tblogs/reinforce_dreward_mc_critic-\" + str(time.time())\n", "\n", "run_reinforce(config, compute_reward=apply_discounted_sum, compute_critic_loss=compute_critic_loss_mc)"]}, {"cell_type": "markdown", "id": "e14a8dfa", "metadata": {"cell_marker": "\"\"\"", "id": "MlmcQP7MQvsH"}, "source": ["### Third algorithm: discounted sum minus baseline"]}, {"cell_type": "markdown", "id": "742d27e8", "metadata": {"cell_marker": "\"\"\"", "id": "N4h4tVMFSV6-"}, "source": ["From [this video](https://www.youtube.com/watch?v=GcJ9hl3T6x8) and [these slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/5_pg_derivation3.pdf), we know that we can substract a baseline to the gradient calculation methods studied above, and that the optimal baseline to reduce the variance is the value function."]}, {"cell_type": "markdown", "id": "b90ccda0", "metadata": {"cell_marker": "\"\"\"", "id": "Np6TveXCSpuE"}, "source": ["Code a `apply_discounted_sum_minus_baseline()` function, using the critic learned simultaneously with the policy."]}, {"cell_type": "code", "execution_count": null, "id": "a01e4aea", "metadata": {"id": "lUxGqCP9S2ZZ", "tags": []}, "outputs": [], "source": ["def apply_discounted_sum_minus_baseline(cfg, reward, v_value):\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "\n", "\n", "torch.manual_seed(config.algorithm.seed)\n", "config=OmegaConf.create(params)\n", "config.logger.log_dir = \"./tblogs/reinforce_reinforce_critic-\" + str(time.time())\n", "\n", "run_reinforce(config, compute_reward=apply_discounted_sum_minus_baseline, compute_critic_loss=compute_critic_loss_mc)"]}, {"cell_type": "markdown", "id": "54f9e713", "metadata": {"cell_marker": "\"\"\"", "id": "PcOKmImsTUbW"}, "source": ["Most probably, this will not work well, as initially the learned critic is a poor estimate of the true $V(s)$. Instead, load an already trained critic that you have saved after convergence from a previous run, and see if it works better."]}, {"cell_type": "markdown", "id": "e29e14b1", "metadata": {"cell_marker": "\"\"\"", "id": "gHBdyHyqU8dj"}, "source": ["Loading and saving a network or a BBRL agent can easily be performed using `agent.save(filename)` and `agent.load(filename)`."]}, {"cell_type": "markdown", "id": "5319de58", "metadata": {"cell_marker": "\"\"\"", "id": "RqcDPPwyVtYa"}, "source": ["# Warning"]}, {"cell_type": "markdown", "id": "6ed09cff", "metadata": {"cell_marker": "\"\"\"", "id": "Zg3fcmqYVaSH"}, "source": ["Be cautious with the use of ProbAgent with just a hidden layer, ProbAgent with build_mlp, and DiscreteActor. Try to be progressive..."]}], "metadata": {"jupytext": {"cell_markers": "\"\"\""}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}
