{"cells": [{"cell_type": "markdown", "id": "6e6e5096", "metadata": {"id": "dYfGJCe52lP4"}, "source": ["\n", "# Outlook\n", "\n", "In this notebook we code the Soft Actor-Critic (SAC) algorithm using BBRL. This algorithm is described in [this paper](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf) and [this paper](https://arxiv.org/pdf/1812.05905.pdf).\n", "To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, [details about the AutoResetGymAgent](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing).\n", "\n", "The algorithm is explained in [this video](https://www.youtube.com/watch?v=U20F-MvThjM) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/12_sac.pdf)."]}, {"cell_type": "markdown", "id": "12fa6ff3", "metadata": {"cell_marker": "\"\"\"", "id": "zJZDcDafp7Uf"}, "source": ["## Installation and Imports\n", "\n", "### Installation"]}, {"cell_type": "code", "execution_count": null, "id": "455fd96e", "metadata": {"id": "tPfvqqHyXSvj", "lines_to_next_cell": 2}, "outputs": [], "source": ["\n", "!pip install easypip"]}, {"cell_type": "code", "execution_count": null, "id": "5341967f", "metadata": {"id": "j0MaggiOl4KU"}, "outputs": [], "source": ["from easypip import easyimport\n", "import time\n", "\n", "easyimport(\"importlib_metadata==4.13.0\")\n", "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n", "bbrl_gym = easyimport(\"bbrl_gym\")\n", "bbrl = easyimport(\"bbrl>=0.1.6\")"]}, {"cell_type": "markdown", "id": "1c9494d4", "metadata": {"cell_marker": "\"\"\"", "id": "m4kV9pWV3wRe"}, "source": ["### Imports\n", "\n", "Below, we import standard python packages, pytorch packages and gym environments.\n", "\n", "This is OmegaConf that makes it possible that by just defining the `def run_a2c(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n", "\n", "More precisely, the code is run by calling\n", "\n", "```py\n", "config=OmegaConf.create(params)\n", "run_a2c(config)\n", "```\n", "\n", "at the very bottom of the notebook, after starting tensorboard.\n", "\n", "[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]}, {"cell_type": "code", "execution_count": null, "id": "43ed85ed", "metadata": {"id": "vktQB-AO5biu"}, "outputs": [], "source": ["import os\n", "import copy\n", "import time\n", "import numpy as np\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "\n", "import gym"]}, {"cell_type": "markdown", "id": "275c7900", "metadata": {"cell_marker": "\"\"\"", "id": "uWBCaTTKZKCs"}, "source": ["### BBRL imports"]}, {"cell_type": "code", "execution_count": null, "id": "6daf1b0f", "metadata": {"id": "7oERG6YRZSvx"}, "outputs": [], "source": ["from bbrl.agents.agent import Agent\n", "from bbrl import get_arguments, get_class, instantiate_class\n", "\n", "# The workspace is the main class in BBRL, this is where all data is collected and stored\n", "from bbrl.workspace import Workspace\n", "\n", "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n", "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n", "# or until a given condition is reached\n", "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n", "\n", "# AutoResetGymAgent is an agent able to execute a batch of gym environments\n", "# with auto-resetting. These agents produce multiple variables in the workspace: \n", "# \u2019env/env_obs\u2019, \u2019env/reward\u2019, \u2019env/timestep\u2019, \u2019env/done\u2019, \u2019env/initial_state\u2019, \u2019env/cumulated_reward\u2019, \n", "# ... When called at timestep t=0, then the environments are automatically reset. \n", "# At timestep t>0, these agents will read the \u2019action\u2019 variable in the workspace at time t \u2212 1\n", "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n", "\n", "from bbrl.utils.replay_buffer import ReplayBuffer"]}, {"cell_type": "markdown", "id": "2877d58b", "metadata": {"cell_marker": "\"\"\"", "id": "JVvAfhKm9S8p"}, "source": ["## Definition of agents\n", "\n", "### Functions to build networks\n", "\n", "We use the same utility functions to build neural networks as usual."]}, {"cell_type": "code", "execution_count": null, "id": "d7a1e17c", "metadata": {"id": "HFLn1t5rmIDb"}, "outputs": [], "source": ["def build_backbone(sizes, activation):\n", "    layers = []\n", "    for j in range(len(sizes) - 1):\n", "        layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n", "    return layers\n", "\n", "\n", "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n", "    layers = []\n", "    for j in range(len(sizes) - 1):\n", "        act = activation if j < len(sizes) - 2 else output_activation\n", "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n", "    return nn.Sequential(*layers)\n", "\n"]}, {"cell_type": "markdown", "id": "5692dfd9", "metadata": {"cell_marker": "\"\"\"", "id": "ouwQ5WhxTKNV"}, "source": ["### The SquashedGaussianActor\n", "\n", "SAC works better with a Squashed Gaussian policy, which enables the reparametrization trick. Note that our attempts to use a `TunableVarianceContinuousActor` as we did for instance in the [notebook about PPO](https://colab.research.google.com/drive/1KTxeRA3e0Npxa8Fa9y1OMcJCeQa41o_N?usp=sharing) completely failed. Such failure is also documented in the [OpenAI spinning up documentation page about SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html).\n", "\n", "The code of the `SquashedGaussianActor` policy is below.\n", "\n", "It relies on a specific type of distribution, the `SquashedDiagGaussianDistribution` which is taken from [the Stable Baselines3 library](https://github.com/DLR-RM/stable-baselines3).\n", "\n", "The fact that we use the reparametrization trick is hidden inside the code of this distribution. In more details, the key is that the [`sample(self)` method](https://github.com/osigaud/bbrl/blob/5c2b42c2ee30077166f86cc1dd562a3dce6203db/bbrl/utils/distributions.py#L200) calls `rsample()`."]}, {"cell_type": "markdown", "id": "1514771d", "metadata": {"cell_marker": "\"\"\"", "id": "WCtEKSP19WEz"}, "source": ["If you want to try using SAC with a squashed Gaussian policy but without using the reparametrization trick, you have to rewrite your own class to deal with a squashed Gaussian distribution."]}, {"cell_type": "code", "execution_count": null, "id": "3e834375", "metadata": {"id": "5LO_VNaOTeJu", "lines_to_next_cell": 2}, "outputs": [], "source": ["from bbrl.utils.distributions import SquashedDiagGaussianDistribution\n", "\n", "class SquashedGaussianActor(Agent):\n", "    def __init__(self, state_dim, hidden_layers, action_dim):\n", "        super().__init__()\n", "        backbone_dim = [state_dim] + list(hidden_layers)\n", "        self.layers = build_backbone(backbone_dim, activation=nn.ReLU())\n", "        self.backbone = nn.Sequential(*self.layers)\n", "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n", "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n", "        self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n", "        # std must be positive\n", "        self.std_layer = nn.Softplus()\n", "\n", "    def dist(self, obs: torch.Tensor):\n", "        \"\"\"Computes action distributions given observation(s)\"\"\"\n", "        backbone_output = self.backbone(obs)\n", "        mean = self.last_mean_layer(backbone_output)\n", "        std_out = self.last_std_layer(backbone_output)\n", "        std = self.std_layer(std_out)\n", "        return self.action_dist.make_distribution(mean, std)\n", "\n", "\n", "    def forward(self, t, stochastic):\n", "        action_dist = self.dist(self.get((\"env/env_obs\", t)))\n", "        action = action_dist.sample() if stochastic else action_dist.mode()\n", "\n", "        log_prob = action_dist.log_prob(action)\n", "        self.set((f\"action\", t), action)\n", "        self.set((\"action_logprobs\", t), log_prob)\n", "\n", "    def predict_action(self, obs, stochastic: bool):\n", "        action_dist = self.dist(obs)\n", "        action = action_dist.sample() if stochastic else action_dist.mode()\n", "        return action"]}, {"cell_type": "markdown", "id": "241df426", "metadata": {"id": "ajqSi5Nbmnxn", "lines_to_next_cell": 2}, "source": ["\n", "### Choosing a specific gym environment\n", "First, we need to make our gym environment. As usual, this is implemented with the simple function below."]}, {"cell_type": "code", "execution_count": null, "id": "912d350f", "metadata": {"id": "Fsb5QRzw7V0o", "lines_to_next_cell": 2}, "outputs": [], "source": ["def make_gym_env(env_name):\n", "    return gym.make(env_name)"]}, {"cell_type": "markdown", "id": "a6f44548", "metadata": {"id": "Din6iU-1DnyH", "lines_to_next_cell": 2}, "source": ["### CriticAgent\n", "\n", "As critics and target critics, SAC uses several instances of ContinuousQAgent class, as DDPG and TD3. See the [DDPG notebook](https://colab.research.google.com/drive/1APBtDiaFwQHKE2rfTZioGfDM8C41e7Il?usp=sharing) for details."]}, {"cell_type": "code", "execution_count": null, "id": "a5042349", "metadata": {"id": "g8y-63nq7Pjo"}, "outputs": [], "source": ["class ContinuousQAgent(Agent):\n", "    def __init__(self, state_dim, hidden_layers, action_dim):\n", "        super().__init__()\n", "        self.is_q_function = True\n", "        self.model = build_mlp(\n", "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n", "        )\n", "\n", "    def forward(self, t, detach_actions=False):\n", "        obs = self.get((\"env/env_obs\", t))\n", "        action = self.get((\"action\", t))\n", "        if detach_actions:\n", "            action = action.detach()\n", "        osb_act = torch.cat((obs, action), dim=1)\n", "        q_value = self.model(osb_act)\n", "        self.set((\"q_value\", t), q_value)\n", "\n", "    def predict_value(self, obs, action):\n", "        osb_act = torch.cat((obs, action), dim=0)\n", "        q_value = self.model(osb_act)\n", "        return q_value\n", "\n"]}, {"cell_type": "markdown", "id": "ffd7d04d", "metadata": {"id": "L9BPA7Kht6DL", "lines_to_next_cell": 2}, "source": ["### Building the complete training and evaluation agents\n", " \n", "In the code below we create the Squashed Gaussian actor, two critics and the corresponding target critics. Beforehand, we checked that the environment takes continuous actions (otherwise we would need a different code)."]}, {"cell_type": "code", "execution_count": null, "id": "aa5b9c02", "metadata": {"id": "UpiApKBfuBCS"}, "outputs": [], "source": ["# Create the SAC Agent\n", "def create_sac_agent(cfg, train_env_agent, eval_env_agent):\n", "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n", "    assert (\n", "        train_env_agent.is_continuous_action()\n", "    ), \"SAC code dedicated to continuous actions\"\n", "\n", "    # Actor\n", "    actor = SquashedGaussianActor(\n", "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n", "    )\n", "\n", "    # Train/Test agents\n", "    tr_agent = Agents(train_env_agent, actor)\n", "    ev_agent = Agents(eval_env_agent, actor)\n", "\n", "    # Builds the critics\n", "    critic_1 = ContinuousQAgent(\n", "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n", "    )\n", "    target_critic_1 = copy.deepcopy(critic_1)\n", "    critic_2 = ContinuousQAgent(\n", "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n", "    )\n", "    target_critic_2 = copy.deepcopy(critic_2)\n", "\n", "    train_agent = TemporalAgent(tr_agent)\n", "    eval_agent = TemporalAgent(ev_agent)\n", "    train_agent.seed(cfg.algorithm.seed)\n", "    return (\n", "        train_agent,\n", "        eval_agent,\n", "        actor,\n", "        critic_1,\n", "        target_critic_1,\n", "        critic_2,\n", "        target_critic_2,\n", "    )\n", "\n"]}, {"cell_type": "markdown", "id": "92356410", "metadata": {"cell_marker": "\"\"\"", "id": "lU3cO6znHyDc"}, "source": ["### The Logger class"]}, {"cell_type": "markdown", "id": "c663b29e", "metadata": {"id": "E4BrXwTLdK0Z", "lines_to_next_cell": 2}, "source": ["The logger class is the same as before, see [this notebook](https://colab.research.google.com/drive/1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x#scrollTo=lU3cO6znHyDc) for explanations."]}, {"cell_type": "code", "execution_count": null, "id": "0e78fbf1", "metadata": {"id": "aOkauz_0H2GA"}, "outputs": [], "source": ["class Logger():\n", "\n", "  def __init__(self, cfg):\n", "    self.logger = instantiate_class(cfg.logger)\n", "\n", "  def add_log(self, log_string, loss, epoch):\n", "    self.logger.add_scalar(log_string, loss.item(), epoch)\n", "\n", "  # Log losses\n", "  def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n", "    self.add_log(\"critic_loss\", critic_loss, epoch)\n", "    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n", "    self.add_log(\"actor_loss\", actor_loss, epoch)\n", "\n"]}, {"cell_type": "markdown", "id": "de855fc7", "metadata": {"id": "f2vq1OJHWCIE"}, "source": ["### Setup the optimizers\n", "\n", "A specificity of SAC is that it can optimize the entropy coefficient named\n", "$\\alpha$. How to tune $\\alpha$ is explained in [this\n", "paper](https://arxiv.org/pdf/1812.05905.pdf).\n", "\n", "Thus we have two functions to set up optimizers, one which deals with the\n", "actor and the critic as usual, and one which deals with the entropy\n", "coefficient. We use a single optimizer to tune the parameters of the actor and\n", "the critic. It would be possible to have two optimizers which would work\n", "separately on the parameters of each component agent, but it would be more\n", "complicated because updating the actor requires the gradient of the critic.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "113e8423", "metadata": {"id": "YFfzXEu2WFWj", "lines_to_next_cell": 2}, "outputs": [], "source": ["# Configure the optimizer for the actor and critic\n", "def setup_optimizers(cfg, actor, critic_1, critic_2):\n", "    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n", "    parameters = actor.parameters()\n", "    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n", "    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n", "    parameters = nn.Sequential(critic_1, critic_2).parameters()\n", "    critic_optimizer = get_class(cfg.critic_optimizer)(\n", "        parameters, **critic_optimizer_args\n", "    )\n", "    return actor_optimizer, critic_optimizer"]}, {"cell_type": "markdown", "id": "04ac624a", "metadata": {"id": "uM04csjWBTSB", "lines_to_next_cell": 2}, "source": ["\n", "For the entropy coefficient optimizer, the code is as follows. Note the trick\n", "which consists in using the log of this entropy coefficient. This trick was\n", "taken from the Stable baselines3 implementation of SAC, which is explained in\n", "[this notebook](https://colab.research.google.com/drive/12LER1_ShWOa_UhOL1nlX-LX_t5KQK9LV?usp=sharing).\n", "\n", "Tuning $\\alpha$ in SAC is an option. To chose to tune it, the `target_entropy`\n", "argument in the parameters should be `auto`. The initial value is given\n", "through the `entropy_coef` parameter. For any other value than `auto`, the\n", "value of $\\alpha$ will stay constant and correspond to the `entropy_coef`\n", "parameter."]}, {"cell_type": "code", "execution_count": null, "id": "45f6b4fe", "metadata": {"id": "Fr6hdgOQ1ODv"}, "outputs": [], "source": ["def setup_entropy_optimizers(cfg):\n", "    if cfg.algorithm.target_entropy == \"auto\":\n", "        entropy_coef_optimizer_args = get_arguments(cfg.entropy_coef_optimizer)\n", "        # Note: we optimize the log of the entropy coefficient which is slightly different from the paper\n", "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n", "        # Comment and code taken from the SB3 version of SAC\n", "        log_entropy_coef = torch.log(\n", "            torch.ones(1) * cfg.algorithm.entropy_coef\n", "        ).requires_grad_(True)\n", "        entropy_coef_optimizer = get_class(cfg.entropy_coef_optimizer)(\n", "            [log_entropy_coef], **entropy_coef_optimizer_args\n", "        )\n", "    else:\n", "        log_entropy_coef = 0\n", "        entropy_coef_optimizer = None\n", "    return entropy_coef_optimizer, log_entropy_coef\n", "\n"]}, {"cell_type": "markdown", "id": "358165bc", "metadata": {"id": "YQNvhO_VAJbh", "lines_to_next_cell": 2}, "source": ["\n", "### Compute the critic loss\n", "\n", "With the notations of my slides, the equation corresponding to Eq. (5) and (6)\n", "in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n", "\n", "$$ loss_Q({\\boldsymbol{\\theta}}) = {\\rm I\\!E}_{(\\mathbf{s}_t, \\mathbf{a}_t,\n", "\\mathbf{s}_{t+1}) \\sim \\mathcal{D}}\\left[\\left( r(\\mathbf{s}_t, \\mathbf{a}_t)\n", "+ \\gamma {\\rm I\\!E}_{\\mathbf{a} \\sim\n", "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_{t+1})}\\left[\\hat{Q}^{\\pi_{\\boldsymbol{\\theta}}}_{\\boldsymbol{\\phi}}(\\mathbf{s}_{t+1},\n", "\\mathbf{a}) - \\alpha\n", "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}|\\mathbf{s}_{t+1})} \\right] -\n", "\\hat{Q}^{\\pi_{\\boldsymbol{\\theta}}}_{\\boldsymbol{\\phi}}(\\mathbf{s}_t,\n", "\\mathbf{a}_t) \\right)^2 \\right] $$\n", "\n", "An important information in the above equation and the one about the actor\n", "loss below is the index of the expectations. These indexes tell us where the\n", "data should be taken from. In the above equation, one can see that the index\n", "of the outer expectation is over samples taken from the replay buffer, whereas\n", "in the inner expectation we consider actions from the current policy at the\n", "next state.\n", "\n", "Thus, to compute the inner expectation, one needs to determine what actions\n", "the current policy would take in the next state of each sample. This is what\n", "the line \n", "\n", "`t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)`\n", "\n", "does. The parameter `t=1` (instead of 0) ensures that we consider the next state.\n", "\n", "Once we have determined these actions, we can determine their Q-values and\n", "their log probabilities, to compute the inner expectation.\n", "\n", "Note that at this stage, we only determine the log probabilities corresponding\n", "to actions taken at the next time step, by contrast with what we do for the\n", "actor in the `compute_actor_loss(...)` function later on.\n", "\n", "Finally, once we have computed the $$\n", "\\hat{Q}^{\\pi_{\\boldsymbol{\\theta}}}_{\\boldsymbol{\\phi}}(\\mathbf{s}_{t+1},\n", "\\mathbf{a}) $$ for both critics, we take the min and store it into\n", "`post_q_values`. By contrast, the Q-values corresponding to the last term of\n", "the equation are taken from the replay buffer, they are computed in the\n", "beginning of the function by applying the Q agents to the replay buffer\n", "*before* changing the action to that of the current policy.\n", "\n", "An important remark is that, if the entropy coefficient $\\alpha$ corresponding\n", "to the `ent_coef` variable is set to 0, then we retrieve exactly the critic\n", "loss computation function of the TD3 algorithm. As we will see later, this is\n", "also true of the actor loss computation.\n", "\n", "This remark proved very useful in debugging the SAC code. We have set\n", "`ent_coef` to 0 and ensured the behavior was strictly the same as the behavior\n", "of TD3."]}, {"cell_type": "code", "execution_count": null, "id": "3372c38c", "metadata": {"id": "N9KRFG-PBRtD", "lines_to_next_cell": 2}, "outputs": [], "source": ["def compute_critic_loss(\n", "    cfg, reward, must_bootstrap,\n", "    t_actor, \n", "    q_agent_1, q_agent_2, \n", "    target_q_agent_1, target_q_agent_2, \n", "    rb_workspace,\n", "    ent_coef\n", "):\n", "    \"\"\"Computes the critic loss for a set of $S$ transition samples\n", "\n", "    Args:\n", "        cfg: The experimental configuration\n", "        reward: _description_\n", "        must_bootstrap: Tensor of indicators (2 x S)\n", "        t_actor: The actor agent (as a TemporalAgent)\n", "        q_agent_1: The first critic (as a TemporalAgent)\n", "        q_agent_2: The second critic (as a TemporalAgent)\n", "        target_q_agent_1: The target of the first critic\n", "        target_q_agent_2: The target of the second critic\n", "        rb_workspace: The transition workspace\n", "        ent_coef: The entropy coefficient\n", "\n", "    Returns:\n", "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n", "    \"\"\"\n", "    # Compute q_values from both critics with the actions present in the buffer:\n", "    # at t, we have Q(s,a) from the (s,a) in the RB\n", "    q_agent_1(rb_workspace, t=0, n_steps=1)\n", "    q_values_rb_1 = rb_workspace[\"q_value\"]\n", "    \n", "    q_agent_2(rb_workspace, t=0, n_steps=1)\n", "    q_values_rb_2 = rb_workspace[\"q_value\"]\n", "\n", "    with torch.no_grad():\n", "        # Replay the current actor on the replay buffer to get actions of the\n", "        # current policy\n", "        t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)\n", "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n", "\n", "        # Compute target q_values from both target critics: at t+1, we have\n", "        # Q(s+1,a+1) from the (s+1,a+1) where a+1 has been replaced in the RB\n", "\n", "        target_q_agent_1(rb_workspace, t=1, n_steps=1)\n", "        post_q_values_1 = rb_workspace[\"q_value\"]\n", "\n", "        target_q_agent_2(rb_workspace, t=1, n_steps=1)\n", "        post_q_values_2 = rb_workspace[\"q_value\"]\n", "\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "    return critic_loss_1, critic_loss_2"]}, {"cell_type": "markdown", "id": "1f052bb3", "metadata": {"id": "RaRH4rg-HZb5", "lines_to_next_cell": 2}, "source": ["As in DDPG and TD3, we use target critics, thus we need the\n", "`soft_update_params(...)` function to make sure that the target critics are\n", "tracking the true critics, using the same equation: $\\theta' \\leftarrow \\tau\n", "\\theta + (1- \\tau) \\theta'$."]}, {"cell_type": "code", "execution_count": null, "id": "e812d8bd", "metadata": {"id": "gAIbEbNGIdb_", "lines_to_next_cell": 2}, "outputs": [], "source": ["def soft_update_params(net, target_net, tau):\n", "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n", "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"]}, {"cell_type": "markdown", "id": "fe2aa5be", "metadata": {"id": "G_OzHrYqYOkR"}, "source": ["### Compute the actor Loss\n", "\n", "\n", "With the notations of my slides, the equation of the actor loss corresponding\n", "to Eq. (7) in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n", "\n", "$$ loss_\\pi({\\boldsymbol{\\theta}}) = {\\rm I\\!E}_{\\mathbf{s}_t \\sim\n", "\\mathcal{D}}\\left[ {\\rm I\\!E}_{\\mathbf{a}_t\\sim\n", "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_t)} \\left[ \\alpha\n", "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}_t|\\mathbf{s}_t) -\n", "\\hat{Q}^{\\pi_{\\boldsymbol{\\theta}}}_{\\boldsymbol{\\phi}}(\\mathbf{s}_t,\n", "\\mathbf{a}_t)} \\right] \\right] $$\n", "\n", "Note that [the paper](https://arxiv.org/pdf/1812.05905.pdf) mistakenly writes\n", "$Q_\\theta(s_t,s_t)$\n", "\n", "As for the critic loss, we have two expectations, one over the states from the\n", "replay buffer, and one over the actions of the current policy. Thus we need to\n", "apply again the current policy to the content of the replay buffer.\n", "\n", "But this time, we consider the current state, thus we parametrize it with\n", "`t=0` and `n_steps=1`. This way, we get the log probabilities and Q-values at\n", "the current step.\n", "\n", "A nice thing is that this way, there is no overlap between the log probability\n", "data used to update the critic and the actor, which avoids having to 'retain'\n", "the computation graph so that it can be reused for the actor and the critic.\n", "\n", "This small trick is one of the features that makes coding SAC the most\n", "difficult.\n", "\n", "Again, once we have computed the Q values over both critics, we take the min\n", "and put it into `current_q_values`.\n", "\n", "As for the critic loss, if we set `ent_coef`\u00a0to 0, we retrieve the actor loss\n", "function of DDPG and TD3, which simply tries to get actions that maximize the\n", "Q values (by minimizing -Q)."]}, {"cell_type": "code", "execution_count": null, "id": "5ce06067", "metadata": {"id": "xLq_ZeFzEHON", "lines_to_next_cell": 2}, "outputs": [], "source": ["def compute_actor_loss(ent_coef, t_actor, q_agent_1, q_agent_2, rb_workspace):\n", "    \"\"\"Actor loss computation\n", "    \n", "    :param ent_coef: The entropy coefficient $\\alpha$\n", "    :param t_actor: The actor agent (temporal agent)\n", "    :param q_agent_1: The first critic (temporal agent)\n", "    :param q_agent_2: The second critic (temporal agent)\n", "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n", "    \"\"\"\n", "    # Recompute the q_values from the current policy, not from the actions in the buffer\n", "\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n", "\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "    current_q_values = torch.min(q_values_1, q_values_2).squeeze(-1)\n", "\n", "    # \u00c0 compl\u00e9ter...  \n", "    assert False, 'Code non impl\u00e9ment\u00e9'\n", "\n", "    return actor_loss.mean()"]}, {"cell_type": "markdown", "id": "80a0bf22", "metadata": {"id": "Jmi91gANWT4z", "lines_to_next_cell": 2}, "source": ["\n", "## Main training loop"]}, {"cell_type": "code", "execution_count": null, "id": "4504660b", "metadata": {"id": "sk85_sRWW-5s"}, "outputs": [], "source": ["\n", "def run_sac(cfg):\n", "    # 1)  Build the  logger\n", "    logger = Logger(cfg)\n", "    best_reward = -10e9\n", "    ent_coef = cfg.algorithm.entropy_coef\n", "\n", "    # 2) Create the environment agent\n", "    train_env_agent = AutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.n_envs,\n", "        cfg.algorithm.seed,\n", "    )\n", "    eval_env_agent = NoAutoResetGymAgent(\n", "        get_class(cfg.gym_env),\n", "        get_arguments(cfg.gym_env),\n", "        cfg.algorithm.nb_evals,\n", "        cfg.algorithm.seed,\n", "    )\n", "\n", "    # 3) Create the A2C Agent\n", "    (\n", "        train_agent,\n", "        eval_agent,\n", "        actor,\n", "        critic_1,\n", "        target_critic_1,\n", "        critic_2,\n", "        target_critic_2,\n", "    ) = create_sac_agent(cfg, train_env_agent, eval_env_agent)\n", "\n", "    t_actor = TemporalAgent(actor)\n", "    q_agent_1 = TemporalAgent(critic_1)\n", "    target_q_agent_1 = TemporalAgent(target_critic_1)\n", "    q_agent_2 = TemporalAgent(critic_2)\n", "    target_q_agent_2 = TemporalAgent(target_critic_2)\n", "    train_workspace = Workspace()\n", "\n", "    # Creates a replay buffer\n", "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n", "\n", "    # Configure the optimizer\n", "    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic_1, critic_2)\n", "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n", "    nb_steps = 0\n", "    tmp_steps = 0\n", "\n", "    # Initial value of the entropy coef alpha. If target_entropy is not auto,\n", "    # will remain fixed\n", "    if cfg.algorithm.target_entropy == \"auto\":\n", "        target_entropy = -np.prod(train_env_agent.action_space.shape).astype(np.float32)\n", "    else:\n", "        target_entropy = cfg.algorithm.target_entropy\n", "\n", "    # Training loop\n", "    for epoch in range(cfg.algorithm.max_epochs):\n", "        # Execute the agent in the workspace\n", "        if epoch > 0:\n", "            train_workspace.zero_grad()\n", "            train_workspace.copy_n_last_steps(1)\n", "            train_agent(\n", "                train_workspace,\n", "                t=1,\n", "                n_steps=cfg.algorithm.n_steps - 1,\n", "                stochastic=True,\n", "            )\n", "        else:\n", "            train_agent(\n", "                train_workspace,\n", "                t=0,\n", "                n_steps=cfg.algorithm.n_steps,\n", "                stochastic=True,\n", "            )\n", "\n", "        transition_workspace = train_workspace.get_transitions()\n", "        action = transition_workspace[\"action\"]\n", "        nb_steps += action[0].shape[0]\n", "        rb.put(transition_workspace)\n", "\n", "        if nb_steps > cfg.algorithm.learning_starts:\n", "            # Get a sample from the workspace\n", "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n", "\n", "            done, truncated, reward, action_logprobs_rb = rb_workspace[\n", "                \"env/done\", \"env/truncated\", \"env/reward\", \"action_logprobs\"\n", "            ]\n", "\n", "            # Determines whether values of the critic should be propagated\n", "            # True if the episode reached a time limit or if the task was not done\n", "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n", "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n", "\n", "            (\n", "                critic_loss_1, critic_loss_2\n", "            ) = compute_critic_loss(\n", "                cfg, \n", "                reward, \n", "                must_bootstrap,\n", "                t_actor,\n", "                q_agent_1,\n", "                q_agent_2,\n", "                target_q_agent_1,\n", "                target_q_agent_2,\n", "                rb_workspace,\n", "                ent_coef\n", "            )\n", "\n", "            logger.add_log(\"critic_loss_1\", critic_loss_1, nb_steps)\n", "            logger.add_log(\"critic_loss_2\", critic_loss_2, nb_steps)\n", "            critic_loss = critic_loss_1 + critic_loss_2\n", "\n", "            actor_loss = compute_actor_loss(\n", "                ent_coef, t_actor, q_agent_1, q_agent_2, rb_workspace\n", "            )\n", "            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n", "\n", "            # Entropy coef update part #####################################################\n", "            if entropy_coef_optimizer is not None:\n", "                # Important: detach the variable from the graph\n", "                # so that we don't change it with other losses\n", "                # see https://github.com/rail-berkeley/softlearning/issues/60\n", "                ent_coef = torch.exp(log_entropy_coef.detach())\n", "                # See Eq. (17) of the SAC and Applications paper\n", "                entropy_coef_loss = -(\n", "                    log_entropy_coef * (action_logprobs_rb + target_entropy)\n", "                ).mean()\n", "                entropy_coef_optimizer.zero_grad()\n", "                # We need to retain the graph because we reuse the\n", "                # action_logprobs are used to compute both the actor loss and\n", "                # the critic loss\n", "                entropy_coef_loss.backward(retain_graph=True)\n", "                entropy_coef_optimizer.step()\n", "                logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, nb_steps)\n", "                logger.add_log(\"entropy_coef\", ent_coef, nb_steps)\n", "\n", "            # Actor update part ###############################\n", "            actor_optimizer.zero_grad()\n", "            actor_loss.backward()\n", "            torch.nn.utils.clip_grad_norm_(\n", "                actor.parameters(), cfg.algorithm.max_grad_norm\n", "            )\n", "            actor_optimizer.step()\n", "\n", "\n", "            # Critic update part ###############################\n", "            critic_optimizer.zero_grad()\n", "            critic_loss.backward()\n", "            torch.nn.utils.clip_grad_norm_(\n", "                critic_1.parameters(), cfg.algorithm.max_grad_norm\n", "            )\n", "            torch.nn.utils.clip_grad_norm_(\n", "                critic_2.parameters(), cfg.algorithm.max_grad_norm\n", "            )\n", "            critic_optimizer.step()\n", "            ####################################################\n", "\n", "            # Soft update of target q function\n", "            tau = cfg.algorithm.tau_target\n", "            soft_update_params(critic_1, target_critic_1, tau)\n", "            soft_update_params(critic_2, target_critic_2, tau)\n", "            # soft_update_params(actor, target_actor, tau)\n", "\n", "        # Evaluate ###########################################\n", "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n", "            tmp_steps = nb_steps\n", "            eval_workspace = Workspace()  # Used for evaluation\n", "            eval_agent(\n", "                eval_workspace,\n", "                t=0,\n", "                stop_variable=\"env/done\",\n", "                stochastic=False,\n", "            )\n", "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n", "            mean = rewards.mean()\n", "            logger.add_log(\"reward/mean\", mean, nb_steps)\n", "            logger.add_log(\"reward/max\", rewards.max(), nb_steps)\n", "            logger.add_log(\"reward/min\", rewards.min(), nb_steps)\n", "            logger.add_log(\"reward/min\", rewards.median(), nb_steps)\n", "\n", "            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n", "            # print(\"ent_coef\", ent_coef)\n", "            if cfg.save_best and mean > best_reward:\n", "                best_reward = mean\n", "                directory = f\"./agents/{cfg.gym_env.env_name}/sac_agent/\"\n", "                if not os.path.exists(directory):\n", "                    os.makedirs(directory)\n", "                filename = directory + \"sac_\" + str(mean.item()) + \".agt\"\n", "                actor.save_model(filename)\n", "                \n"]}, {"cell_type": "markdown", "id": "67df9aa7", "metadata": {"id": "uo6bc3zzKua_"}, "source": ["## Definition of the parameters"]}, {"cell_type": "code", "execution_count": null, "id": "d6643dc0", "metadata": {"id": "JB2B8zELNWQd"}, "outputs": [], "source": ["\n", "params={\n", "  \"save_best\": True,\n", "  \"logger\":{\n", "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n", "    \"log_dir\": \"./tblogs/CartPoleContinuous-v1/sac-\" + str(time.time()),\n", "    \"cache_size\": 10000,\n", "    \"every_n_seconds\": 10,\n", "    \"verbose\": False,    \n", "    },\n", "\n", "  \"algorithm\":{\n", "    \"seed\": 1,\n", "    \"n_envs\": 8,\n", "    \"n_steps\": 32,\n", "    \"buffer_size\": 1e6,\n", "    \"batch_size\": 256,\n", "    \"max_grad_norm\": 0.5,\n", "    \"nb_evals\":10,\n", "    \"eval_interval\": 2000,\n", "    \"learning_starts\": 10000,\n", "    \"max_epochs\": 8000,\n", "    \"discount_factor\": 0.98,\n", "    \"entropy_coef\": 1e-7,\n", "    \"target_entropy\": \"auto\",\n", "    \"tau_target\": 0.05,\n", "    \"architecture\":{\n", "      \"actor_hidden_size\": [32, 32],\n", "      \"critic_hidden_size\": [256, 256],\n", "    },\n", "  },\n", "  \"gym_env\":{\n", "    \"classname\": \"__main__.make_gym_env\",\n", "    \"env_name\": \"CartPoleContinuous-v1\",\n", "    },\n", "  \"actor_optimizer\":{\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 1e-3,\n", "    },\n", "  \"critic_optimizer\":{\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 1e-3,\n", "    },\n", "  \"entropy_coef_optimizer\":{\n", "    \"classname\": \"torch.optim.Adam\",\n", "    \"lr\": 1e-3,\n", "    }\n", "}"]}, {"cell_type": "markdown", "id": "970f142b", "metadata": {"cell_marker": "\"\"\"", "id": "jp7jDeGkaoM1"}, "source": ["### Launching tensorboard to visualize the results"]}, {"cell_type": "code", "execution_count": null, "id": "a70cd20c", "metadata": {"tags": ["not-colab"]}, "outputs": [], "source": ["import sys\n", "import os\n", "import os.path as osp\n", "print(f'''Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir=\"{params[\"logger\"][\"log_dir\"]}\"''')"]}, {"cell_type": "code", "execution_count": null, "id": "aa412bbb", "metadata": {"id": "l42OUoGROlSt"}, "outputs": [], "source": ["config=OmegaConf.create(params)\n", "torch.manual_seed(config.algorithm.seed)\n", "run_sac(config)"]}, {"cell_type": "markdown", "id": "dfe2bcea", "metadata": {"id": "paHdoNlz9Lpg"}, "source": ["Now we can look at the agent"]}, {"cell_type": "code", "execution_count": null, "id": "6ca47dab", "metadata": {}, "outputs": [], "source": ["\n", "from bbrl.visu.play import play, load_agent, Path\n", "agent = load_agent(Path(f\"agents/{config.gym_env.env_name}/sac_agent\"), \"sac_\")\n", "\n", "def play(env: gym.Env, agent: torch.nn.Module):\n", "    \"\"\"Render the agent\"\"\"\n", "    if agent is None:\n", "        print(\"No agent\")\n", "        return\n", "\n", "    sum_reward = 0.\n", "    \n", "    try:\n", "        print(agent)\n", "        with torch.no_grad():\n", "            obs = env.reset()\n", "            env.render()\n", "            done = False\n", "            while not done:\n", "                obs = torch.Tensor(obs)\n", "                action = agent.predict_action(obs, False)\n", "                obs, reward, done, info = env.step(action.numpy())\n", "                sum_reward += reward\n", "                env.render()\n", "    finally:\n", "        env.close()\n", "\n", "    return reward\n", "\n", "play(make_gym_env(config.gym_env.env_name), agent)"]}, {"cell_type": "markdown", "id": "1d61b40f", "metadata": {"id": "paHdoNlz9Lpg"}, "source": ["## Exercises\n", "\n", "- use the same code on the Pendulum-v1 environment. This one is harder to\n", "  tune. Get the parameters from the\n", "  [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) and see if\n", "  you manage to get SAC working on Pendulum"]}, {"cell_type": "code", "execution_count": null, "id": "2a4b516b", "metadata": {}, "outputs": [], "source": []}], "metadata": {"jupytext": {"cell_markers": "\"\"\""}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}
